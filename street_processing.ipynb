{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this file is to\n",
    "* Process the 1836 dataset (finding / working with duplicate streets)\n",
    "* Define which streets belong to only one or both datasets\n",
    "* Have one final dataset containing all georeferenced streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.ops import linemerge, Point\n",
    "#from shapely.errors import ShapelyDeprecationWarning\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "#from importnb import Notebook\n",
    "from preprocessing import preprocess\n",
    "from collections import Counter\n",
    "from paris_methods import duplicate_processing, duplicate_final, assign_gridnumber, translate_geopoints, create_grid\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shapefiles\n",
    "Openparis = gpd.read_file(\"data/voie.zip\", encoding = 'utf-8')\n",
    "Vasserot = gpd.read_file(\"data/vasserot.zip\")\n",
    "\n",
    "# Set right EPSG for Geodata\n",
    "Openparis = Openparis.to_crs(epsg=3857)\n",
    "Vasserot = Vasserot.to_crs(epsg=3857)\n",
    "\n",
    "#change mistakes in streetnames\n",
    "mask = Vasserot.loc[:,\"NOM_ENTIER\"] == \"Rue Lafayette\"\n",
    "Vasserot.loc[mask,[\"NOM\",\"NOM_ENTIER\"]] = [\"la Fayette\", \"Rue la Fayette\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess streets with leas code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Openparis = preprocess(Openparis, \"l_longmin\")\n",
    "Vasserot = preprocess(Vasserot, \"NOM_ENTIER\")\n",
    "\n",
    "# TODO Rename those streets\n",
    "Vasserot[\"voie\"] = Vasserot[\"NOM_ENTIER_prep\"]\n",
    "Openparis[\"voie\"] = Openparis[\"l_longmin_prep\"]\n",
    "\n",
    "# Remove empty lines\n",
    "Vasserot = Vasserot.dropna(subset=[\"voie\"])\n",
    "\n",
    "# Add year\n",
    "Vasserot = Vasserot.assign(year= [[1836]]*len(Vasserot))\n",
    "Openparis = Openparis.assign(year= [[2022]]*len(Openparis))\n",
    "\n",
    "# create buffer around streets, important for merging duplicate streets\n",
    "buffer = 100\n",
    "Vasserot[\"buffer\"] = Vasserot[\"geometry\"].apply(lambda x: x.buffer(buffer))\n",
    "Openparis[\"buffer\"] = Openparis[\"geometry\"].apply(lambda x: x.buffer(buffer))\n",
    "\n",
    "\n",
    "\n",
    "# Find all duplicates\n",
    "Duplicates = Vasserot[Vasserot.duplicated(subset=['voie'], keep=False)].sort_values(\"voie\")\n",
    "Unique = Vasserot[~Vasserot.duplicated(subset=['voie'], keep=False)].sort_values(\"voie\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Process 1836 streets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge duplicates if streets are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DuplicatesProcessed = duplicate_processing(Duplicates, \"voie\")\n",
    "\n",
    "# Sanity check if all streetnames are in the newly created dataframe\n",
    "#len(Duplicates[\"voie\"].unique()) == DuplicatesProcessed[\"voie\"].unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Nr. of unique streets before and after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = Counter(Duplicates[\"voie\"].value_counts())\n",
    "plt.bar(freqs.keys(), freqs.values(), width = 0.9)\n",
    "freqs = Counter(DuplicatesProcessed[\"voie\"].value_counts())\n",
    "plt.bar(freqs.keys(), freqs.values(), width = 0.9)\n",
    "plt.show()\n",
    "\n",
    "print(\"Duplicates before: \", sum(Duplicates[\"voie\"].value_counts()>1))\n",
    "print(\"Duplicates after: \", sum(DuplicatesProcessed[\"voie\"].value_counts()>1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StillDuplicates = DuplicatesProcessed[DuplicatesProcessed.duplicated(subset=['voie'], keep=False)].sort_values(\"voie\")\n",
    "NewlyUnique = DuplicatesProcessed[~DuplicatesProcessed.duplicated(subset=['voie'], keep=False)].sort_values(\"voie\")\n",
    "# Adding newly unique streets\n",
    "Unique = pd.concat([Unique, NewlyUnique])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Streets that are still duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed in order to visualize results\n",
    "StillDuplicates = StillDuplicates.drop(columns=[\"buffer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Street to visualize\n",
    "a = StillDuplicates[\"voie\"].value_counts()[StillDuplicates[\"voie\"].value_counts()>=2]\n",
    "Trueduplicates = a.index\n",
    "mask = StillDuplicates[\"voie\"] == Trueduplicates[8]\n",
    "# Visualize\n",
    "#StillDuplicates[mask].explore()\n",
    "StillDuplicates.explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset containing old and new streets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Vasserot and Openparis dataframe for final comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique from Vasserot DF\n",
    "Unique = Unique.iloc[:,[0,2,3,6,8,14,16,17,18]]\n",
    "Unique = Unique.rename(columns={\"ROWID\":\"rowid\", \"NOM_ENTIER\":\"streetname\",\"TYPE\":\"type\",\"ARTICLE\":\"article\",\"NOM\":\"name\", \"voie\":\"streetname_prep\"})\n",
    "Unique = Unique.assign(matching = [[]] * len(Unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniques from Openparis DF\n",
    "Openparis = Openparis.iloc[:,[2,3,4,5,6,15,17,18,19]]\n",
    "Openparis = Openparis.rename(columns={\"l_longmin\": \"streetname\",\"c_desi\":\"type\",\"c_liaison\":\"article\",\"l_voie\":\"name\",\"l_courtmin\":\"streetname_short\",\"voie\":\"streetname_prep\"})\n",
    "# assign random rowid to Openparis data because they dont have them\n",
    "Openparis = Openparis.assign(rowid = np.random.randint(7000, 200000, size=len(Openparis)))\n",
    "Openparis = Openparis.assign(matching = [[]] * len(Openparis))\n",
    "\n",
    "Merged = pd.concat([Unique, Openparis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if streets with same name are at same location \n",
    "MergedProcessed = duplicate_final(Merged, \"streetname_prep\")\n",
    "MergedProcessed = preprocess(MergedProcessed, \"name\")\n",
    "\n",
    "# Create Datasets containing duplicates and unique streets\n",
    "FinalDuplicates = MergedProcessed[MergedProcessed.duplicated(subset=['streetname_prep'], keep=False)].sort_values(\"streetname_prep\")\n",
    "FinalUnique= MergedProcessed[~MergedProcessed.duplicated(subset=['streetname_prep'], keep=False)].sort_values(\"streetname_prep\")\n",
    "\n",
    "# Assign right classes to data\n",
    "FinalUnique = FinalUnique.convert_dtypes()\n",
    "FinalDuplicates = FinalDuplicates.convert_dtypes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results in the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalDuplicates= FinalDuplicates.drop(columns=[\"buffer\"])\n",
    "FinalDuplicates.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalUnique[\"geometry\"].explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FinalUnique.to_pickle(\"data/FinalUnique.pkl\")\n",
    "FinalDuplicates.to_pickle(\"data/FinalDuplicate.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c2682987f00c8c803475925d827e887daeba32793bd1ae3ff6e12f2969d73b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
