{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this file is to\n",
    "* Process the 1836 dataset (finding / working with duplicate streets)\n",
    "* Define which streets belong to only one or both datasets\n",
    "* Have one final dataset containing all georeferenced streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.ops import linemerge, Point\n",
    "import warnings\n",
    "try:\n",
    "    from shapely.errors import ShapelyDeprecationWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "except:\n",
    "    print(\"Couldn't install ShapelyDeprecationWarning\")\n",
    "\n",
    "from preprocessing import preprocess\n",
    "from collections import Counter\n",
    "from paris_methods import duplicate_processing, duplicate_final, assign_gridnumber, translate_geopoints, create_grid, check_overlap\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shapefiles\n",
    "Openparis = gpd.read_file(\"data/voie.zip\", encoding = 'utf-8')\n",
    "Vasserot = gpd.read_file(\"data/vasserot.zip\")\n",
    "\n",
    "# Set right EPSG for Geodata\n",
    "Openparis = Openparis.to_crs(epsg=3857)\n",
    "Vasserot = Vasserot.to_crs(epsg=3857)\n",
    "\n",
    "#change mistakes in streetnames\n",
    "mask = Vasserot.loc[:,\"NOM_ENTIER\"] == \"Rue Lafayette\"\n",
    "Vasserot.loc[mask,[\"NOM\",\"NOM_ENTIER\"]] = [\"la Fayette\", \"Rue la Fayette\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Openparis = preprocess(Openparis, \"l_longmin\")\n",
    "Vasserot = preprocess(Vasserot, \"NOM_ENTIER\")\n",
    "\n",
    "# TODO Rename those streets\n",
    "Vasserot[\"voie\"] = Vasserot[\"NOM_ENTIER_prep\"]\n",
    "Openparis[\"voie\"] = Openparis[\"l_longmin_prep\"]\n",
    "\n",
    "# Remove empty lines\n",
    "Vasserot = Vasserot.dropna(subset=[\"voie\"])\n",
    "\n",
    "# Add year\n",
    "Vasserot = Vasserot.assign(year= [[1836]]*len(Vasserot))\n",
    "Openparis = Openparis.assign(year= [[2022]]*len(Openparis))\n",
    "\n",
    "# create buffer around streets, important for merging duplicate streets\n",
    "buffer = 100\n",
    "Vasserot[\"buffer\"] = Vasserot[\"geometry\"].apply(lambda x: x.buffer(buffer))\n",
    "Openparis[\"buffer\"] = Openparis[\"geometry\"].apply(lambda x: x.buffer(buffer))\n",
    "\n",
    "# Find all duplicates\n",
    "Duplicates = Vasserot[Vasserot.duplicated(subset=['voie'], keep=False)].sort_values(\"voie\")\n",
    "Unique = Vasserot[~Vasserot.duplicated(subset=['voie'], keep=False)].sort_values(\"voie\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Process 1836 streets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge duplicates if streets are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DuplicatesProcessed = duplicate_processing(Duplicates, \"voie\")\n",
    "\n",
    "# Sanity check if all streetnames are in the newly created dataframe\n",
    "#len(Duplicates[\"voie\"].unique()) == DuplicatesProcessed[\"voie\"].unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Nr. of unique streets before and after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = Counter(Duplicates[\"voie\"].value_counts())\n",
    "plt.bar(freqs.keys(), freqs.values(), width = 0.9)\n",
    "freqs = Counter(DuplicatesProcessed[\"voie\"].value_counts())\n",
    "plt.bar(freqs.keys(), freqs.values(), width = 0.9)\n",
    "plt.show()\n",
    "\n",
    "print(\"Duplicates before: \", sum(Duplicates[\"voie\"].value_counts()>1))\n",
    "print(\"Duplicates after: \", sum(DuplicatesProcessed[\"voie\"].value_counts()>1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StillDuplicates = DuplicatesProcessed[DuplicatesProcessed.duplicated(subset=['voie'], keep=False)].sort_values(\"voie\")\n",
    "NewlyUnique = DuplicatesProcessed[~DuplicatesProcessed.duplicated(subset=['voie'], keep=False)].sort_values(\"voie\")\n",
    "# Adding newly unique streets\n",
    "Unique = pd.concat([Unique, NewlyUnique])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Streets that are still duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed in order to visualize results\n",
    "StillDuplicates = StillDuplicates.drop(columns=[\"buffer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Street to visualize\n",
    "a = StillDuplicates[\"voie\"].value_counts()[StillDuplicates[\"voie\"].value_counts()>=2]\n",
    "Trueduplicates = a.index\n",
    "mask = StillDuplicates[\"voie\"] == Trueduplicates[8]\n",
    "# Visualize\n",
    "#StillDuplicates[mask].explore()\n",
    "StillDuplicates.explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset containing old and new streets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Vasserot and Openparis dataframe for final comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique from Vasserot DF\n",
    "Unique = Unique.iloc[:,[0,2,3,6,8,14,16,17,18]]\n",
    "Unique = Unique.rename(columns={\"ROWID\":\"rowid\", \"NOM_ENTIER\":\"streetname\",\"TYPE\":\"type\",\"ARTICLE\":\"article\",\"NOM\":\"name\", \"voie\":\"streetname_prep\"})\n",
    "Unique = Unique.assign(matching = [[]] * len(Unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniques from Openparis DF\n",
    "Openparis = Openparis.iloc[:,[2,3,4,5,6,15,17,18,19]]\n",
    "Openparis = Openparis.rename(columns={\"l_longmin\": \"streetname\",\"c_desi\":\"type\",\"c_liaison\":\"article\",\"l_voie\":\"name\",\"l_courtmin\":\"streetname_short\",\"voie\":\"streetname_prep\"})\n",
    "# assign random rowid to Openparis data because they dont have them\n",
    "Openparis = Openparis.assign(rowid = np.random.randint(7000, 200000, size=len(Openparis)))\n",
    "Openparis = Openparis.assign(matching = [[]] * len(Openparis))\n",
    "\n",
    "Merged = pd.concat([Unique, Openparis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if streets with same name are at same location \n",
    "MergedProcessed = duplicate_final(Merged, \"streetname_prep\")\n",
    "MergedProcessed = preprocess(MergedProcessed, \"name\")\n",
    "\n",
    "# Create Datasets containing duplicates and unique streets\n",
    "FinalDuplicates = MergedProcessed[MergedProcessed.duplicated(subset=['streetname_prep'], keep=False)].sort_values(\"streetname_prep\")\n",
    "FinalUnique= MergedProcessed[~MergedProcessed.duplicated(subset=['streetname_prep'], keep=False)].sort_values(\"streetname_prep\")\n",
    "\n",
    "# Assign right classes to data\n",
    "FinalUnique = FinalUnique.convert_dtypes()\n",
    "FinalDuplicates = FinalDuplicates.convert_dtypes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results in the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalDuplicates= FinalDuplicates.drop(columns=[\"buffer\"])\n",
    "FinalDuplicates.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalUnique[\"geometry\"].explore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FinalUnique.to_pickle(\"data/FinalUnique.pkl\")\n",
    "FinalDuplicates.to_pickle(\"data/FinalDuplicate.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping on Short Streetnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group streets based on their short name\n",
    "grouped_streets = FinalUnique.groupby(\"name_prep\", as_index=False).agg({\"streetname\": \", \".join, \n",
    "        \"geometry\": list, \"year\": list, \"rowid\": list, \"name\": \"first\", \"buffer\":list})\n",
    "\n",
    "# split streets in those that are unique and those that aren't\n",
    "unique_short_streets = grouped_streets[grouped_streets['buffer'].str.len() == 1]\n",
    "unique_short_streets[[\"year\", \"geometry\", \"rowid\", \"buffer\"]] = unique_short_streets[[\"year\", \"geometry\", \"rowid\", \"buffer\"]].apply(lambda x: x.str[0])\n",
    "multiple_short_streets = grouped_streets[grouped_streets['buffer'].str.len() > 1]\n",
    "\n",
    "print(f\"#streets with unique short streetname: {len(unique_short_streets)}, not unique: {len(multiple_short_streets)}\")\n",
    "unique_short_streets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in a new column if all streets with the same short streetname overlap\n",
    "multiple_short_streets[\"all_overlap\"] = multiple_short_streets[\"buffer\"].apply(check_overlap)\n",
    "# divide data in overlapping and not overlapping\n",
    "overlap = multiple_short_streets[multiple_short_streets[\"all_overlap\"]==True]\n",
    "no_overlap = multiple_short_streets[multiple_short_streets[\"all_overlap\"]==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if type of streets differ in the overlapping/non overlapping datasets\n",
    "overlap_prefixes = [tuple([street.split()[0] for street in street_list]) for street_list in overlap.streetname.str.split(\", \")]\n",
    "print(\"overlapping top 10:\\n\", Counter(overlap_prefixes).most_common(10))\n",
    "\n",
    "no_overlap_prefixes = [tuple([street.split()[0] for street in street_list]) for street_list in no_overlap.streetname.str.split(\", \")]\n",
    "print(\"\\nNon-overlapping top 10:\\n\",Counter(no_overlap_prefixes).most_common(10))\n",
    "\n",
    "# -> they do not differ a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#overlapping streets: 491 \n",
      "#not overlapping streets: 289 \n",
      "ratio of overlapping/all = 0.6294871794871795\n"
     ]
    }
   ],
   "source": [
    "# statistics\n",
    "print(\"#overlapping streets:\", len(overlap), \"\\n#not overlapping streets:\", len(no_overlap),\n",
    "        \"\\nratio of overlapping/all =\", len(overlap)/len(multiple_short_streets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge geometry of overlapping streets and drop \"all_overlap\" column\n",
    "overlap[\"geometry\"] = overlap[\"geometry\"].apply(lambda row: gpd.GeoSeries(row).unary_union)\n",
    "overlap.drop(columns=\"all_overlap\", inplace=True)\n",
    "\n",
    "# append overlapping streets to unique_short_streets\n",
    "unique_short_streets = pd.concat([unique_short_streets, overlap])\n",
    "# update non_unique_short_streets\n",
    "non_unique_short_streets = no_overlap.drop(columns=\"all_overlap\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Short Streetdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_short_streets.to_pickle(\"data/unique_short_streets.pkl\")\n",
    "# multiple_short_streets will be neglected further on\n",
    "multiple_short_streets.to_pickle(\"data/not_unique_short_streets.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-FDH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d883feb109874adde3860b85ffbf32e7d2f62c837e96b59e44cfd6e783a4204"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
