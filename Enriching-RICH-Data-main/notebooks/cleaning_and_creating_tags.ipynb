{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d373b1b7-3336-4f32-84b6-0db26e9d407c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tags for Profession\n",
    "\n",
    "This notebook contains the code and commentary on the steps involved in the creation of tags for the professions of people in Paris.\n",
    "\n",
    "The algorithm is explained in detail at <ADD THE URL TO THE DOCUMENT>. In brief, the data generated from the [first phase of the project](https://quartier-richelieu.fr/) provided a dataset of entries from the trade directories of Paris. For each year a list of names, professions, and address sets was created. The address for the entries belonging to the Richelieu Quartier was cleaned and geo-referenced. However, the profession was not cleaned and normalized across years and the abbreviations were not filled. In the notebook, the profession for each entry of the dataset is cleaned by providing tags that represent the profession and filling the abbreviations with full forms. \n",
    "\n",
    "Before this notebook, \n",
    "\n",
    "The `cleaning_special_characters.ipynb` notebook was run to generate `all_paris_jobs_splchar_cleaned.csv` file (the content is explained in [Reading the Data after cleaning for special characters](reading-the-data-after-cleaning-for-special-characters) section. This file will serve as the starting point of this notebook.\n",
    "\n",
    "Secondly, the french language dictionaries provided by `Morphalou3` and `Prolex-Unitex` were used to create a set of unique words to serve as a list of correctly spelled words (the street names are added to the list in this notebook) for this project and saved as `words_from_french_language_dictionaries.json`.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook performs the following keys tasks\n",
    "\n",
    "1. During the OCR process, sometimes a single word is mistakenly broken into multiple words. So, an attempt is made to combine them.\n",
    "2. As the profession strings contain keywords and connecting words, the connecting words are removed and keywords are stored with cleaning at apostrophe and dot as tokens.\n",
    "3. The tokens that are not in the list of correct words are tried to merge with those in the list of correct words based on the token co-occurrence, frequency, and similarity.\n",
    "4. After merging, the tokens containing a dot are identified as potential abbreviations and they are filled based on co-occurrence, frequency, and similarity.\n",
    "5. After completing the abbreviations, the tokens are called tags, and the data is stored on the disk.\n",
    "\n",
    "The pipeline is built on the following two major assumptions,\n",
    "\n",
    "1. Most of the words have a correct spelling\n",
    "2. The correct spelling appears more frequently than the misspelled one.\n",
    "    \n",
    "## Similarity of words (tokens)\n",
    " \n",
    "The token similarity is the Levenshtein similarity between the tokens. The [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings is the \"minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other\". The distance is normalized between 0 and 1 and converted into similarity. For this project, the similarity is calculated using the [RapidFuzz](https://github.com/maxbachmann/RapidFuzz) library's `fuzz.ratio` function without any processing of the strings. The reason for choosing this library is that the function to calculate the similarity between strings accepts a threshold for similarity and use it is as an early stopping criterion in calculating the similarity between the stings (based on the lengths of the strings, It is not possible to obtain a high similarity between the strings when the length of them is significantly different). It returns zero as the similarity when the similarity is less than the given threshold.    \n",
    "\n",
    "## Imports\n",
    "\n",
    "Import the existing python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7f3ed-c435-4918-a2b8-40fae56532fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_french = set(stopwords.words(\"french\"))\n",
    "from collections import Counter\n",
    "from typing import Callable\n",
    "from typing import Counter as Counter_type\n",
    "from typing import Dict, FrozenSet, List, NoReturn, Optional, Set, Tuple, Union\n",
    "\n",
    "import colorama\n",
    "import textdistance\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from unidecode import unidecode\n",
    "\n",
    "colorama.deinit()\n",
    "colorama.init(strip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06de75c-84d0-4333-8dc3-5c10a70666a2",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "the pipeline requires the users to set some hyperparameters at various steps in the process. The hyper parameters are\n",
    "\n",
    "1. `MINIMUM_TOKEN_LENGTH`: The minimum number of alphanumeric characters to be present in a token to be considered as valid.\n",
    "2. `MINIMUM_TOKEN_FREQUENCY`: The minimum frequency for a token to be not considered as low frequent token.\n",
    "3. `MIN_THRESHOLD_REPLACE_BROKEN_WORD`: The minimum similarity for a word to be considered as replacement for a set of consecutive low frequent and non correct tokens.\n",
    "4. `MINIMUM_TOKEN_SIMILARITY`: The minimum similarity (_fuzz.ratio_) between tokens to be considered as similar.\n",
    "5. `MINIMUM_ABVR_FULLFORM_SIMILARITY`: The minimum threshold for the modified Jaccard similarity score (detailed in [Algorithm for completing the abbreviations with support](#algorithm-for-completing-the-abbreviations-with-support)) between the full form and the abbreviation.\n",
    "6. `MINIMUM_INTER_ABVR_SIMILARITY`: The minimum threshold between the filled and unfilled abbreviations to be considered similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4690372-c9d4-45c8-8a89-e64594a8414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_TOKEN_LENGTH = 3\n",
    "MINIMUM_TOKEN_FREQUENCY = 51\n",
    "MIN_THRESHOLD_REPLACE_BROKEN_WORD = 79.49\n",
    "MINIMUM_TOKEN_SIMILARITY = 74.49\n",
    "MINIMUM_ABVR_FULLFORM_SIMILARITY = 50\n",
    "MINIMUM_INTER_ABVR_SIMILARITY = 69.49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ff1eb-76db-4ec1-aefd-2a5f0b67e304",
   "metadata": {},
   "source": [
    "The intermediate outcomes of the process are stored in the folder with the name containing the similarity score. In the next cell, a folder is created in the `intermediate_steps` folder with the similarity score in its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd689477-588a-490d-a2a2-cd2654d8575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_steps_folder_prefix = \"./../data/intermediate_steps/sim_score_\"+str(int(np.ceil(MINIMUM_TOKEN_SIMILARITY)))+\"/\"\n",
    "os.makedirs(os.path.dirname(intermediate_steps_folder_prefix), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3817f-18fa-425f-b0d9-2197101d8c73",
   "metadata": {},
   "source": [
    "## Splitting a string into a set of words\n",
    "\n",
    "As the aim of the project is to create one-word tags, the strings with more than one word should be split into single words. The words are split at space, hyphen, apostrophe, and dot. After splitting the stings, the words that have less than the specified number of alphanumeric characters are removed. \n",
    "\n",
    "While space is an apparent choice, the reason for choosing the other delimiters are\n",
    "\n",
    "- Hyphen: Generally, the hyphens are used to indicate a link between words or the end of the line, and the em dashes are used to provide an emphasis. However, the observation of the dataset has revealed that their usage is not consistent i.e. the hyphen and the em dash were used interchangeably by the OCR process. To standardize the process, the words are split at the hyphen.\n",
    "- Apostrophe: The apostrophe is mostly used when combining the complementary pronoun and a word starting with a vowel. Thus the word is split at apostrophe and only the keyword is retained.\n",
    "- Dot: In the dataset, a dot was mostly used to write abbreviations. Nevertheless, a word with a dot at the end and the next word in a string are combined. Thus, an attempt is made to split the words around the dot and check to disambiguate them into multiple probable words.\n",
    "\n",
    "Examples: \n",
    "- a.p. moller-maersk to [a.p., moller, maersk]\n",
    "- Aire-sur-l'Adourto to [aire, sur, lâ€™adour] and [aire, sur, adour] (as _l_ is an complementary pronoun)\n",
    "\n",
    "The method of splitting a string designed for this project is as follows\n",
    "\n",
    "1. The string is split in space and hyphen.\n",
    "2. For each substring after split\n",
    "    1. retain the substring if it is not present in the French stop words (obtained from nltk - corpus - stopwords) and has a minimum length after removing the non-alphanumeric characters.\n",
    "    2. If there is an apostrophe in the substring, split the sub-string at the apostrophe. Splitting at apostrophe involves,\n",
    "        1. If the left part of the substring (i.e. the character before the apostrophe) is in complementary pronouns then the only right part of the substring will be a possible substring to return.\n",
    "        2. If the left part of the substring (i.e. the word before the apostrophe) has more than one character and the last character of the first part is in the complementary pronouns then the first part without the last character and the second part as it is will be considered as the possible substrings to return if both the parts are in the list of correctly spelled words.\n",
    "        3. If there is no complementary pronoun before the apostrophe and then the string itself will be possible substrings to return. \n",
    "    3. If any of the substrings in the possible substrings to return contains a dot, then the substrings are split at a dot\n",
    "        1. If the part of the substring after a dot is a stop word, then the substring in the possible sub\\strings to return is updated by removing the characters after the dot.\n",
    "        2. If both the parts of the substring belong to a dictionary then the substring is split into sub substrings without the dot and the original substring in possible tokens to return is replaced by two substrings.\n",
    "    4.  At last, all the possible substrings that are valid are returned.\n",
    "        - A string (word) is said to be valid if it does not belong to a set of stop words and has at least the specified number of alpha numeric characters.\n",
    "\n",
    "### Utility functions to split a string\n",
    "\n",
    "Below are utility functions to split a string. These functions are used to split the strings in creating the list of correctly spelled words and the tokens for the professions. \n",
    "\n",
    "The functions are\n",
    "\n",
    "1. `check_presence`: Returns whether an element is present in a given sequence of objects.\n",
    "2. `check_token_length`: Returns whether the string has the minimum length after removing non-word characters.\n",
    "3. `is_valid_token`: Returns whether the string is valid based on the `:func:check_token_length` and `:func:check_presence` in stopwords.\n",
    "4. `split_metier_strings_to_tokens`: Splits and returns the input strings at space, hyphens, apostrophe after removing stop words.\n",
    "5. `clean_tokens`: Splits the tokens (generated from splitting at space and hyphen) containing an apostrophe and dot to singular tokens if they contain complementary pronouns. Further, splits these tokens at the dot (if they contain any) and validates all of them.\n",
    "6. `apos_split`: The function accepts a string with an apostrophe and splits the string at the apostrophe.\n",
    "7. `complementary_pronouns_check`: Returns the string after the apostrophe, if the character before the apostrophe is a complementary pronoun, else returns None.\n",
    "8. `dot_split`: Returns the list of tokens splitting at a dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0605b0-ddb3-4091-99b1-5b6c4593d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_presence(element: Union[str, FrozenSet], check_in: Union[Set, Dict]) -> bool:\n",
    "    \"\"\"Returns whether an element is present in a given sequence of objects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : str or FrozenSet\n",
    "            The element to check.\n",
    "        check_in : Set or Dict\n",
    "             The sequence of objects to check for the element.\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Boolean indicating the presence (or not) of the element.\n",
    "        \"\"\"\n",
    "    return element in check_in\n",
    "\n",
    "\n",
    "def check_token_length(token: str, minimum_length: int) -> bool:\n",
    "    \"\"\"Returns whether the string has the minimum length after removing non-word characters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        token : str\n",
    "            The string to check.\n",
    "        minimum_length : int\n",
    "             The minimum length of the string without non-alphanumeric characters.\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Boolean indicating whether the string is of minimum length.\n",
    "        \"\"\"\n",
    "    return len(re.sub(\"\\W+\", \"\", token)) >= minimum_length\n",
    "\n",
    "\n",
    "def is_valid_token(token: str, minimum_length: int, stopwords: Set[str]) -> bool:\n",
    "    \"\"\"Returns whether the string is valid based on the :func:`check_token_length` and :func:`check_presence` in stopwords.\n",
    "        A string is valid if it is not in the french stop words and has a length at least of ``minimum_length``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        token : str\n",
    "            The string to check.\n",
    "        minimum_length : int\n",
    "            The minimum length of the string without non-alphanumeric characters.\n",
    "        stopwords: Set[str]\n",
    "            The set of stop words in French.\n",
    "\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            A boolean indicating whether a string is valid.\n",
    "        \"\"\"\n",
    "\n",
    "    # Initialise the validities to True\n",
    "    length_validity, stop_word_validity = True, True\n",
    "\n",
    "    if isinstance(minimum_length, int):\n",
    "        # check the length validity\n",
    "        length_validity = check_token_length(token, minimum_length)\n",
    "\n",
    "    if stopwords:\n",
    "        # check the presence in stop words\n",
    "        stop_word_validity = not check_presence(token, stopwords)\n",
    "\n",
    "    return length_validity & stop_word_validity\n",
    "\n",
    "\n",
    "def complementary_pronouns_check(\n",
    "    apos_splits: List[str],\n",
    "    before_apos_possibilities: Set[str],\n",
    "    minimum_length: int,\n",
    "    stop_words: Set[str],\n",
    ") -> Union[str, None]:\n",
    "    \"\"\"Returns the string after the apostrophe,\n",
    "        if the character before the apostrophe is a complementary pronoun provided in ``before_apos_possibilities``,\n",
    "        else returns None.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        apos_splits : List[str]\n",
    "            The list of words that are generated by splitting a string at the apostrophe.\n",
    "        before_apos_possibilities : Set[str]\n",
    "            A set of words that are considered complementary pronouns.\n",
    "        minimum_length : int\n",
    "            The minimum length of the string without non-alphanumeric characters.\n",
    "        stopwords: Set[str]\n",
    "            The set of stop words in French.\n",
    "\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        possible_token : str\n",
    "            The valid string after the apostrophe.\n",
    "        None\n",
    "            if the character before the apostrophe is not a complementary pronoun or if the string after the apostrophe is not valid.\n",
    "        \"\"\"\n",
    "    if check_presence(apos_splits[0], before_apos_possibilities):\n",
    "        # if first part of the split is in the complementary pronouns (Pronoms complÃ©ments) then the second part is considered as the token\n",
    "        possible_token = apos_splits[1]\n",
    "        if is_valid_token(possible_token, minimum_length, stop_words):\n",
    "            # if the new token is valid, return the new token\n",
    "            return possible_token\n",
    "    return None\n",
    "\n",
    "\n",
    "def dot_split(\n",
    "    after_apos_splits: List[str],\n",
    "    stop_words: Set[str],\n",
    "    correctly_spelled_words: Set[str],\n",
    ") -> List[str]:\n",
    "    \"\"\"Returns the list of tokens splitting at a dot.\n",
    "        If the token after the dot belongs to stop words, then the token is changed to be the token up to the dot.\n",
    "        If the part of the token after the dot does not belong to stop words but both the parts around the dot belong\n",
    "            to the list of correctly spelled words then the token is changed to two sub tokens with parts around the dot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    after_apos_splits : List[str]\n",
    "        The list of words that are generated by splitting a string at apostrophe.\n",
    "    stop_words : Set[str]\n",
    "        The set of stop words in French.\n",
    "    correctly_spelled_words: Set[str]\n",
    "        A set of words with correct spellings (mostly belonging to the French language).\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dot_splitted_toks : List[str]\n",
    "        The result of splitting the tokens (already split at apostrophe) at a dot.\n",
    "    \"\"\"\n",
    "    dot_splitted_toks = []\n",
    "    # empty list to store the possible splits\n",
    "\n",
    "    for ind in range(0, len(after_apos_splits)):\n",
    "        # for each spiltted token at apostrophe\n",
    "        if \".\" in after_apos_splits[ind] and not after_apos_splits[ind][-1] == \".\":\n",
    "            # if the token contains a dot and if the dot it not at the end (a dot at the end can mean that the token is potentially an abbrevation)\n",
    "            dot_split_parts = after_apos_splits[ind].split(\".\", 1)\n",
    "            # split at the first occurence of the dot\n",
    "            if check_presence(dot_split_parts[-1], stop_words):\n",
    "                # if the part after the dot is in the stop words\n",
    "                dot_splitted_toks.append(dot_split_parts[0] + \".\")\n",
    "                # append the token before the dot along with the dot\n",
    "                continue\n",
    "            elif all(\n",
    "                dot_split_tok in correctly_spelled_words\n",
    "                for dot_split_tok in dot_split_parts\n",
    "            ):\n",
    "                # else if the the words around the dot are in the list of correctly spelled words\n",
    "                dot_splitted_toks.extend(dot_split_parts)\n",
    "                continue\n",
    "        # else append the token without splitting at dot\n",
    "        dot_splitted_toks.append(after_apos_splits[ind])\n",
    "\n",
    "    return dot_splitted_toks\n",
    "\n",
    "\n",
    "def apos_split(\n",
    "    token: str,\n",
    "    minimum_length: int,\n",
    "    correctly_spelled_words: Set[str],\n",
    "    stop_words: Set[str],\n",
    ") -> List[str]:\n",
    "    \"\"\"The function accepts a string with an apostrophe and splits the string at the apostrophe.\n",
    "        If the token before the apostrophe belongs to complementary pronouns, then the word after the apostrophe becomes the token.\n",
    "        Else if the last character of the part of the token before the apostrophe is a complementary pronoun\n",
    "            and both the words around the apostrophe (after removing the complementary pronoun before the apostrophe)\n",
    "            belong to the list of correctly spelled words then the token is changed to two sub tokens with parts around the apostrophe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : str\n",
    "        The token to split at the apostrophe.\n",
    "    minimum_length : int\n",
    "            The minimum length of the string without non-alphanumeric characters.\n",
    "    stopwords: Set[str]\n",
    "        The set of stop words in French.\n",
    "    correctly_spelled_words: Set[str]\n",
    "        A set of words with correct spellings.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    apos_split_subtokens : List[str]\n",
    "        The result of splitting the token at an apostrophe.\n",
    "    \"\"\"\n",
    "\n",
    "    apos_split_subtokens = [token]\n",
    "    before_apos_possibilities = set([\"l\", \"d\", \"n\", \"s\", \"m\", \"del\"])\n",
    "    # A set of complementary pronouns that when occured before an apostrophe can be ignored.\n",
    "\n",
    "    # if the token contains an apostrophe, split at its first occurrence.\n",
    "    apos_split = token.split(\"'\", 1)\n",
    "\n",
    "    second_part = complementary_pronouns_check(\n",
    "        apos_split, before_apos_possibilities, minimum_length, stop_words,\n",
    "    )\n",
    "    if second_part:\n",
    "        # If the part before apostrophe is a complementary pronoun, the part after apostrophe will be the new token.\n",
    "        apos_split_subtokens = [second_part]\n",
    "\n",
    "    elif len(apos_split[0]) > 1:\n",
    "        # Else if last character first part of the split is in the complementary pronouns (Pronoms complÃ©ments) then the tokens might have been joined together. If new tokens after removing the complementary pronouns are valid tokens then they are considered as new tokens.\n",
    "        if check_presence(apos_split[0][-1], before_apos_possibilities):\n",
    "            subtokens_around_apos = [apos_split[0][:-1], apos_split[1]]\n",
    "\n",
    "            if correctly_spelled_words and len(subtokens_around_apos) > 1:\n",
    "                # if there is no dot in substrings and all the elements in the split list are in then correctly_spelled_words and return them\n",
    "                if not all(\n",
    "                    check_presence(pos_tok, correctly_spelled_words)\n",
    "                    for pos_tok in subtokens_around_apos\n",
    "                ):\n",
    "                    apos_split_subtokens = [\n",
    "                        ele\n",
    "                        for ele in subtokens_around_apos\n",
    "                        if is_valid_token(ele, minimum_length, stop_words)\n",
    "                    ]\n",
    "    return apos_split_subtokens\n",
    "\n",
    "\n",
    "def clean_tokens(\n",
    "    splitted_token: str,\n",
    "    minimum_length: int,\n",
    "    correctly_spelled_words: Set[str],\n",
    "    stop_words: Set[str],\n",
    ") -> List[str]:\n",
    "    \"\"\"Splits the tokens containing an apostrophe and dot to singular tokens if they contain complementary pronouns.\n",
    "        Further, splits these tokens at the dot (if they contain any) and validates all of them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    splitted_token : str\n",
    "        The string generated due to split at space and hyphen.\n",
    "    minimum_length : int\n",
    "        The minimum length of the string without non-alphanumeric characters.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        A set of words with correct spellings.\n",
    "    stopwords: Set[str]\n",
    "        The set of stop words in French.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of strings containing the substring of the original token that is first split at the apostrophe\n",
    "            and then at a dot or the input token, if it does not contain an apostrophe.\n",
    "        An empty list if the input token is not valid.\n",
    "    \"\"\"\n",
    "\n",
    "    possible_subtokens = []\n",
    "    if is_valid_token(splitted_token, minimum_length, stop_words):\n",
    "        # if the input string is valid\n",
    "\n",
    "        possible_subtokens = [splitted_token]\n",
    "\n",
    "        if \"'\" in splitted_token:\n",
    "            # if the token contains a apostrophe, try splitting at apostrophe and return the valid tokens\n",
    "            possible_subtokens = apos_split(\n",
    "                splitted_token, minimum_length, correctly_spelled_words, stop_words\n",
    "            )\n",
    "\n",
    "        if any(\".\" in pos_split for pos_split in possible_subtokens):\n",
    "            # if the token or the tokens after splitting at apostrophe contains a dot, try splitting at dot and return the valid tokens\n",
    "            possible_subtokens = dot_split(\n",
    "                possible_subtokens, stop_words, correctly_spelled_words\n",
    "            )\n",
    "\n",
    "    return [\n",
    "        ele\n",
    "        for ele in possible_subtokens\n",
    "        if is_valid_token(ele, minimum_length, stop_words)\n",
    "    ]\n",
    "\n",
    "\n",
    "def split_metier_strings_to_tokens(\n",
    "    string_to_split: str,\n",
    "    minimum_length: int,\n",
    "    correctly_spelled_words: Union[Set[str], None],\n",
    "    stop_words: Union[Set[str], None],\n",
    ") -> List[str]:\n",
    "    \"\"\"Splits and returns the input strings at space, hyphens, apostrophe after removing stop words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string_to_split : str\n",
    "        The string to split into tokens.\n",
    "    minimum_length : int\n",
    "        The minimum length of the string without non-alphanumeric characters.\n",
    "    correctly_spelled_words : Union[Set[str], None]\n",
    "        A set of words with correct spellings.\n",
    "        When creating a list of correct words, this parameter should be set to None as there is no list of correct words yet.\n",
    "    stopwords: Union[Set[str], None]\n",
    "        The set of stop words in French.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metier_tokens : List[str]\n",
    "        The list of tokens for the given input string\n",
    "    \"\"\"\n",
    "\n",
    "    # split the input string at space and hyphen\n",
    "    split_space_hyphen = list(filter(None, re.split(\"\\s|-\", string_to_split)))\n",
    "\n",
    "    metier_tokens = []\n",
    "    # a list to store the token for the given input string\n",
    "    for split_token in split_space_hyphen:\n",
    "        # for each split part, check if the token is valid and try to split at apostrophe, if it contains one.\n",
    "        cleaned = clean_tokens(\n",
    "            split_token, minimum_length, correctly_spelled_words, stop_words\n",
    "        )\n",
    "        if cleaned:\n",
    "            # append the valid token to the list\n",
    "            metier_tokens.extend(cleaned)\n",
    "    return metier_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d47f8-d2ba-48f7-ac6c-0749143c288e",
   "metadata": {},
   "source": [
    "## Creation of list of correctly spelled words\n",
    "\n",
    "The list of correctly spelled words plays a key role in the process of creating tags as described in the algorithm. In this section, three sources are used to create a set of words that are assumed to have correct spellings. As these sources not only contain a word, the strings are split at space, hyphen. The apostrophe and the complementary pronoun before the apostrophe are also removed, as the same pre-processing step will also be applied to the professions. Two types of datasets are used to create the list of correctly spelled words. The first type is the language dictionaries obtained from `Morphalou3` (for various French words) and `Prolex-Unitex` (for proper nouns in French). The second one is for the list of street names of Paris obtained from `Open Data Paris`.\n",
    "\n",
    "\n",
    "- Using the dictionaries provided by `Morphalou3` and `Prolex-Unitex` a list of words was created and saved as `words_from_french_language_dictionaries.json`. The keys in the JSON files are root forms of the words (the meaning of root forms is very loosely used here) and the values are a list of words that are derived from the root form. The script used to create the JSON file can be found at `creating_french_dictionary_words_set.ipynb`.\n",
    "- For the names of the streets, the table of streets of Paris is obtained from [Street names table](https://opendata.paris.fr/explore/dataset/denominations-emprises-voies-actuelles/table/?disjunctive.siecle&disjunctive.statut&disjunctive.typvoie&disjunctive.arrdt&disjunctive.quartier&disjunctive.feuille&sort=typo_min). The `denominations-emprises-voies-actuelles.csv` file contains details of the streets of Paris in 28 columns. The `DÃ©nomination complÃ¨te minuscule` columns are read and the strings are split and added to the list of correctly spelled words.\n",
    "\n",
    "\n",
    "### Utility function to read the JSON file and street names file and return the list of correctly spelled words\n",
    "\n",
    "The functions are \n",
    "\n",
    "1. `create_correctly_spelled_words_list_from_dictionaries`: Read and splits the strings in the words from the language dictionary (`category_wise_frech_dictionary.json`) at space, hyphen, and apostrophe and stored as a set of words.\n",
    "2. `create_correctly_spelled_words_list_from_street_names`: Read and splits the street names in the CSV file of street names of the given column name at space, hyphen, and apostrophe and stored as a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e6f88-6a3e-43fc-bda2-ec511d87549c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_correctly_spelled_words_list_from_dictionaries(\n",
    "    dictionary_file_path: str,\n",
    "    minimum_length: int,\n",
    "    include_original_string: bool = False,\n",
    ") -> Set[str]:\n",
    "    \"\"\"Read and splits the strings in the words from the language dictionary at space, hyphen, and apostrophe\n",
    "        and stored them as a set of words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dictionary_file_path : str\n",
    "        The path to the JSON dictionary file with keys as root words and values as a list of derived words.\n",
    "    minimum_length : int\n",
    "        The minimum length of the string without non-alphanumeric characters.\n",
    "    include_original_string : bool, optional\n",
    "        After splitting the derived words, the derived word is also added to the list of correctly spelled words if this is set to True,\n",
    "        by default False.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dictionary_words_correctly_spelled : Set[str]\n",
    "        A set of words that can be considered correctly spelled.\n",
    "    \"\"\"\n",
    "    # load the json file\n",
    "    with open(dictionary_file_path, encoding=\"utf8\") as fr_dict:\n",
    "        dictionary = json.load(fr_dict)\n",
    "\n",
    "    dictionary_words_correctly_spelled = set()\n",
    "\n",
    "    for lemme, flexions in dictionary.items():\n",
    "        # for each root\n",
    "        flexions = flexions + [lemme]\n",
    "        for flexion in flexions:\n",
    "            flexion_ = flexion.lower()\n",
    "            splitted_flexion = split_metier_strings_to_tokens(\n",
    "                flexion_,\n",
    "                minimum_length,\n",
    "                correctly_spelled_words=set(),\n",
    "                stop_words=set(),\n",
    "            )\n",
    "            if include_original_string:\n",
    "                if not check_presence(flexion_, set(splitted_flexion)):\n",
    "                    splitted_flexion.append(flexion_)\n",
    "            dictionary_words_correctly_spelled.update(splitted_flexion)\n",
    "\n",
    "    return dictionary_words_correctly_spelled\n",
    "\n",
    "\n",
    "def create_correctly_spelled_words_list_from_street_names(\n",
    "    street_names_csv_file: str,\n",
    "    sep: str,\n",
    "    column_name: str,\n",
    "    minimum_length: int,\n",
    "    include_original_string: bool = False,\n",
    ") -> Set[str]:\n",
    "    \"\"\"Read and splits the street names in the CSV file of street names of the given column name at space, hyphen,\n",
    "        and apostrophe and stored as a set of words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    street_names_csv_file : str\n",
    "        The path to the CSV file with street names.\n",
    "    seperator : str\n",
    "        The seperator of the csv file.\n",
    "    column_name : str\n",
    "        The name of the column in the CSV file with street names to read.\n",
    "    minimum_length : int\n",
    "        The minimum length of the string without non-alphanumeric characters.\n",
    "    include_original_string : bool, optional\n",
    "        After splitting the street name, the street name is also added to the list of correctly spelled words if this is set to True,\n",
    "        by default False.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    street_names_correctly_spelled : Set[str]\n",
    "        A set of words that can be considered correctly spelled.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    col_list = [column_name]\n",
    "\n",
    "    street_names_df = pd.read_csv(\n",
    "        street_names_csv_file,\n",
    "        dtype={column_name: \"str\"},\n",
    "        usecols=col_list,\n",
    "        sep=sep,\n",
    "        header=0,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    street_names_df[column_name] = street_names_df[column_name].str.lower()\n",
    "\n",
    "    street_names_correctly_spelled = set()\n",
    "\n",
    "    for street_name in street_names_df[column_name].unique():\n",
    "        splitted_street_name = split_metier_strings_to_tokens(\n",
    "            street_name, minimum_length, correctly_spelled_words=set(), stop_words=set()\n",
    "        )\n",
    "\n",
    "        if include_original_string:\n",
    "            if not check_presence(street_name, set(splitted_street_name)):\n",
    "                splitted_street_name.append(street_name)\n",
    "\n",
    "        street_names_correctly_spelled.update(splitted_street_name)\n",
    "\n",
    "    return street_names_correctly_spelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3245d-fb92-4c82-a307-cd917b2df69b",
   "metadata": {},
   "source": [
    "The list of correctly spelled words from dictionaries and street names together is stored in `words_with_correct_spellings` and is obtained in the next cell while allowing only tokens with at least 3 alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f40d0-a7f4-44f3-a522-c4761e5b8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the set of words from the language dictionary\n",
    "words_from_lang_dictionary = create_correctly_spelled_words_list_from_dictionaries(\n",
    "    dictionary_file_path=\"./../data/intermediate_steps/words_from_french_language_dictionaries.json\",\n",
    "    minimum_length=MINIMUM_TOKEN_LENGTH,\n",
    "    include_original_string=False,\n",
    ")\n",
    "\n",
    "'''\n",
    "# get the set of words from the street names\n",
    "words_from_street_names = create_correctly_spelled_words_list_from_street_names(\n",
    "    street_names_csv_file=\"./../data/external_data/Street-names/denominations-emprises-voies-actuelles.csv\",\n",
    "    sep=\";\",\n",
    "    column_name=\"DÃ©nomination complÃ¨te minuscule\",\n",
    "    minimum_length=MINIMUM_TOKEN_LENGTH,\n",
    "    include_original_string=False,\n",
    ")\n",
    "\n",
    "\n",
    "# combine the two sets to get a single set of correctly spelled words.\n",
    "words_with_correct_spellings = set.union(\n",
    "    words_from_lang_dictionary, words_from_street_names\n",
    ")\n",
    "'''\n",
    "\n",
    "words_with_correct_spellings = set(\n",
    "    filter(lambda word: \".\" not in word, words_from_lang_dictionary)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86602980-8657-4397-84ad-50cc8c620a71",
   "metadata": {},
   "source": [
    "### Saving\n",
    "\n",
    "The set of correctly spelled words is stored as a pickle file to the disk to avoid re creating the set every time and also to be used in another notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b9288-ea86-4219-acc2-6094f46838c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/words_with_correct_spellings.pickle\", \"wb\"\n",
    ") as outfile:\n",
    "    pickle.dump(words_with_correct_spellings, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9499ace-82fe-4677-8741-307ed0ba9683",
   "metadata": {},
   "source": [
    "### Reading\n",
    "\n",
    "The set of correctly spelled words stored as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6ae7b-a2c6-4440-8479-5c39eabb7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/words_with_correct_spellings.pickle\", \"rb\"\n",
    ") as outfile:\n",
    "    words_with_correct_spellings = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b04afa-fde5-4a71-b087-64e61005a204",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Combine the words broken by mistake during OCR (Step 1)\n",
    "\n",
    "In the first step, we try to combine the words that were broken by mistake. The algorithm is as follows\n",
    "\n",
    "1. Split the profession string at space, a hyphen, apostrophe, and dot to create a token set for each unique profession.\n",
    "2. Create a counter for each unique token.\n",
    "3. Go over each token set, if either of the consecutive entries is not in the list of correct words and either of them has a frequency less than a threshold then try to concatenate these entries.\n",
    "4. Check if the concatenated word or a word that is similar with a certain threshold exists in the already existing tokens.\n",
    "    1. If the exact concatenated word is not present in the existing tokens, then the **process** module of [RapidFuzz](https://github.com/maxbachmann/RapidFuzz) library is used to extract the closet word from a list of words. The `process.extractOne` function is used for this purpose. It accepts the arguments in the following order:\n",
    "        - The seed word (concatenated word)\n",
    "        - The list of words to search in (The list of unique tokens)\n",
    "        - A processor to pre-process the strings\n",
    "        - A scorer to calculate the similarity between strings (fuzz.ratio)\n",
    "        - The minimum similarity between the seed word and the word in the list of words is to be selected and returned.\n",
    "    \n",
    "5. If such a word exists and if the frequency of such a word is greater than or equal to all the individual consecutive tokens used to obtain it, then replace the consecutive tokens with the found word.\n",
    "6. Update the token set for the profession using the words obtained after combining.\n",
    "\n",
    "### Utility functions to combine the mistakenly broken words\n",
    "\n",
    "The functions are\n",
    "\n",
    "1. `simple_processor`: A string processor to return the same string as input.\n",
    "2. `tokens_after_combining_low_freq_consecutive_tokens`: The function accepts a dictionary with key as the profession and value as the list of corresponding tokens, a threshold frequency to determine if a token is less frequent and the minimum similarity for the word to replace the less frequent & incorrectly spelled tokens and returns the dictionary with key as the profession and value as the list of corresponding updated tokens.\n",
    "3. `get_possible_missplit_tokens`: The function accepts a list of tokens along with their counter, a threshold frequency to determine if a token is less frequent, and returns a list of a sublist of tokens that can be combined if either of the consecutive entries is not in the list of correctly spelled words and either of them has a frequency less than a threshold.\n",
    "2. `get_possible_combined_word`: The function accepts a list of words that have low frequency and are not in the list of correctly spelled words and returns the list containing possible replacement and similarity score or None if there is no word within the given similarity threshold.\n",
    "3. `get_updated_tokens_after_combining`: This function accepts the current list of tokens for a profession string, the list of possible combinations, and returns the updated list of tokens for the profession after combining the tokens. The combinations are applied in the order of the similarity score and only one combination is applied per token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120139e-16a5-4056-8a0c-f6ae4660b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_processor(token: str) -> str:\n",
    "    \"\"\"A string processor to return the same string as input.\n",
    "        This dummy processor is used to avoid the default processor of the Rapidfuzz module to calculate string similarity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : str\n",
    "        The input string to process.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The output string same as the input string.\n",
    "    \"\"\"\n",
    "    return token\n",
    "\n",
    "def get_possible_combined_word(\n",
    "    tokens_to_combine: List[str],\n",
    "    unique_tokens: Set[str],\n",
    "    previous_combinations: Dict[\n",
    "        FrozenSet[List[str]], Set[Tuple[Tuple[str], str, float]]\n",
    "    ],\n",
    "    token_counter_full_data: Counter_type[str],\n",
    "    minimum_threshold_for_replacement_word: float,\n",
    "    processor: Optional[Callable[[str], str]] = simple_processor,\n",
    "    scorer: Optional[Callable[[str, str], float]] = fuzz.ratio,\n",
    ") -> Union[Tuple[str, float], None]:\n",
    "    \"\"\"The function accepts a list of words that have low frequency and are not in the list of correctly spelled words,\n",
    "        the unique tokens in the dataset, the minimum similarity for the word to replace the tokens with,\n",
    "        and other keyword arguments and returns the list containing possible replacement and similarity score\n",
    "        or None if there is no word within the given similarity threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_to_combine : List[str]\n",
    "        The list of tokens that can be combined.\n",
    "    unique_tokens : Set[str]\n",
    "        The unique tokens that are present in the dataset.\n",
    "    previous_combinations : Dict[Frozenset[List[str]], Set[Tuple[Tuple[str], str, float]]]\n",
    "        A python dictionary that stores the previously combined tokens. The key is a frozenset of the list of tokens that can be combined.\n",
    "        The value is a set of tuples that contains three entries.\n",
    "            First, the tokens combined (as a tuple to preserve the order of token appearance),\n",
    "            second, the string that the tokens are changed to, and\n",
    "            lastly, the similarity score between the string obtained by concatenating the strings in the ``tokens_to_combine`` list\n",
    "                and the string that the tokens are changed to.\n",
    "    token_counter_full_data : Counter_type[str]\n",
    "        A counter object that holds the number of times a token has appeared in the dataset.\n",
    "    minimum_threshold_for_replacement_word : float\n",
    "        The minimum similarity score to replace the tokens in the ``tokens_to_combine`` with the word found in the ``unique_tokens``.\n",
    "    processor :  Optional[Callable[[str], str]] = simple_processor, optional\n",
    "        The preprocessing to be performed on the string before computing the similarity score, by default simple_processor.\n",
    "    scorer : Optional[Callable[[str, str], float]], optional\n",
    "        The function to be used to compute similarity, by default fuzz.ratio.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[str, float]\n",
    "        If the tokens in the ``tokens_to_combine`` have a suitable replacement then a list with the replacement word,\n",
    "            its similarity score with the string obtained by concatenating tokens in ``tokens_to_combine``.\n",
    "    None\n",
    "        If there is no suitable replacement for the tokens in ``tokens_to_combine`` then None is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # concatenate the words in ``tokens_to_combine``\n",
    "    possible_combination = \"\".join(tokens_to_combine)\n",
    "\n",
    "    if check_presence(possible_combination, unique_tokens):\n",
    "        # if the concatenated word is already in the dataset, then that word is the possible replacement and the similarity score is 100 as the exact word is present.\n",
    "        replacement = [possible_combination, 100, \"\"]\n",
    "    else:\n",
    "        # if the concatenated word is not in the dataset, then first check if the same words were combined earlier in this process.\n",
    "        set_tokens = frozenset(tokens_to_combine)\n",
    "        # Check if the frozenset of the ``tokens_to_combine`` is present in the previous_combinations``\n",
    "        if check_presence(set_tokens, previous_combinations):\n",
    "            # if the set of tokens is present, then check if the tokens were present in the same order as in the ``tokens_to_combine``.\n",
    "            for combinations in previous_combinations[set_tokens]:\n",
    "                if combinations[0] == tokens_to_combine:\n",
    "                    # if the order of the tokens is same then\n",
    "                    if None in combinations:\n",
    "                        # if this combination is rejected earlier, return None\n",
    "                        return None\n",
    "                    # return the replacement word and its similarity score.\n",
    "                    return combinations[-2:]\n",
    "        # if the tokens were not combined earlier or not in the same order, then extract the closest match from the unique tokens of the dataset with the given similarity threshold.\n",
    "        # The process returns None if no match is found.\n",
    "        replacement = process.extractOne(\n",
    "            possible_combination,\n",
    "            unique_tokens,\n",
    "            processor=simple_processor,\n",
    "            scorer=fuzz.ratio,\n",
    "            score_cutoff=minimum_threshold_for_replacement_word,\n",
    "        )\n",
    "\n",
    "    if replacement:\n",
    "        # If there is a replacement (either the concatenated word or the word extracted from the unique tokens)\n",
    "        if not check_presence(replacement[0], tokens_to_combine):\n",
    "            # check if the replacement word is not one of the tokens to combine. If so, then the tokens are not replaced. Otherwise,\n",
    "            freq_of_replacement = token_counter_full_data[replacement[0]]\n",
    "            freq_of_org_tokens = [\n",
    "                token_counter_full_data[tok_to_comb]\n",
    "                for tok_to_comb in tokens_to_combine\n",
    "            ]\n",
    "            if all(\n",
    "                freq_of_replacement >= freq_org_token\n",
    "                for freq_org_token in freq_of_org_tokens\n",
    "            ):\n",
    "                # check if the frequency of the replacement word is greater than or equal to the frequency of the words in the ``tokens_to_combine``.\n",
    "                # if yes, then return the replacement word and its frequency. The last entry of ``replacement`` is the index of the word in ``unique_tokens`` set. As it is not needed, only the first two entries which are the word and the similarity are returned.\n",
    "                return replacement[:-1]\n",
    "    # Return None, if a replacement is not found.\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_updated_tokens_after_combining(\n",
    "    current_tokens: List[str],\n",
    "    possible_combinations: List[Tuple[Tuple[str], str, float]],\n",
    ") -> List[str]:\n",
    "    \"\"\"This function accepts the current list of tokens for a profession string, the list of possible combinations,\n",
    "        and returns the updated list of tokens for the profession after combining the tokens as per the ``possible combinations``.\n",
    "        The combinations are applied in the order of the similarity score and only one combination is applied per token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_tokens : List[str]\n",
    "        The list of tokens for a profession that are generated by splitting the string.\n",
    "    possible_combinations : List[Tuple[Tuple[str], str, float]]\n",
    "        The list of the possible combinations obtained by trying to combine words in the ``current_tokens`` that are less frequent\n",
    "            and/or do not appear in the list of correctly spelled words.\n",
    "        The list of possible combinations has 3 elements, the first is the tokens that are to be combined,\n",
    "            second, the word to replace the tokens, and\n",
    "            third is the similarity score of the replacement word with the word obtained by concatenating tokens in the first element.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        The updated list of tokens for the profession that is obtained by combining words according to ``possible_combinations``.\n",
    "    \"\"\"\n",
    "\n",
    "    # sort the possible_combinations in descending order of their similarity\n",
    "    sorted_combinations = sorted(\n",
    "        possible_combinations, key=lambda x: x[-1], reverse=True\n",
    "    )\n",
    "\n",
    "    # deepcopy of the current tokens\n",
    "    updated_tokens = copy.deepcopy(current_tokens)\n",
    "\n",
    "    changed_indv_tokens = set()\n",
    "    # The tokens replaced for this tokens list are stored in the ``changed_indv_tokens`` set, to avoid replacing the same token multiple times.\n",
    "\n",
    "    for combination in sorted_combinations:\n",
    "        # for each possible combination\n",
    "\n",
    "        start_ind, end_ind = None, None\n",
    "        sub_tokens = list(combination[0])\n",
    "\n",
    "        if not any(check_presence(item, changed_indv_tokens) for item in sub_tokens):\n",
    "            # If the tokens to combine are not already combined in another possible combination for the same token list,\n",
    "\n",
    "            sub_tokens_len = len(sub_tokens)\n",
    "\n",
    "            for itr in range(len(updated_tokens)):\n",
    "\n",
    "                if updated_tokens[itr : itr + sub_tokens_len] == sub_tokens:\n",
    "                    # find the indices where the tokens to be combine start and end.\n",
    "                    start_ind = itr\n",
    "                    end_ind = itr + sub_tokens_len - 1\n",
    "\n",
    "                    # Replace the tokens with the word\n",
    "                    updated_tokens = (\n",
    "                        updated_tokens[:start_ind]\n",
    "                        + [combination[-2]]\n",
    "                        + updated_tokens[end_ind + 1 :]\n",
    "                    )\n",
    "\n",
    "                    # add the replaced tokens to the ``changed_indv_tokens``set.\n",
    "                    changed_indv_tokens.update(sub_tokens)\n",
    "                    break\n",
    "    # After updating the all possible tokens, return the new list of tokens.\n",
    "    return updated_tokens\n",
    "\n",
    "\n",
    "def get_possible_missplit_tokens(\n",
    "    current_tokens: List[str],\n",
    "    token_counter_full_data: Counter_type[str],\n",
    "    correctly_spelled_words: Set[str],\n",
    "    min_tok_frequency: int,\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"The function accepts a list of tokens along with their counter, their presence in the list of correctly spelled words\n",
    "        and a threshold frequency to determine if a token is less frequent and\n",
    "        returns a list of a sublist of tokens that can be combined if either of the consecutive entries is\n",
    "        not in the list of correctly spelled words and either of them has a frequency less than a threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_tokens : List[str]\n",
    "        The list of tokens for a profession string obtained by splitting the string.\n",
    "    token_counter_full_data : Counter_type[str]\n",
    "        A counter object that holds the number of times a token has appeared in the dataset.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to list of correctly spelled words.\n",
    "    min_tok_frequency : int\n",
    "        The threshold frequency for a token to be considered as less frequent.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[List[str]]\n",
    "        The list of sublists of tokens that can be combined into one.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialise list to store potentially broken words\n",
    "    sub_set_tokens = []\n",
    "\n",
    "    for itr in range(len(current_tokens) - 1):\n",
    "\n",
    "        # for each token compare it with its next token\n",
    "\n",
    "        curr_tok = current_tokens[itr]\n",
    "        next_tok = current_tokens[itr + 1]\n",
    "\n",
    "        if check_presence(curr_tok, correctly_spelled_words) and check_presence(\n",
    "            next_tok, correctly_spelled_words\n",
    "        ):\n",
    "            if (token_counter_full_data[curr_tok] >= min_tok_frequency) and (\n",
    "                token_counter_full_data[next_tok] >= min_tok_frequency\n",
    "            ):\n",
    "                # ignore the pair of tokens if both of them are in the list of correctly spelled words and both of them have frequency greater than the threshold for minimum frequency\n",
    "                continue\n",
    "        # else append the pair of tokens to potentially broken words\n",
    "        sub_set_tokens.append([curr_tok, next_tok])\n",
    "\n",
    "    # The abpve loop only searches for words broken into two parts, however, words can be broken into more than two parts. This next step will combine and create new paris of possible broken words. Two potentailly broken words lists are combined if the last element of one list is same as the first element of the another list. The loop is excuted as long as there are no possible combinations of broken words.\n",
    "    search_for_additional_combs = True\n",
    "\n",
    "    while search_for_additional_combs:\n",
    "        initial_cobinations = copy.deepcopy(sub_set_tokens)\n",
    "        # copy the current list of broken words\n",
    "\n",
    "        new_combs = []\n",
    "        # list to store the new combinations of broken words\n",
    "        for itr in range(len(initial_cobinations) - 1):\n",
    "            if sub_set_tokens[itr][1:] == sub_set_tokens[itr + 1][:-1]:\n",
    "                # if the last part of the current list is same as the first part of next list\n",
    "                new_comb = sub_set_tokens[itr] + sub_set_tokens[itr + 1][-1:]\n",
    "                # combine them to create a new combination\n",
    "                if not check_presence(new_comb, sub_set_tokens):\n",
    "                    # if the combination is not already added, append it to the list of new combination of broken words\n",
    "                    new_combs.append(new_comb)\n",
    "\n",
    "        if new_combs:\n",
    "            # update the list of potentially broken words\n",
    "            sub_set_tokens += new_combs\n",
    "            if len(new_combs) == 1:\n",
    "                search_for_additional_combs = False\n",
    "        else:\n",
    "            # stop the search for potentially broken words if there are no more lists to combine.\n",
    "            search_for_additional_combs = False\n",
    "\n",
    "    return sub_set_tokens\n",
    "\n",
    "\n",
    "def tokens_after_combining_low_freq_consecutive_tokens(\n",
    "    metier_token_mapping: Dict[str, List[str]],\n",
    "    unique_tokens: Set[str],\n",
    "    token_counter_full_data: Counter_type[str],\n",
    "    correctly_spelled_words: Set[str],\n",
    "    min_tok_frequency: int,\n",
    "    minimum_threshold_for_replacement_word: float,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"The function accepts a dictionary with key as the profession and value as the list of corresponding tokens,\n",
    "        along with the list of all unique tokens and their counter, their presence in the list of correctly spelled words,\n",
    "        a threshold frequency to determine if a token is less frequent and\n",
    "        the minimum similarity for the word to replace the less frequent & incorrectly spelled tokens and\n",
    "        returns the dictionary with key as the profession and value as the list of corresponding updated tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metier_token_mapping : Dict[str, List[str]]\n",
    "        A python dictionary with the key as the profession string obtained from the dataset and the value is a list of tokens obtained by splitting the string. \n",
    "    unique_tokens : Set[str]\n",
    "        A set of unique tokens obtained from all the profession strings in the dataset.\n",
    "    token_counter_full_data : Counter_type[str]\n",
    "        A counter object that holds the number of times a token has appeared in the dataset\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    min_tok_frequency : int\n",
    "        The threshold frequency for a token to be considered as less frequent.\n",
    "    minimum_threshold_for_replacement_word : float\n",
    "        The minimum similarity score to replace the tokens in the ``tokens_to_combine`` with the word found in the ``unique_tokens``.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[str]]\n",
    "        A python dictionary with the key as the profession string obtained from the dataset and\n",
    "        the value is a list of tokens updated after combining the possibly wrongly split words. \n",
    "    \"\"\"\n",
    "\n",
    "    earlier_combinations = {}\n",
    "    # a dictionary to store the previously combined tokens. it will help in reducing the run time.\n",
    "\n",
    "    updated_metier_token_mapping = {}\n",
    "    # a dictionary to store the new profession string and tokens list mapping.\n",
    "\n",
    "    updated_tokenlists = {}\n",
    "    # a dictionary to store the tokens for a profession strings as keys and the updated list of tokens for the token set in  the key. This will help reducing the run time for two profession strings that will have same tokens after splitting the string.\n",
    "\n",
    "    for unique_met_string, tokens in tqdm_notebook(metier_token_mapping.items()):\n",
    "        # for each unique profession string and its tokens\n",
    "\n",
    "        updated_tokens = tokens\n",
    "        # set the updated tokens to be the current list of tokens (to return in case no tokens are not combined.)\n",
    "\n",
    "        set_tokens = frozenset(tokens)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            # if there is more than one token, otherwise there is no interest of combining the tokens at all\n",
    "\n",
    "            found_updated_tokens = False\n",
    "            if check_presence(set_tokens, updated_tokenlists):\n",
    "                # check if the same set of tokens are already updated for a different profession string.\n",
    "                for toks, updtoks in updated_tokenlists[set_tokens]:\n",
    "                    if toks == tokens:\n",
    "                        # not only the set of strings should match but also the order of the tokens in both the lists should be same.\n",
    "                        updated_tokens = updtoks\n",
    "                        found_updated_tokens = True\n",
    "                        break\n",
    "\n",
    "            if not found_updated_tokens:\n",
    "                # if the list of  tokens are not previously merged, get the list of potentially broken words\n",
    "                sub_set_tokens = get_possible_missplit_tokens(\n",
    "                    tokens,\n",
    "                    token_counter_full_data,\n",
    "                    correctly_spelled_words,\n",
    "                    min_tok_frequency,\n",
    "                )\n",
    "                possible_combinations = (\n",
    "                    []\n",
    "                )  # a list to store the possible combinations of tokens\n",
    "                for combinable_tokens in sub_set_tokens:\n",
    "                    # for each list potentially broken words, get a replacement word\n",
    "\n",
    "                    replacement = get_possible_combined_word(\n",
    "                        combinable_tokens,\n",
    "                        unique_tokens,\n",
    "                        earlier_combinations,\n",
    "                        token_counter_full_data,\n",
    "                        minimum_threshold_for_replacement_word,\n",
    "                    )\n",
    "\n",
    "                    if replacement:\n",
    "                        # if there is a replacement word, then append it to the list of possible combinations\n",
    "                        possible_combinations.append(\n",
    "                            (tuple(combinable_tokens), replacement[0], replacement[1])\n",
    "                        )\n",
    "                    else:\n",
    "                        # even if there is no replacement word, append None to the possible combinations as this combination will not be searched in other profession strings.\n",
    "                        possible_combinations.append(\n",
    "                            (tuple(combinable_tokens), None, None)\n",
    "                        )\n",
    "\n",
    "                filtered_possible_combinations = [\n",
    "                    pos_comb\n",
    "                    for pos_comb in possible_combinations\n",
    "                    if None not in pos_comb\n",
    "                ]\n",
    "                # filter the combinations that are not void\n",
    "\n",
    "                if filtered_possible_combinations:\n",
    "                    # if there are any possible combinations, update the current tokens list.\n",
    "                    updated_tokens = get_updated_tokens_after_combining(\n",
    "                        tokens, filtered_possible_combinations\n",
    "                    )\n",
    "\n",
    "                # using the list of possible combinations for this profession string, update the previous_combinations dictionary to remember the merges\n",
    "                for combination in possible_combinations:\n",
    "                    # print(\"combination: \", combination, type(combination))\n",
    "                    set_combined_toks = frozenset(combination[0])\n",
    "                    if not check_presence(set_combined_toks, earlier_combinations):\n",
    "                        earlier_combinations[set_combined_toks] = set()\n",
    "                    if not check_presence(\n",
    "                        combination, earlier_combinations[set_combined_toks]\n",
    "                    ):\n",
    "                        earlier_combinations[set_combined_toks].update([combination])\n",
    "\n",
    "        # update the mapping of profession string to tokens updated after combining tokens (if combined, otherwise the original set of tokens are assigned)\n",
    "        updated_metier_token_mapping[unique_met_string] = updated_tokens\n",
    "\n",
    "        # update the dictionary that maintains the updated list of tokens based on the token lists.\n",
    "        if not check_presence(set_tokens, updated_tokenlists):\n",
    "            updated_tokenlists[set_tokens] = []\n",
    "        updated_tokenlists[set_tokens].append([tokens, updated_tokens])\n",
    "\n",
    "    return updated_metier_token_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522b2bc-1a0c-482f-a8b4-4ebd25373b44",
   "metadata": {},
   "source": [
    "### Reading the Data after cleaning for special characters\n",
    "\n",
    "After performing some cleaning on the special characters, the data set is stored at `all_paris_jobs_splchar_cleaned.csv`. \n",
    "\n",
    "The CSV file contains 10 columns and their description is \n",
    "1. `doc_id`: The unique id of the document of Gallica.\n",
    "2. `page`: The page in which the entry is present in the document.\n",
    "3. `row`: The row of the entry on the page.\n",
    "4. `Nom`: The name of the person.\n",
    "5. `mÃ©tier_original`: The profession before cleaning the special characters.\n",
    "6. `rue`: The name of the street in the address of the person.\n",
    "7. `numÃ©ro`: The number in the street in the address of the person.\n",
    "8. `annee`: The year in which the entry is published.\n",
    "9. `gallica_page`: The page number adjusted from the `page` column that can be used on the Gallica.\n",
    "10. `mÃ©tier`: The profession string after cleaning special characters\n",
    "\n",
    "In the next cell, this file is read as a data frame, and the rows that have a `mÃ©tier` less than 3 characters are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_jobs_spl_clean = pd.read_csv(\"./../data/strict_addressing.csv\", names=[\"doc_id\", \"page\", \"row\", \"Nom\", \"mÃ©tier_original\", \"rue\", \"numÃ©ro\" , \"annee\"],\n",
    "                             dtype={\"doc_id\":'str', \"page\":'int', \"row\":'str', \"Nom\":'str', \"mÃ©tier\":'str', \"rue\":'str', \"numÃ©ro\":'str', \"annee\":'str'},\n",
    "                             header=0, encoding=\"utf-8\")\n",
    "paris_jobs_spl_clean[\"mÃ©tier\"] = paris_jobs_spl_clean[\"mÃ©tier_original\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267198f-59d1-4ce3-9801-3679cbb04cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''paris_jobs_spl_clean = pd.read_csv(\n",
    "    \"./../data/intermediate_steps/all_paris_jobs_splchar_cleaned.csv\",\n",
    "    dtype={\n",
    "        \"doc_id\": \"str\",\n",
    "        \"page\": \"str\",\n",
    "        \"row\": \"str\",\n",
    "        \"Nom\": \"str\",\n",
    "        \"mÃ©tier_original\": \"str\",\n",
    "        \"rue\": \"str\",\n",
    "        \"numÃ©ro\": \"str\",\n",
    "        \"annee\": \"str\",\n",
    "        \"gallica_page\": \"str\",\n",
    "        \"mÃ©tier\": \"str\",\n",
    "    },\n",
    "    header=0,\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "'''\n",
    "\n",
    "paris_jobs_spl_clean.dropna(subset=[\"mÃ©tier\"], inplace=True)\n",
    "\n",
    "# Removing the rows with mÃ©tier column having less than 3 characters.\n",
    "paris_jobs_spl_clean = paris_jobs_spl_clean[\n",
    "    ~(paris_jobs_spl_clean[\"mÃ©tier\"].str.len() < MINIMUM_TOKEN_LENGTH)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eaa0d6-7c6d-46e9-9b99-da8eac739518",
   "metadata": {},
   "source": [
    "After reading the data, \n",
    "\n",
    "1. create a python dictionary (`unique_metier_split_mapping`) with key as the unique profession in the dataset and the values are a list of tokens obtained by splitting the profession string without a minimum length requirement or filtering the stopwords as the aim, in this case, is to meld the words that are broken into multiple words during the OCR process.\n",
    "2. Create a set of unique tokens (`unique_tokens_before_step_1`) from all the professions and a counter (`tokens_counter_before_step_1`) to hold the frequency of their appearance.\n",
    "3. Create a set of tokens that are in the set of correct words (`tokens_in_correct_words_before_step_1`) from the set of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00ae54-43e5-49a6-a8e3-2db263bd0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store the profession string and corresponding tokens\n",
    "unique_metier_split_mapping = {\n",
    "    unique_met_string: split_metier_strings_to_tokens(\n",
    "        unique_met_string,\n",
    "        minimum_length=None,\n",
    "        correctly_spelled_words=set(),\n",
    "        stop_words=set(),\n",
    "    )\n",
    "    for unique_met_string in paris_jobs_spl_clean[\"mÃ©tier\"].unique()\n",
    "}\n",
    "\n",
    "# get the list of all tokens\n",
    "individual_tokens_full_data = [\n",
    "    tok\n",
    "    for met_string in paris_jobs_spl_clean[\"mÃ©tier\"]\n",
    "    for tok in unique_metier_split_mapping[met_string]\n",
    "]\n",
    "\n",
    "# create a Counter object on the list of all tokens\n",
    "tokens_counter_before_step_1 = Counter(individual_tokens_full_data)\n",
    "\n",
    "# get the keys of the counter object as they will be unique entries and store them as a set.\n",
    "unique_tokens_before_step_1 = set(tokens_counter_before_step_1.keys())\n",
    "\n",
    "# create a set of tokens in list of correct words\n",
    "tokens_in_correct_words_before_step_1 = unique_tokens_before_step_1.intersection(\n",
    "    words_with_correct_spellings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16548c98-e8a2-4a31-a3c2-b05a230866a4",
   "metadata": {},
   "source": [
    "### Combining the mistakenly split tokens\n",
    "\n",
    "The `tokens_after_combining_low_freq_consecutive_tokens` function is called to combine the wrongly broken tokens in a profession string and the update tokens for each profession are stored in `unique_metier_split_mapping_consq_merged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a10b1-77a8-47b5-81e7-62cadf2154f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_metier_split_mapping_consq_merged = tokens_after_combining_low_freq_consecutive_tokens(\n",
    "    metier_token_mapping=unique_metier_split_mapping,\n",
    "    unique_tokens=unique_tokens_before_step_1,\n",
    "    token_counter_full_data=tokens_counter_before_step_1,\n",
    "    correctly_spelled_words=tokens_in_correct_words_before_step_1,\n",
    "    min_tok_frequency=MINIMUM_TOKEN_FREQUENCY,\n",
    "    minimum_threshold_for_replacement_word=MIN_THRESHOLD_REPLACE_BROKEN_WORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e024f5-230e-412c-bdb8-ecdaf224688e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Saving the tokens per metier after merging low frequency consecutive tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b130e-396c-4862-93c4-079e3bc2b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/unique_metier_tokens_mapping_after_consq_merged.json\",\n",
    "    \"w\",\n",
    "    encoding=\"utf8\",\n",
    ") as outfile:\n",
    "    json.dump(\n",
    "        unique_metier_split_mapping_consq_merged, outfile, indent=4, ensure_ascii=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b72693-23bf-4dc0-9781-5c74706ee6b2",
   "metadata": {},
   "source": [
    "#### Reading the tokens per metier after merging low frequency consecutive tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092b32e-9a8a-444b-9487-a20ff822f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/unique_metier_tokens_mapping_after_consq_merged.json\",\n",
    "    encoding=\"utf8\",\n",
    ") as f:\n",
    "    unique_metier_split_mapping_consq_merged = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba30ee-1e63-4f74-b894-f454859b2cf1",
   "metadata": {},
   "source": [
    "### Updating the dataset (CSV file) with the new tokens\n",
    "\n",
    "The updated tokens are joined with a space to form a string again (`metier_after_setp_1`) and the data frame is updated with the new profession string under the column `metier_after_S1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6974b-2a1c-406e-877c-2111d9c52009",
   "metadata": {},
   "outputs": [],
   "source": [
    "metier_after_setp_1 = {\n",
    "    met_str: \" \".join(updated_tokens)\n",
    "    for met_str, updated_tokens in unique_metier_split_mapping_consq_merged.items()\n",
    "}\n",
    "paris_jobs_spl_clean[\"metier_after_S1\"] = paris_jobs_spl_clean[\"mÃ©tier\"].map(\n",
    "    metier_after_setp_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d0ac4-3299-48b7-92db-c223e0887bc5",
   "metadata": {},
   "source": [
    "#### Saving the data frame with updated metier after step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7d0d4-3587-4b63-bd53-4357f3fdb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_jobs_spl_clean.to_csv(\n",
    "    \"./../data/intermediate_steps/all_paris_jobs_after_step_1.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6723f7-c089-476e-823e-3331919a994b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split the metier into tokens (Step 2)\n",
    "\n",
    "The crux of the second step is to create a set of tokens for each profession string excluding the stop words. In the process of creating these tokens, some cleaning of the tokens is also performed.\n",
    "- In the first step, the profession strings were split at space, a hyphen, apostrophe, and dot and later joined with space after combining mistakenly split tokens. Hence, in this step, although the same pipeline (the same set of functions are used), the profession strings obtained after step 1 are split at space in practice. However, in this split, the words that have less than 3 alphanumeric characters and those in the stop words list are not included.\n",
    "- Then the words that are potentially indicating a number are removed.\n",
    "- Following the numbers, the words that are not in the list of correct words are converted into their normal form and replaced with the most frequent non-normalized form.\n",
    "- Lastly, the tokens containing a dot in the middle are split into separate tokens when the part of the word represents an abbreviation. This step will result in a set of tokens for each profession.\n",
    "\n",
    "### Reading the data frame with updated profession token sets (as strings) after step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615789e-b3b9-4708-a3ad-10ef5984ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_jobs_after_s1 = pd.read_csv(\n",
    "    \"./../data/intermediate_steps/all_paris_jobs_after_step_1.csv\",\n",
    "    dtype={\n",
    "        \"doc_id\": \"str\",\n",
    "        \"page\": \"str\",\n",
    "        \"row\": \"str\",\n",
    "        \"Nom\": \"str\",\n",
    "        \"mÃ©tier_original\": \"str\",\n",
    "        \"rue\": \"str\",\n",
    "        \"numÃ©ro\": \"str\",\n",
    "        \"annee\": \"str\",\n",
    "        #\"gallica_page\": \"str\",\n",
    "        \"mÃ©tier\": \"str\",\n",
    "        \"metier_after_S1\": \"str\",\n",
    "    },\n",
    "    header=0,\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fda01-37e3-4f0a-ac90-a87f939438e0",
   "metadata": {},
   "source": [
    "### Removing the words shorthanded for numbers\n",
    "\n",
    "The 3 letter words with `re` or `er` were short hands for writing number like `premiÃ¨re` or `troisiÃ¨me` or `quatriÃ¨me`. These will be removed. The entries with `re` or `er` present are `lre`,`fer`,`ire`,`ler`,`Ä¯re`,`tre`,`ser`,`pre`,`fre`,`!re`,`Ã¨re`,`are`,`ier`,`bre`,`jre`,`jer`,`der`,`mer`,`her`,`(re`,`nre`,`gre`,`ere`,`Ä«re`,`cer`,`ter`,`per`,`ger`,`Ã¬re`,`ver`,`yre`,`qre`,`cre`,`vre`,`mre`,`ner`,`Ã­re`,`dre`,`rer`,`ber`,`ure`,`Ã®re`. \n",
    "\n",
    "However, some of these three-letter words are present in the list of correct words or do not indicate a number. The rows containing these words were studied and only the following will be replaced\n",
    "\n",
    "1. All the three-letter words starting with `l` or `q` or `any digit` and ending with `re` or `er`.\n",
    "2. All the three-letter words starting with `i` or `f` or `m` or `(` or `Ä«` or `!` or `Ã¬` or `Ä¯` or  `j` or `Ã®` or `Ã­` and ending with `re`.\n",
    "3. All the three-letter words starting with `g` or `a` or `p` or `b` or `y`and ending with `re` and followed by a space and `i`\n",
    "4. All `ter` words\n",
    "5. All `jer` words followed by a space and `i` or `a`.\n",
    "6. All `tre` words followed by a space and `ins` or `cl` or `iss`.\n",
    "7. All `fer` words followed by a space and `inst` or `ar`.\n",
    "\n",
    "The idea of checking if the word is followed by a certain character comes from the fact that most of the time the numbers are used in the content of mentioning the arrondissement or the instances. Hence the entries are checked for `i` or `a` or `ins` or `iss` or `inst` or `ar` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe2c28-4d3e-453e-8cfe-ac133d2594c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_jobs_after_s1[\"metier_after_S1\"] = paris_jobs_after_s1[\"metier_after_S1\"].str.replace(r\"(?<=\\s)(\\d|l|q)(re|er)(?=\\s)\", \"\", regex=True)\n",
    "\n",
    "paris_jobs_after_s1[\"metier_after_S1\"] = paris_jobs_after_s1[\"metier_after_S1\"].str.replace(r\"(?<=\\s)(i|f|m|\\(|Ä«|!|Ã¬|Ä¯|j|Ã®|Ã­)re(?=\\s)\", \"\", regex=True)\n",
    "\n",
    "paris_jobs_after_s1[\"metier_after_S1\"] = paris_jobs_after_s1[\"metier_after_S1\"].str.replace(r\"(?<=\\s)(g|a|p|b|y)re(?=\\si)\", \"\", regex=True)\n",
    "\n",
    "paris_jobs_after_s1[\"metier_after_S1\"] = paris_jobs_after_s1[\"metier_after_S1\"].str.replace(r\"(?<=\\s)ter(?=\\s)\", \"\", regex=True)\n",
    "\n",
    "paris_jobs_after_s1[\"metier_after_S1\"] = paris_jobs_after_s1[\"metier_after_S1\"].str.replace(r\"(?<=\\s)jer(?=\\s(i|a))\", \"\", regex=True)\n",
    "\n",
    "paris_jobs_after_s1[\"metier_after_S1\"] = paris_jobs_after_s1[\"metier_after_S1\"].str.replace(r\"(?<=\\s)tre(?=\\s(ins|iss|cl))\", \"\", regex=True)\n",
    "\n",
    "paris_jobs_after_s1[\"metier_after_S1\"] = paris_jobs_after_s1[\"metier_after_S1\"].str.replace(r\"(?<=\\s)fer(?=\\s(inst|ar))\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b236df-c885-4b50-a1ad-e85e625359b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split the profession string and filter stop words\n",
    "\n",
    "The metier string is split into a set of tokens without stop words and words with less than 3 alphanumeric characters.\n",
    "\n",
    "The next cell creates a python dictionary (`undecoded_metier_token_mapping`) with key as the unique profession (after combining wrongly broken words) in the dataset and the values is a list of tokens obtained by splitting the profession string with a minimum token length of 3, filtering the stopwords and splitting the words at dot and apostrophe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b1acc-0f76-4226-8cfb-b3ceb3d76b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnormal_metier_token_mapping = {\n",
    "    met_str: split_metier_strings_to_tokens(\n",
    "        string_to_split=met_str,\n",
    "        minimum_length=MINIMUM_TOKEN_LENGTH,\n",
    "        correctly_spelled_words=words_with_correct_spellings,\n",
    "        stop_words=stop_words_french,\n",
    "    )\n",
    "    for met_str in paris_jobs_after_s1[\"metier_after_S1\"].unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd687432-b102-4ab5-beaa-f7c8a39c79e3",
   "metadata": {},
   "source": [
    "#### Saving the splitting of profession strings to tokens (without cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a303cd-64c8-4faf-a590-25a7c63f54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/nonnormal_profession_token_mapping.json\",\n",
    "    \"w\",\n",
    "    encoding=\"utf8\",\n",
    ") as outfile:\n",
    "    json.dump(nonnormal_metier_token_mapping, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a5c92-b9d2-4fe4-8f55-14bce236d086",
   "metadata": {},
   "source": [
    "#### Reading the splitting of profession strings to tokens (without cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a88800-a99d-40d5-804e-c593394ab3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/nonnormal_profession_token_mapping.json\",\n",
    "    encoding=\"utf8\",\n",
    ") as f:\n",
    "    nonnormal_metier_token_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1c90c-6129-430c-beb4-5d736ec85682",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalizing (and Denormalizing) tokens\n",
    "\n",
    "Some tokens in the dataset have non-french alphabets and some words that have ligature are sometimes broken into individual characters and sometimes not. For example, eggs as `Å“ufs` and `oeufs`, sister as `sÅ“ur` or `soeur`, variants of the words vins: vinÅ¡, vÃ¯ns, vá»‹ns, viÃ±s, vÃ®ns, vá»‰ns, vÃ­ns, vinÅŸ, viÅ†s, vÃ¬ns, viÅ„s, vÄ«ns. To correct these tokens, the tokens of the dataset that are not in the set of correctly spelled words shall be converted to normal form using the `unidecode` function. If the normal form is different from the non-normal form then the token is temporarily converted into normal form and then all the tokens having the same normal form are replaced with the non-normal form of the token (having the same normal form) that appears the highest number of times in the dataset.\n",
    "\n",
    "#### Utility function to normalize (and denormalize) tokens.\n",
    "\n",
    "The functions are\n",
    "\n",
    "1. `update_tokens_to_most_frequent_form`: The function accepts the dictionary of profession string to non-normal tokens list, along with the dictionary with the tokens to be modified as key as value as the string that replaces the token and returns the updated mapping of profession string and list of tokens (after selecting the major non-normal form for tokens having the same normal form).\n",
    "2. `get_normal_nonnormal_map`: The function accepts the list of unique tokens obtained from the profession strings and returns a dictionary with the normalized forms of the tokens not in the correct words set.\n",
    "3. `get_denormalised_mapping`:  The function accepts the mapping of normal to a list of the non-normal forms along with the counter for the tokens from the dataset and selects the token with the highest frequency among those having the same normal form to replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14b6d6-6058-43ac-85bf-7280c11ec22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_nonnormal_map(\n",
    "    unique_tokens: List[str], correctly_spelled_words: Set[str]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"The function accepts the list of unique tokens obtained from the profession strings, a set of correctly spelled words,\n",
    "        and returns a dictionary with the normalized forms of the tokens not in the correct words set.\n",
    "        The dictionary contains the normalized form of the token as a key and the list of non-normalized forms of the tokens as value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unique_tokens : List[str]\n",
    "        The list of unique tokens of the dataset.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normal_to_original_list_mapping : Dict[str, List[str]]\n",
    "        A python dictionary with the normalized form of the token as key and the list of non-normalized forms of the tokens as value.\n",
    "    \"\"\"\n",
    "\n",
    "    # empty dictionary to hold the return values\n",
    "    normal_to_original_list_mapping = {}\n",
    "\n",
    "    for tok in unique_tokens:\n",
    "        # for each unique token\n",
    "        if not check_presence(tok, correctly_spelled_words):\n",
    "            # if the token is not in the list of correctly spelled words, get its normalized form\n",
    "            decoded = unidecode(tok)\n",
    "            if decoded != tok:\n",
    "                if not check_presence(decoded, normal_to_original_list_mapping):\n",
    "                    normal_to_original_list_mapping[decoded] = []\n",
    "                # save the inverse mapping of normalized form to non-normalized form in another dictionary.\n",
    "                normal_to_original_list_mapping[decoded].append(tok)\n",
    "    return normal_to_original_list_mapping\n",
    "\n",
    "\n",
    "def get_denormalised_mapping(\n",
    "    normal_to_original_list_mapping: Dict[str, List[str]],\n",
    "    tokens_counter_before_normalization: Counter_type[str],\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"The function accepts the mapping of normal to a list of the non-normal forms along with the counter for the tokens\n",
    "        from the dataset and selects the token with the highest frequency among those having the same normal form to replace them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    normal_to_original_list_mapping : Dict[str, List[str]]\n",
    "         A python dictionary with the normalized form of the token as key and the list of non-normalized forms of the tokens as value.\n",
    "    tokens_counter_before_normalization : Counter_type[str]\n",
    "        The counter of the unique tokens in the dataset before normalizing the tokens.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "         A python dictionary with the non-normalized form of the token as key and the token to replace it as value.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize a dictionary to store the normal form to non normal form mapping after select the token\n",
    "    denormalization_mapping = {}\n",
    "\n",
    "    for normal_token, nonnormal_tokens in normal_to_original_list_mapping.items():\n",
    "        # for each normal form and all the tokens that have the considered normal form\n",
    "\n",
    "        all_tokens_ = [normal_token] + nonnormal_tokens\n",
    "\n",
    "        # create a dictionay with key as the tag and value as its frequency in the dataset.\n",
    "        tags_freq_counts = {\n",
    "            token_: tokens_counter_before_normalization[token_]\n",
    "            for token_ in all_tokens_\n",
    "        }\n",
    "\n",
    "        # select the tag that has the maximum frequency\n",
    "        selected_tag = max(tags_freq_counts, key=lambda k: tags_freq_counts[k])\n",
    "\n",
    "        # update the mapping to store the selected tag for all the tokens in the current loop.\n",
    "        for token_ in all_tokens_:\n",
    "            denormalization_mapping[token_] = selected_tag\n",
    "\n",
    "    # return the normal to non normal mapping of all the token\n",
    "    return denormalization_mapping\n",
    "\n",
    "\n",
    "def update_tokens_to_most_frequent_form(\n",
    "    profession_token_mapping_non_normalised: Dict[str, List[str]],\n",
    "    denormalization_mapping: Dict[str, str],\n",
    ") -> List[str]:\n",
    "    \"\"\"The function accepts the dictionary of profession string to non-normal tokens list,\n",
    "        along with the dictionary with the tokens to be modified as key as value as the string that replaces the token\n",
    "        and returns the updated mapping of profession string and list of tokens\n",
    "        (after selecting the major non-normal form for tokens having the same normal form).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profession_token_mapping_non_normalised : Dict[str, List[str]]\n",
    "        The dictionary with key as a unique profession in the dataset and the values are the list of non-normalized tokens\n",
    "        (that were obtained by splitting the profession string).\n",
    "    denormalization_mapping : Dict[str, str]\n",
    "        A python dictionary with the non-normalized form of the token as key and the token to replace it as value.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    profession_token_mapping_de_normalised : Dict[str, List[str]]\n",
    "        The dictionary with key as a unique profession in the dataset and the values are the list of denormalized tokens\n",
    "        (after selecting the most frequent token with the same normal form).\n",
    "    \"\"\"\n",
    "\n",
    "    # the list of tokens updated with the normalized form the mapping or retain the same token if there is no mapping.\n",
    "    profession_token_mapping_de_normalised = {\n",
    "        profession_string: [\n",
    "            denormalization_mapping.get(curr_token, curr_token)\n",
    "            for curr_token in profs_tokens\n",
    "        ]\n",
    "        for profession_string, profs_tokens in profession_token_mapping_non_normalised.items()\n",
    "    }\n",
    "\n",
    "    return profession_token_mapping_de_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ce834-60fd-43d5-8a70-7e5b5b5d8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of non-normalized tokens from the dataset\n",
    "all_tokens_before_normalzing = [\n",
    "    metier_token\n",
    "    for met_str in paris_jobs_after_s1[\"metier_after_S1\"].to_list()\n",
    "    for metier_token in nonnormal_metier_token_mapping[met_str]\n",
    "]\n",
    "\n",
    "# create a counter for the tokens before normalising\n",
    "counter_tokens_before_normalizing = Counter(all_tokens_before_normalzing)\n",
    "\n",
    "# get the list of unique non-normalized tokens from the dataset\n",
    "unique_nonnormal_tokens = set(counter_tokens_before_normalizing.keys())\n",
    "\n",
    "# seperate the non-normalized tokens that have correct spellings\n",
    "nonnormal_tokens_with_correct_spellings = unique_nonnormal_tokens.intersection(\n",
    "    words_with_correct_spellings\n",
    ")\n",
    "\n",
    "# get the non-normalized to normalized mapping\n",
    "normal_to_nonnormal_list_mapping = get_normal_nonnormal_map(\n",
    "    unique_nonnormal_tokens, nonnormal_tokens_with_correct_spellings\n",
    ")\n",
    "\n",
    "# get the mapping to update the token with same normal form with the highesht frequent non-normal form\n",
    "de_normalised_maping = get_denormalised_mapping(\n",
    "    normal_to_nonnormal_list_mapping, counter_tokens_before_normalizing\n",
    ")\n",
    "\n",
    "# update the tokens in profession to tokens mapping with their slected non normal forms\n",
    "denormalised_metier_token_mapping = update_tokens_to_most_frequent_form(\n",
    "    nonnormal_metier_token_mapping, de_normalised_maping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32722da4-c836-46df-a524-f8096491a8fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Splitting words with dots between words\n",
    "\n",
    "In the text, sometimes the profession has two abbreviations to describe it and they are not separated by space during OCR. For example `imprimeur.lithogr.` or `nÃ©goc.commiss.`. In this step after normalizing the tokens, these types of tokens where they are joined by a dot are split into multiple tokens. The idea is as follows: \n",
    "\n",
    "\n",
    "1. For each token that is not in the list of correctly spelled words and contains a dot (The tokens that have only one dot and that one at the end are ignored as they are potential abbreviations.) \n",
    "    1. The token is split at the dot and checked\n",
    "    2. If the majority (more than half plus one) of the resultant split has a length greater than one (Otherwise, the token could be a spelling mistake that has dot instead of alphabets and such token is ignored), then for each sub token (the result of the split):\n",
    "        1. Split the token at apostrophe if it contains an apostrophe (this provides a chance to split the tokens that have multiple apostrophes which were not split earlier)\n",
    "        2. If the token is not in the stop words, add a dot to the token at the end, if the token with a dot at the end has a higher frequency than the token without the dot.\n",
    "    3. If the result of splitting at dots is not the same as the result after updating tokens based on the frequency, then the token is updated to the list of new tokens obtained from the split.  \n",
    "    \n",
    "    \n",
    "#### Utility function to split words with dots between words\n",
    "\n",
    "The functions are\n",
    "\n",
    "1. `break_tokens_with_dots_around_words`: The function accepts the list of unique tokens present in the dataset along with their counter, the minimum length of the token, a set of correctly spelled words, and a list of stop words and returns a mapping to further split for the tokens that are not in the list of correctly spelled words and have a dot in them.\n",
    "2. `update_token_mappings_after_multiple_dot_split`: The function accepts the token sets before splitting the tokens with a dot in between and the mapping of tokens with a dot in between and their split and returns the updated token sets after replacing the tokens in the mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728bf16-3ddd-4797-8c28-6c42c4c32302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_tokens_with_dots_around_words(\n",
    "    denormalized_unique_tokens: List[str],\n",
    "    denormalized_tokens_counter: Counter_type[str],\n",
    "    minimum_length: int,\n",
    "    correctly_spelled_words: Set[str],\n",
    "    stop_words: Set[str],\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"The function accepts the list of unique tokens present in the dataset along with their counter,\n",
    "        the minimum length of the token, a set of correctly spelled words, and a list of stop words\n",
    "        and returns a mapping to further split for the tokens that are not in the list of correctly spelled words\n",
    "        and have a dot in them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    denormalized_unique_tokens : List[str]\n",
    "        A list of unique tokens from the profession strings after normalizing them.\n",
    "    denormalized_tokens_counter : Counter_type[str]\n",
    "        The counter of the ``normalized_unique_tokens`` in the dataset.\n",
    "    minimum_length : int\n",
    "        The minimum length of the string without non-alphanumeric characters.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    stop_words : Set[str]\n",
    "        The set of stop words in French.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    resplitted_tokens : Dict[str, str]\n",
    "        A dictionary with a key as a unique token and value is a list of strings that the token should be changed to.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise a dictionary to store the tokens that are updated after this split.\n",
    "    resplitted_tokens = {}\n",
    "\n",
    "    for normal_token in denormalized_unique_tokens:\n",
    "        # Don't change the tokens that are in the dictionary or have a dot at the end (probable abbrevation)\n",
    "        if (\n",
    "            not check_presence(normal_token, correctly_spelled_words)\n",
    "            and (\".\" in normal_token)\n",
    "            and not (normal_token.count(\".\") == 1 and normal_token[-1] == \".\")\n",
    "        ):\n",
    "            # split the token at dot (all occurrences)\n",
    "            split_on_dot = list(filter(None, normal_token.split(\".\")))\n",
    "            # get the length of the strings after split.\n",
    "            lens_of_splits = [len(split) for split in split_on_dot]\n",
    "            # get the lengths of splits after removing tokens with only one character.\n",
    "            lens_of_splits_without_singles = [\n",
    "                split_len for split_len in lens_of_splits if split_len != 1\n",
    "            ]\n",
    "\n",
    "            # if the majority of the splits have a more than one character, update the tokens based on the frequency\n",
    "            if len(lens_of_splits_without_singles) >= np.floor(\n",
    "                (len(lens_of_splits) / 2) + 1\n",
    "            ):\n",
    "\n",
    "                # list to store the result of splitting of the token\n",
    "                sep_toks = []\n",
    "\n",
    "                for split_word in split_on_dot:\n",
    "                    # for each sub token obtained as the result of splitting at dot\n",
    "                    split_word_toks = [split_word]\n",
    "                    if \"'\" in split_word:\n",
    "                        # split the sub token at apostrophe if there is an apostrophe\n",
    "                        split_word_toks = apos_split(\n",
    "                            split_word,\n",
    "                            minimum_length,\n",
    "                            correctly_spelled_words,\n",
    "                            stop_words,\n",
    "                        )\n",
    "                    if split_word_toks:\n",
    "                        # if the token still remains after splitting at apostrophe (if there is no apostrophe then this condition is redundant)\n",
    "                        for split_word_tok in split_word_toks:\n",
    "                            # for eack sub sub token that is the result of splitting at apostrophe\n",
    "                            if not check_presence(split_word_tok, stop_words):\n",
    "                                # if the sub sub token is not in stop words\n",
    "                                if (\n",
    "                                    denormalized_tokens_counter[split_word_tok + \".\"]\n",
    "                                    > denormalized_tokens_counter[split_word_tok]\n",
    "                                ):\n",
    "                                    # if the sub sub token with a dot at the end is more frequent than the one without a dot then append the token with the dot\n",
    "                                    sep_toks.append(split_word_tok + \".\")\n",
    "                                    continue\n",
    "                            # if either the sub sub token is a stopword or the frequency with a dot at the end is not more than the frequency without a dot, then append the sub sub token as it is (also the case when there is no sub sub token with a dot the end)\n",
    "                            sep_toks.append(split_word_tok)\n",
    "\n",
    "                if sep_toks != split_on_dot:\n",
    "                    # if the result of the token split at dot and updating of those tokens based on the frequency is different (indicating that the tokens are updated)\n",
    "                    sep_toks = [\n",
    "                        septok\n",
    "                        for septok in sep_toks\n",
    "                        if is_valid_token(septok, minimum_length, stop_words)\n",
    "                    ]\n",
    "                    # the mapping from the original token to the splitted tokens is stored in the dictionary\n",
    "                    resplitted_tokens[normal_token] = sep_toks\n",
    "\n",
    "    return resplitted_tokens\n",
    "\n",
    "\n",
    "def update_token_mappings_after_multiple_dot_split(\n",
    "    profession_token_mapping_before_resplit: Dict[str, List[str]],\n",
    "    dict_of_splitted_tokens: Dict[str, List[str]],\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"The function accepts the token sets before splitting the tokens with a dot in between and\n",
    "        the mapping of tokens with a dot in between and their split and\n",
    "        returns the updated token sets after replacing the tokens in the mapping. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profession_token_mapping_before_resplit : Dict[str, List[str]]\n",
    "        The dictionary with key as unique profession in the dataset and the values are the list of normalised tokens (that were obtained by splitting the profession string).\n",
    "    dict_of_splitted_tokens : Dict[str, List[str]]\n",
    "        A dictionary with key as a unqiue token and value is a list of strings that the token should to changed to.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_profession_token_map : Dict[str, List[str]]\n",
    "        The dictionary with key as unique profession in the dataset and the values are the list of normalised and resplitted for dot tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise dictionary to store the new profession string to tokens mapping\n",
    "    new_profession_token_map = {}\n",
    "\n",
    "    for profs_string, profs_tokens in profession_token_mapping_before_resplit.items():\n",
    "        # for each profession string and its tokens\n",
    "        new_tokens = []\n",
    "        # initialise a list to store the new list of tokens for the profession string\n",
    "        for token in profs_tokens:\n",
    "            # for each token of the profession string\n",
    "            if token in dict_of_splitted_tokens:\n",
    "                # append the updated list of tokens, if the token is in the dictionary of updated tokend\n",
    "                new_tokens.extend(dict_of_splitted_tokens[token])\n",
    "            else:\n",
    "                # else append token itself\n",
    "                new_tokens.extend([token])\n",
    "\n",
    "        # add the profession string and its new tokens list to the mapping\n",
    "        new_profession_token_map[profs_string] = new_tokens\n",
    "\n",
    "    # return mapping\n",
    "    return new_profession_token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c04011-196c-46b8-b44c-98958ab91f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the list of all tokens in the dataset after denormalising the them\n",
    "all_tokens_after_denormalzing = [\n",
    "    metier_token\n",
    "    for met_str in paris_jobs_after_s1[\"metier_after_S1\"].to_list()\n",
    "    for metier_token in denormalised_metier_token_mapping[met_str]\n",
    "]\n",
    "\n",
    "# create a counter for the tokens after denormalising\n",
    "counter_tokens_after_denormalizing = Counter(all_tokens_after_denormalzing)\n",
    "\n",
    "# get the list of unique denormalized tokens from the dataset\n",
    "unique_tokens_after_denormalizing = set(counter_tokens_after_denormalizing.keys())\n",
    "\n",
    "# seperate the denormalized tokens that have correct spellings\n",
    "correctly_spelled_tokens_after_denormalizing = unique_tokens_after_denormalizing.intersection(\n",
    "    words_with_correct_spellings\n",
    ")\n",
    "\n",
    "# get the re split mapping of tokens with dot between the words\n",
    "tokens_resplitted_mapping = break_tokens_with_dots_around_words(\n",
    "    unique_tokens_after_denormalizing,\n",
    "    counter_tokens_after_denormalizing,\n",
    "    MINIMUM_TOKEN_LENGTH,\n",
    "    correctly_spelled_tokens_after_denormalizing,\n",
    "    stop_words_french,\n",
    ")\n",
    "\n",
    "resplitted_metier_token_mapping = update_token_mappings_after_multiple_dot_split(\n",
    "    denormalised_metier_token_mapping, tokens_resplitted_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed12540-f3f1-4d60-aae7-49510dd70a76",
   "metadata": {},
   "source": [
    "### Collecting professions with the same tokens\n",
    "\n",
    "Until this stage, the order of the tokens is preserved for each unique profession string. At this stage, the cleaning (without spelling correction) is completed and as the intention is to create single word tags for each profession, the order of the tokens is ignored from here on. The tokens and dataset are updated accordingly.\n",
    "\n",
    "#### Utility function to collect professions with same tokens\n",
    "\n",
    "The function is \n",
    "\n",
    "1. `get_unordered_tokens_to_metiers_mapping`: This function accepts the mapping of the profession strings to their tokens and returns an inverse mapping with tokens sets as the keys and the values as the list of profession strings having those tokens (the ordering of the tokens is ignored.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbae1e6-6506-434d-994e-be96fc76f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unordered_tokens_to_metiers_mapping(\n",
    "    metier_token_mapping: Dict[str, List[str]]\n",
    ") -> Dict[FrozenSet[str], List[str]]:\n",
    "    \"\"\"This function accepts the mapping the profession strings to their tokens and\n",
    "        returns an inverse mapping with tokens sets as the keys and the values as the list of profession strings\n",
    "        having those tokens (the ordering of the tokens is ignored.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metier_token_mapping : Dict[str, List[str]]\n",
    "        The dictionary with key as unique profession in the dataset and the values are the list of normalised and resplitted for dot tokens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[FrozenSet[str], List[str]]\n",
    "        The dictionary with key as tokens set and values as  list of unique profession in the dataset that have the token set as the key.\n",
    "    \"\"\"\n",
    "\n",
    "    # intialise a dictionary to store the tokens as key and the profession string as the value\n",
    "    unordered_same_token_metiers = {}\n",
    "\n",
    "    for met_str, met_toks in metier_token_mapping.items():\n",
    "        # for each profession string and its corresponding tokens\n",
    "        frozen_toks = frozenset(met_toks)\n",
    "        # the list of tokens is converted into a frozen set as this is lossen the order of the tokens and frozenset can be used as a key for the dictionary.\n",
    "        if not check_presence(frozen_toks, unordered_same_token_metiers):\n",
    "            # if the tokens set is not already present in the mapping\n",
    "            unordered_same_token_metiers[frozen_toks] = []\n",
    "            # create an empty list corresponding to the set of tokens to store the profession strings having those tokens\n",
    "        # append the profession string to the list corresponding to its tokens set.\n",
    "        unordered_same_token_metiers[frozen_toks].append(met_str)\n",
    "\n",
    "    return unordered_same_token_metiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478b99e-b701-4e93-ad90-051ea5e4778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mapping of token sets to profession strings\n",
    "same_token_sets_metiers = get_unordered_tokens_to_metiers_mapping(\n",
    "    resplitted_metier_token_mapping\n",
    ")\n",
    "\n",
    "# The keys of the mapping of token sets to profession strings is updated from frozen sets to strings (sepearated by space) as frozen sets cannot be saved to disk\n",
    "same_token_metiers = {\n",
    "    \" \".join(set_tok): org_mets for set_tok, org_mets in same_token_sets_metiers.items()\n",
    "}\n",
    "\n",
    "# The inverse mapping of profession strings to serialized token sets is created to update the dataset.\n",
    "same_token_metiers_inv_map = {\n",
    "    org_met: set_tok\n",
    "    for set_tok, org_mets in same_token_metiers.items()\n",
    "    for org_met in org_mets\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22dfebb-a6d9-4cf2-bc9d-d697b2212364",
   "metadata": {},
   "source": [
    "### Saving the serialized token sets to profession strings mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a096eec-764b-4386-bd07-939ccc613525",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/searlized_tokens_profession_mapping.json\",\n",
    "    \"w\",\n",
    "    encoding=\"utf8\",\n",
    ") as outfile:\n",
    "    json.dump(same_token_metiers, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5087bc5-5f0f-4854-bd02-91e0702d4968",
   "metadata": {},
   "source": [
    "### Reading the serialized token sets to profession strings mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55de939-8c86-4094-a365-5499e74c09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/searlized_tokens_profession_mapping.json\",\n",
    "    encoding=\"utf8\",\n",
    ") as f:\n",
    "    same_token_metiers = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721729f-f40a-44cc-9058-2e56a5ff7b4c",
   "metadata": {},
   "source": [
    "The csv file containing the dataset is updated with the serialized tokens as the profession string after step 2 and stored in a column with the name `metier_after_S2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ce3ec-7e83-4793-9cad-3d8e8b129cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column to store the serialized token sets\n",
    "paris_jobs_after_s1[\"metier_after_S2\"] = paris_jobs_after_s1[\"metier_after_S1\"].map(\n",
    "    same_token_metiers_inv_map\n",
    ")\n",
    "\n",
    "# remove the rows of the dataset that do not have empty serialized token sets\n",
    "paris_jobs_after_s1[\"metier_after_S2\"].replace(\"\", float(\"NaN\"), inplace=True)\n",
    "paris_jobs_after_s1.dropna(subset=[\"metier_after_S2\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a99bf-f1aa-463d-b0dd-3c5e2d670ac6",
   "metadata": {},
   "source": [
    "### Saving the dataframe with serialized token sets (seperated by space) for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e97182-873c-478c-998b-fed2dcc897cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_jobs_after_s1.to_csv(\n",
    "    \"./../data/intermediate_steps/all_paris_jobs_after_step_2.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e7806-7f27-4ec6-bc2d-fc2e3d8b2c27",
   "metadata": {},
   "source": [
    "### Reading the dataframe with serialized token sets (seperated by space) for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a71cee-20d7-473d-b9f3-d3d909cbf820",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_jobs_after_s1 = pd.read_csv(\n",
    "    \"./../data/intermediate_steps/all_paris_jobs_after_step_2.csv\",\n",
    "    dtype={\n",
    "        \"doc_id\": \"str\",\n",
    "        \"page\": \"str\",\n",
    "        \"row\": \"str\",\n",
    "        \"Nom\": \"str\",\n",
    "        \"mÃ©tier_original\": \"str\",\n",
    "        \"rue\": \"str\",\n",
    "        \"numÃ©ro\": \"str\",\n",
    "        \"annee\": \"str\",\n",
    "        #\"gallica_page\": \"str\",\n",
    "        \"mÃ©tier\": \"str\",\n",
    "        \"metier_after_S1\": \"str\",\n",
    "        \"metier_after_S2\": \"str\",\n",
    "    },\n",
    "    header=0,\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07177203-6a8b-438a-b3a5-d9c0e0b93423",
   "metadata": {},
   "source": [
    "In the next cells, before going to step 3, the unique tokens present in the dataset after step 2 are stored to the disk. For each token, the frequency and the urls in gallica where it appears are also stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bfa49-108f-4121-8ff9-7b53f2990aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise empty dictionary to store token, its count and the urls\n",
    "unique_denormalised_tokens = {}\n",
    "\n",
    "for _, row in tqdm_notebook(\n",
    "    paris_jobs_after_s1.iterrows(), total=len(paris_jobs_after_s1)\n",
    "):\n",
    "    # for row in the dataset\n",
    "\n",
    "    url = \"https://gallica.bnf.fr/ark:/12148/{}/f{}.zoom\".format(\n",
    "        row[\"doc_id\"], row[\"gallica_page\"]\n",
    "    )\n",
    "    # generate the url for the entry\n",
    "\n",
    "    tokens = row[\"metier_after_S2\"].split()\n",
    "    # get the tokens for that row form the `metier_after_S2` column\n",
    "\n",
    "    for token in tokens:\n",
    "        # for each token\n",
    "        if not check_presence(token, unique_denormalised_tokens):\n",
    "            # create an entry in the dictionary if was not already created.\n",
    "            unique_denormalised_tokens[token] = {\"count\": 0, \"links\": []}\n",
    "\n",
    "        # increase the count of the token\n",
    "        unique_denormalised_tokens[token][\"count\"] += 1\n",
    "        # add the url of the entry\n",
    "        unique_denormalised_tokens[token][\"links\"].append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b5b9e-08d2-4d08-8bd1-0cf5bc6cd4ef",
   "metadata": {},
   "source": [
    "The unique tokens with the count and links is sorted based on the frequency and duplicate urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546bfab-b38a-4023-a417-a874388834c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unique_denormalised_tokens = {\n",
    "    k: v\n",
    "    for k, v in sorted(unique_denormalised_tokens.items(), key=lambda e: -e[1][\"count\"])\n",
    "}\n",
    "\n",
    "for token_key, token_val_dict in sorted_unique_denormalised_tokens.items():\n",
    "    token_val_dict[\"links\"] = list(set(token_val_dict[\"links\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f9933-6a9f-4ef8-aa4f-357a6dbecf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The number of unqiue profession at the oneset {},\\nafter cleaning special characters {},\\nafter merging mistakenly split words {},\\nafter token cleaning {}.\".format(\n",
    "        paris_jobs_after_s1[\"mÃ©tier_original\"].nunique(),\n",
    "        paris_jobs_after_s1[\"mÃ©tier\"].nunique(),\n",
    "        paris_jobs_after_s1[\"metier_after_S1\"].nunique(),\n",
    "        paris_jobs_after_s1[\"metier_after_S2\"].nunique(),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The number of unique tokens after step 2 {}\".format(\n",
    "        len(sorted_unique_denormalised_tokens)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a6807-48f7-42f7-bff6-6aa8bef4c9d4",
   "metadata": {},
   "source": [
    "### Saving the unique tokens with counts and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5bbe48-e186-418b-9007-b481d3c5da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/unique_denormalised_tokens.json\", \"w\", encoding=\"utf8\"\n",
    ") as outfile:\n",
    "    json.dump(sorted_unique_denormalised_tokens, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd69b7a-5134-401a-b937-2fa706d7a677",
   "metadata": {},
   "source": [
    "### Reading the unique tokens with counts and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce87a52-fb25-4bcf-a5eb-e31d65187363",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"./../data/intermediate_steps/unique_denormalised_tokens.json\", encoding=\"utf8\"\n",
    ") as f:\n",
    "    sorted_unique_denormalised_tokens = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb57c60-d3e4-42f7-bb39-2abf5e9f5b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Merge misspelled tokens with the correctly spelled tokens (Step 3)\n",
    "\n",
    "In the third step, the tokens that do not have a correct spelling (identified through the list of correct words) will be merged with the closest correctly spelled word iteratively till there are no more possible merges. The merge is performed in three rounds. In the first two rounds, the context of the tokens (with respect to other tokens) is considered, and in the last round, the spellings are merged ignoring the context.\n",
    "\n",
    "### Algorithm for merging tokens\n",
    "\n",
    "- In the first round, the tokens are merged according to the [Algorithm for contextual merging](#algorithm-for-contextual-merging).\n",
    "- In the second round in each per profession token set (after round 1) the less frequent tokens are removed. A number defined apriori will be used to determine if a token is frequent or not and the same process as round 1 is repeated.\n",
    "- In the third round, the low frequent tokens per token set are added back to the per profession token sets (after round 2), however, this time the tokens are considered individually i.e. all tokens sets will be of length 1 and the same process as round 1 is repeated.\n",
    "- After round 3, the per profession token sets (after round 2) are updated to change the tokens that are merged and the dataset is saved to disk.\n",
    "\n",
    "#### Algorithm for contextual merging\n",
    "\n",
    "1. While the tokens can be merged, continue the iteration\n",
    "2. Create a counter for each unique token present in the dataset at this step\n",
    "3. Group the per profession token sets (the result of step 2) based on the length of the sets.\n",
    "4. For each length of the token set\n",
    "    1. For each token set of the considered token set length\n",
    "        1. Compare with all other token sets of the same length, if the two token sets differ only by one token (i.e. all the tokens are the same except one) and if the tokens are mergeable (cf [Algorithm for checking tokens mergability](#algorithm-for-checking-tokens-mergability))\n",
    "            1. With the same tokens in both the token sets as a base, store all such pairs of mergeable tokens as values (Intuitively this means that the pairs of tokens are probably the same and they can be merged with confidence).\n",
    "                1. For the tokens of length one, there is no specific base and a dummy string is considered as the base and the merge is continued.\n",
    "5. For each base of token sets, produce a base-wise update mapping that stores the token and the token to which it should be updated (cf [Algorithm for merging token pairs](#algorithm-for-merging-token-pairs)).\n",
    "6. Combine the base-wise update mapping's produced per base of token sets to generate a token update mapping for the iteration.\n",
    "    1. This step is performed to combine the updates of tokens from the same token set that were merged under different bases.\n",
    "7. Using the token update mapping for the iteration, update the profession string to token sets mapping.\n",
    "8. If the profession string to token sets mapping is unchanged from the previous iteration (i.e. if no token is updated in the iteration) then stop the merging, else continue the merging from _2_.\n",
    "\n",
    "##### Algorithm for checking tokens mergeability\n",
    "\n",
    "Two tokens can be mergeable if the tokens have a similarity greater than a certain threshold (defined apriori) and\n",
    "\n",
    "1. If the high frequent token is in the list of correctly spelled words and the low frequent token is not in the list of correctly spelled words.\n",
    "2. If both the tokens have the same frequency and only one of them is in the list of correctly spelled words\n",
    "\n",
    "In both cases, the token that is not in the list of correctly spelled words is merged to the word that is in the list of correctly spelled words.\n",
    "\n",
    "###### Token similarity\n",
    "\n",
    "The token similarity is the Levenshtein similarity between the tokens. The [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings is the \"minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other\". The distance is normalized between 0 and 1 and converted into similarity. For this project, the similarity is calculated using the [RapidFuzz](https://github.com/maxbachmann/RapidFuzz) library's `fuzz.ratio` function without any processing of the strings. The reason for choosing this library is that the function to calculate the similarity between strings accepts a threshold for similarity and use it is as an early stopping criterion in calculating the similarity between the stings (based on the lengths of the strings, It is not possible to obtain a high similarity between the strings when the length of them is significantly different). It returns zero as the similarity when the similarity is less than the given threshold.\n",
    "\n",
    "#####  Algorithm for merging token pairs\n",
    "\n",
    "For a given base and the pairs of tokens that are mergeable, a two-step process is followed to determine to which token the pairs are changed.\n",
    "\n",
    "1. Loop over all the pairs to create a mapping with key as the token and value as a list of tuples containing that token that it can be merged with and the similarity between the two tokens.\n",
    "2. For each of the tokens to be merged,\n",
    "    1. If there is only one token that it can be merged with then the token to be merged is changed to the token it can be merged with.\n",
    "    2. else if there are more than one possible tuples, then the token to be merged is changed to the token with the highest similarity score.\n",
    "        1. If there are multiple tokens with the same similarity score, then the token to be merged is changed to the token that has appeared more frequently among those that have the same frequency.\n",
    "    3. If the token is not yet merged, then the token is left as it is without merging.\n",
    "    \n",
    "### Utility functions to merge misspelled tokens with the correctly spelled tokens\n",
    "\n",
    "The functions are\n",
    "\n",
    "1. `merge_close_spellings`: The function accepts the list of tokens sets for all the dataset along with a set of correctly spelled words, the thresholds to check if tokens can be classified as low frequency and if two tokens are similar and return the updated list of token sets for all the dataset and an array with the similarity scores of tokens that are merged.\n",
    "2. `get_similar_tokens_differed_by_one_token`: The function accepts the clustering of token sets based on their length at current iteration, along with the clustering at previous iteration (to help in early stopping), the frequency counter for the tokens in the dataset, list of correctly spelled words and a minimum similarity score for the tokens to be able declared similar and return a mapping(Python dictionary). The returned python dictionary has the set of tokens common between two token sets (except one token) and the value is another dictionary. The key for the secondary dictionary is the pair of tokens that share the tokens in the primary dictionary that are mergeable and different only by these two tokens with the value as another dictionary. In the tertiary and last dictionary, there are two elements, the `sim_score` key has the similarity score between the secondary token pair as the value and the `merge_to` key has one of the tokens in the secondary dictionary key pair that will absorb the other token.\n",
    "3. `are_tokens_mergable`: The function accepts two tokens, along with their frequency, presence in the list of correctly spelled words, and a threshold of similarity score and returns the similarity score and the token that will absorb the other token. The algorithm to decide if two tokens are mergeable is described as `Algorithm for checking tokens mergeability's in the documentation above.\n",
    "4. `group_connected_words_with_common_base`: The function accepts the dictionary of common tokens and the mergeable tokens for the base along with their counter and their presence in the list of correctly spelled words and returns the mapping of the updates to the tokens per tokens set of profession strings.\n",
    "5. `get_replacements`: This function accepts the similar tokens for having common tokens along with their similarity and returns one, a dictionary with the key as the token and the value as the token to replace the key, and two, a set of tokens updated in the current iteration.\n",
    "6. `get_tok_count`: The function accepts the profession token sets for all the entries of a dataset and returns a counter containing the frequency for each token.\n",
    "7. `get_unique_token_sets`: Return the unique profession token sets present in the dataset\n",
    "8. `get_length_wise_token_sets`: The function accepts the unique token sets of the dataset and clusters them based on the length of the token sets.\n",
    "9. `print_token_statistics`: The function prints the statistics about the changes in the number of tokens after the iteration or round.\n",
    "10. `print_merge_statistics`: The function prints the mean, median, standard deviation, minimum and maximum values of the similarity scores.\n",
    "11. `get_current_token_sets`: Get the last token set for a list of list of token sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353800a-16f6-4704-97ec-633798762fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_processor(token: str) -> str:\n",
    "    \"\"\"A string processor to return the same string as input.\n",
    "        This dummy processor is used to avoid the default processor of the Rapidfuzz module to calculate string similarity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : str\n",
    "        The input string to process.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The output string same as the input string.\n",
    "    \"\"\"\n",
    "    return token\n",
    "\n",
    "def get_current_token_sets(\n",
    "    list_of_list_of_token_sets: List[List[FrozenSet[str]]],\n",
    ") -> List[FrozenSet[str]]:\n",
    "    \"\"\"Get the last tokenset for a list of list of tokensets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_list_of_token_sets : List[List[FrozenSet[str]]]\n",
    "        The current list of list of tokensets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[FrozenSet[str]]\n",
    "        The current list of tokensets\n",
    "    \"\"\"\n",
    "    return [tok_list[-1] for tok_list in list_of_list_of_token_sets]\n",
    "\n",
    "\n",
    "def get_tok_count(token_sets_list: List[FrozenSet[str]]) -> Counter_type[str]:\n",
    "    \"\"\"The function accepts the profession token sets for all the entries of a dataset and returns a counter containig the frequency for each token\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_sets_list : List[FrozenSet[str]]\n",
    "        The list of tokens sets that represent the profession string for all the entries of the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counter_all_tokens : Counter_type[str]\n",
    "        The counter with the frequency for each token in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # collect the individual tokens in each tokenset into a list\n",
    "    all_tokens = [\n",
    "        metier_token\n",
    "        for metier_tokens in token_sets_list\n",
    "        for metier_token in metier_tokens\n",
    "    ]\n",
    "    # Create a counter object on the list of tokens\n",
    "    counter_all_tokens = Counter(all_tokens)\n",
    "    # return the counter object\n",
    "    return counter_all_tokens\n",
    "\n",
    "\n",
    "def get_unique_token_sets(\n",
    "    token_sets_list: List[FrozenSet[str]],\n",
    ") -> List[FrozenSet[str]]:\n",
    "    \"\"\"Return the unique profession tokens sets present in the dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_sets_list : List[FrozenSet[str]]\n",
    "        The list of tokens sets that represent the profession string for all the entries of the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[FrozenSet[str]]\n",
    "        The list of unique tokens sets obtained by removing the duplicates in the input list.\n",
    "    \"\"\"\n",
    "\n",
    "    return set(token_sets_list)\n",
    "\n",
    "\n",
    "def get_length_wise_token_sets(\n",
    "    unique_profession_token_sets: List[FrozenSet[str]],\n",
    ") -> Dict[int, List[FrozenSet[str]]]:\n",
    "    \"\"\"The fucntion accepts the unque token sets of the dataset and clusters them based on the length of the token sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unique_profession_token_sets : List[FrozenSet[str]]\n",
    "        The list of unique tokens sets per profession string in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    len_wise_token_sets : Dict[int, List[FrozenSet[str]]]\n",
    "        A dictionary with length of the token sets as the key and the token sets as the values in a list.\n",
    "    \"\"\"\n",
    "\n",
    "    # intialise a dictionary to store the length of the token sets as the key and the token sets as the values in a list.\n",
    "    len_wise_token_sets = {}\n",
    "\n",
    "    for froz_tok in unique_profession_token_sets:\n",
    "        # for each token set\n",
    "        set_len = len(froz_tok)  # get the length of the token set\n",
    "        if not check_presence(set_len, len_wise_token_sets):\n",
    "            # if the length of the token set is not present as a key, add it to the dictionary\n",
    "            len_wise_token_sets[set_len] = []\n",
    "        # append the token set to the dictionary at the corresponding length\n",
    "        len_wise_token_sets[set_len].append(froz_tok)\n",
    "\n",
    "    # sort the length wise clustered token sets based on the length (key of the clusters)\n",
    "    # the sorting is performed for the sake of neatness.\n",
    "    len_wise_token_sets = {\n",
    "        k: v for k, v in sorted(len_wise_token_sets.items(), key=lambda e: e[0])\n",
    "    }\n",
    "\n",
    "    # return the length wise clustered token sets.\n",
    "    return len_wise_token_sets\n",
    "\n",
    "\n",
    "def are_tokens_mergable(\n",
    "    tokens_to_check: Set[str],\n",
    "    tokens_counter: Counter_type[str],\n",
    "    correctly_spelled_words: Set[str],\n",
    "    minimum_similarity: float,\n",
    ") -> Tuple[Union[float, None], Union[str, None]]:\n",
    "    \"\"\"The function accepts two tokens, along with their frequency, presence in list of correctly spelled words and a threshold of similarity score and return the similarity score and the token that will absorb the other token. The alogorithm to decide if two tokens are mergable is described as ``Algorithm for checking tokens mergability`` in the documentaion above.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_to_check : Set[str]\n",
    "        A set of two token strings to be checked for mergability.\n",
    "    tokens_counter : Counter_type[str]\n",
    "        The frequency counter of the tokens in the dataset.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    minimum_similarity : float\n",
    "        The minimum threshold for the similarity score between the token strings to be mergable.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sim_score : Union[float, None]\n",
    "        The similarity score between the tokens. None if the similarity score is less than the threshold similarity score or if the tokens are not mergable due to other conditions\n",
    "    merge_to : Union[str, None]\n",
    "        One of the two input tokens that will absorb the other token as a result of the merge. None if the tokens are not mergable.\n",
    "    \"\"\"\n",
    "\n",
    "    # initalising similarity score and the token to be merged to as None, assuming the tokens are not mergable\n",
    "    sim_score = None\n",
    "    merge_to = None\n",
    "    # a falg to indicate if both the tokens have same frequency\n",
    "    tokens_with_same_frequency = False\n",
    "\n",
    "    # assign the higher frequency token as first token and the lower frequency token as second token.\n",
    "    token_one, token_two = tokens_to_check\n",
    "\n",
    "    if tokens_counter[token_one] == tokens_counter[token_two]:\n",
    "        # if both the tokens have same frequency the flag is set to True\n",
    "        tokens_with_same_frequency = True\n",
    "    elif tokens_counter[token_two] > tokens_counter[token_one]:\n",
    "        # assign the higher frequency token as first token and the lower frequency token as second token.\n",
    "        token_one, token_two = token_two, token_one\n",
    "\n",
    "    # flags to store if the tokens are in the list of correctly spelled words\n",
    "    token_one_correct_spell = check_presence(token_one, correctly_spelled_words)\n",
    "    token_two_correct_spell = check_presence(token_two, correctly_spelled_words)\n",
    "\n",
    "    if (token_one_correct_spell and (not token_two_correct_spell)) or (\n",
    "        tokens_with_same_frequency\n",
    "        and (token_one_correct_spell ^ token_two_correct_spell)\n",
    "    ):\n",
    "        # calculate similarity\n",
    "        # 1. if higher frequency token is in the dictionary and lower frequency token is not in the dictionary\n",
    "        # or\n",
    "        # 2. if only one token is in the dictionary when the tokens have same frequency.\n",
    "        sim_score = round(\n",
    "            fuzz.ratio(\n",
    "                token_one, token_two, processor=None, score_cutoff=minimum_similarity\n",
    "            )\n",
    "        )\n",
    "        # the `fuzz.ratio` function from the rapidfuzz library returns the 0 similarity score if the similarity score between the input strings is less than the threshold provided as another argument.\n",
    "\n",
    "        # the token present in the dictionary will be assigned to the `merge_to` to absorb the other token\n",
    "        merge_to = token_one\n",
    "        if token_two_correct_spell:\n",
    "            merge_to = token_two\n",
    "\n",
    "    return sim_score, merge_to\n",
    "\n",
    "\n",
    "def get_similar_tokens_differed_by_one_token(\n",
    "    len_wise_token_sets: Dict[int, List[FrozenSet[str]]],\n",
    "    prev_len_wise_token_sets: Dict[int, List[FrozenSet[str]]],\n",
    "    tokens_counter: Counter_type[str],\n",
    "    correctly_spelled_words: Set[str],\n",
    "    minimum_similarity: float,\n",
    ") -> Dict[FrozenSet[str], Dict[FrozenSet[str], Dict[str, Union[float, str]]]]:\n",
    "    \"\"\"The function accepts the clustering of token sets based on their length at current iteration, along with the clustering at previous iteration (to help in early stopping), the frequncy counter for the tokens in the dataset, list of correctly spelled words and a minimum similarity score for the tokens to be able declared similar and return a mapping(Python dictionary). The returned python dictionary has the set of tokens common between two token sets (except one token) and the value is another dictionary. The key for the secondary dictionary is the pair of tokens that share the tokens in the primary dictionary that are mergable and different only by these two tokens with the value as another dictionary. In the tertiary and last dictionary there are two elements, the ``sim_score`` key has the similarity score between the secondary token pair as the value and ``merge_to`` key has the one of the token in the secondary dictionary key pair that will absorb the other token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    len_wise_token_sets : Dict[int, List[FrozenSet[str]]]\n",
    "        A dictionary with length of the token sets as the key and the token sets as the values in a list at current iteration.\n",
    "    prev_len_wise_token_sets : Dict[int, List[FrozenSet[str]]]\n",
    "        A dictionary with length of the token sets as the key and the token sets as the values in a list at previous iteration.\n",
    "    tokens_counter : Counter_type[str]\n",
    "        The frequency counter of the tokens in the dataset.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    minimum_similarity : float\n",
    "        The minimum threshold for the similarity score between the token strings to be mergable.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    similar_tokens_with_base : Dict[FrozenSet[str], Dict[FrozenSet[str], Dict[str, Union[float, str]]]]\n",
    "        The three level dictionary with common tokens as primary key and the mergable tokens as secondary key and the similarity score and the token that will absorb the other token in the third level.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialise the empty dictionary to store return values.\n",
    "    similar_tokens_with_base = {}\n",
    "\n",
    "    # loop over the length wise clustered token sets\n",
    "    for str_len, str_lists in len_wise_token_sets.items():\n",
    "        # for token sets of same length\n",
    "\n",
    "        if (set(str_lists) != set(prev_len_wise_token_sets.get(str_len, []))):\n",
    "            # If the token sets of a given length at this iteration are not same as the token sets of a same length in previous iteration, continue with the process. If the sets are same with previous iteration, it means that the those tokens are not merged and the checking for mergability is redundant.\n",
    "\n",
    "            print(\"Checking for token sets of length {}\".format(str_len))\n",
    "\n",
    "            len_str_lists = len(str_lists)\n",
    "\n",
    "            for ind_lst_1 in tqdm_notebook(range(len_str_lists)):\n",
    "\n",
    "                for ind_lst_2 in range(ind_lst_1 + 1, len_str_lists):\n",
    "                    # for each pair of token sets of same length\n",
    "\n",
    "                    # get the tokens that are different in the two token sets (acts as secondary key)\n",
    "                    set_sym_diff = str_lists[ind_lst_1] ^ str_lists[ind_lst_2]\n",
    "\n",
    "                    if len(set_sym_diff) == 2:\n",
    "                        # if the sets have only two elements different i.e. if they are differed only by one token check if the tokens can be merged\n",
    "\n",
    "                        score, merge_to = are_tokens_mergable(\n",
    "                            set_sym_diff,\n",
    "                            tokens_counter,\n",
    "                            correctly_spelled_words,\n",
    "                            minimum_similarity,\n",
    "                        )\n",
    "\n",
    "                        if score:\n",
    "                            # if the tokens are mergable,\n",
    "\n",
    "                            # the same tokens present in both the token sets are obtained to be the primary key of the dictionary.\n",
    "                            froz_set_same = frozenset(\n",
    "                                str_lists[ind_lst_1].intersection(str_lists[ind_lst_2])\n",
    "                            )\n",
    "\n",
    "                            # for token sets of length one, there will be any primary key. So a dummy variable is created to act as a primary key\n",
    "                            if not froz_set_same:\n",
    "                                froz_set_same = \"SINGLE_TOKEN\"\n",
    "\n",
    "                            if not check_presence(\n",
    "                                froz_set_same, similar_tokens_with_base\n",
    "                            ):\n",
    "                                # create the second level dictionary with the primary key\n",
    "                                similar_tokens_with_base[froz_set_same] = {}\n",
    "                            if not check_presence(\n",
    "                                set_sym_diff, similar_tokens_with_base[froz_set_same]\n",
    "                            ):\n",
    "                                # create the third level dictionary with the secondary key and add the tertiary keys and the corresponding values.\n",
    "                                similar_tokens_with_base[froz_set_same][\n",
    "                                    set_sym_diff\n",
    "                                ] = {\n",
    "                                    \"sim_score\": score,\n",
    "                                    \"merge_to\": merge_to,\n",
    "                                }\n",
    "        else:\n",
    "            print(\"Skipping checking for token sets of length {}\".format(str_len))\n",
    "    return similar_tokens_with_base\n",
    "\n",
    "\n",
    "def get_replacements(\n",
    "    similar_tokens_for_base: Dict[FrozenSet[str], Dict[str, Union[float, str]]],\n",
    "    tokens_counter: Counter_type[str],\n",
    ") -> Tuple[Dict[str, str], Set[str]]:\n",
    "    \"\"\"This function accepts the similar tokens for having common tokens along with their similarity and \n",
    "        returns one, a dictionary with the key as the token and the value as the token to replace the key,\n",
    "        and two, a set of tokens updated in the current iteration. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    similar_tokens_for_base : Dict[FrozenSet[str], Dict[str, Union[float, str]]]\n",
    "        The two level dictionary per a set of common tokens with the mergable tokens as primary key and\n",
    "        the similarity score and the token that will absorb the other token.\n",
    "    tokens_counter : Counter_type[str]\n",
    "        The frequency counter of the tokens in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    replacements : Dict[str, str]\n",
    "        A dictionary with the mapping from the token to change to the token to be changed with for\n",
    "        a common set of tokens that are different only by one token.\n",
    "    all_tokens_round : Set[str]\n",
    "        The set of tokens that are updated in the iteration\n",
    "    \"\"\"\n",
    "\n",
    "    # intialize an empty dictionary to hold the possible pairs of token and similarity score that a token with low frequency and not in the list of correctly spelled words can be merged with.\n",
    "    local_replacements = {}\n",
    "\n",
    "    # intialize an empty dictionary to hold the merge mapping from of the token with low frequency and not in dictionary to token with high frequeny and that is present in the the list of correctly spelled words.\n",
    "    replacements = {}\n",
    "\n",
    "    # initialize an empty set to hold the list of tokens being updated for the considered base set of common tokens.\n",
    "    all_tokens_round = set()\n",
    "\n",
    "    # First, get the possible list of tokens the low frequent and not in the list of correctly spelled words token can be merged with.\n",
    "\n",
    "    for similar_tokens, props in similar_tokens_for_base.items():\n",
    "        # for each pair of similar tokens and their properties of similarity scores and the token that will absorb the other token\n",
    "\n",
    "        # the token in the list of correctly spelled words and with high frequency is set as the `merge_to` variable and the lower frequency, not in the list of correctly spelled words token is assigned to `to_be_merged` variable.\n",
    "        token_one, token_two = similar_tokens\n",
    "        merge_to = props[\"merge_to\"]\n",
    "        if merge_to == token_one:\n",
    "            to_be_merged = token_two\n",
    "        else:\n",
    "            to_be_merged = token_one\n",
    "\n",
    "        if not check_presence(to_be_merged, local_replacements):\n",
    "            # add the `to_be_merged` key to the local_replacements dictionary as key\n",
    "            local_replacements[to_be_merged] = []\n",
    "        if not check_presence(merge_to, local_replacements[to_be_merged]):\n",
    "            # add the `merge_to` and the similarity scoore under the possible replacements for `to_be_merged` variable.\n",
    "            local_replacements[to_be_merged].append((merge_to, props[\"sim_score\"]))\n",
    "\n",
    "    # Second, decide the token to which the low frequent not in the list of correctly spelled words will be merged to.\n",
    "\n",
    "    for to_be_merged, possible_merges in local_replacements.items():\n",
    "        # for each `to_be_merged` token and the list of possible merges\n",
    "\n",
    "        selected_mergee = None\n",
    "        if len(possible_merges) == 1:\n",
    "            # if there is only one token, then the low frequent and not in the list of correctly spelled words token is merged with it.\n",
    "            selected_mergee = possible_merges[0][0]\n",
    "        else:\n",
    "            # if there are more one one possible tokens, then the `merge_to` tokens are sorted based on the similarity of the `to_be_merged` and `merge_to` tokens\n",
    "            sim_scores = [sim_score for _, sim_score in possible_merges]\n",
    "            mergee_contenders = np.array(possible_merges)[\n",
    "                np.where(sim_scores == np.max(sim_scores))\n",
    "            ]\n",
    "            if len(mergee_contenders) == 1:\n",
    "                # select the token with highest similarity score to merge with.\n",
    "                selected_mergee = mergee_contenders[0][0]\n",
    "            else:\n",
    "                # if there are more than one possible tokens with same similarity score, then the `merge_to` tokens that have same similarity score are sorted based on their frequency in the dataset.\n",
    "                mergee_freq = [\n",
    "                    tokens_counter[mergee] for mergee, _ in mergee_contenders\n",
    "                ]\n",
    "                mergee_contenders_freq = np.array(mergee_contenders)[\n",
    "                    np.where(mergee_freq == np.max(mergee_freq))\n",
    "                ]\n",
    "                if len(mergee_contenders_freq) == 1:\n",
    "                    # select the token with highest frequency to merge with.\n",
    "                    selected_mergee = mergee_contenders_freq[0][0]\n",
    "\n",
    "                # if there are more than one tokens with same similarity score and same frequency, then the `to_be_merged` is no not merged with any token for this base.\n",
    "        if selected_mergee:\n",
    "            replacements[to_be_merged] = selected_mergee\n",
    "            all_tokens_round.update(\n",
    "                [to_be_merged]\n",
    "            )  # update the list of tokens updated in the iteration\n",
    "    return replacements, all_tokens_round\n",
    "\n",
    "\n",
    "def group_connected_words_with_common_base(\n",
    "    similar_tokens_with_base: Dict[\n",
    "        FrozenSet[str], Dict[FrozenSet[str], Dict[str, Union[float, str]]]\n",
    "    ],\n",
    "    tokens_counter: Counter_type[str],\n",
    ") -> Dict[FrozenSet[str], Dict[str, str]]:\n",
    "    \"\"\"The function accepts the dictionary of common tokens and the mergable tokens for the base along with their counter and their presence in the list of correctly spelled words and returns the mapping of the updates to the tokens per tokens set of profession strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    similar_tokens_with_base : Dict[FrozenSet[str], Dict[FrozenSet[str], Dict[str, Union[float, str]]]]\n",
    "        The three level dictionary with common tokens as primary key and the mergable tokens as secondary key and the similarity score and the token that will absorb the other token in the third level.\n",
    "    tokens_counter : Counter_type[str]\n",
    "        The frequency counter of the tokens in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    replacements_per_profession_token_set : Dict[FrozenSet[str], Dict[str, str]]\n",
    "        A two level dictionary, with the token set per profession string as the primary key and the value is a dictionary with the the token to change as the key and the token to be change with as the value.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Merging connected words with common base\")\n",
    "\n",
    "    # intialise a dictionary to store the updates on each token per each a profession string to token set mapping (only the tokens sets that have a change are stored)\n",
    "    replacements_per_profession_token_set = {}\n",
    "\n",
    "    for same_key, mergable_token_pairs in similar_tokens_with_base.items():\n",
    "        # for each base and dictionary of mergable token pairs for that base, get the possible replacements for the tokens for the given base\n",
    "\n",
    "        (same_base_replacements, all_tokens_this_round,) = get_replacements(\n",
    "            mergable_token_pairs, tokens_counter,\n",
    "        )\n",
    "\n",
    "        for token in all_tokens_this_round:\n",
    "            # for each token dealt during the iteration for merge\n",
    "\n",
    "            # create the complete token set\n",
    "            if same_key == \"SINGLE_TOKEN\":\n",
    "                # if the base is the predefined keyword \"SINGLE_TOKEN\", then the token set consists only the token\n",
    "                met_str_tokens = frozenset([token])\n",
    "            else:\n",
    "                # else, merge base with the token to generate the full token set (as we created base for those token sets that are differed by one token)\n",
    "                met_str_tokens = frozenset(same_key.union(set([token])))\n",
    "\n",
    "            if not check_presence(\n",
    "                met_str_tokens, replacements_per_profession_token_set\n",
    "            ):\n",
    "                # create an entry in the dictionary to store the per profession string token changes\n",
    "                replacements_per_profession_token_set[met_str_tokens] = {}\n",
    "\n",
    "            if check_presence(token, same_base_replacements):\n",
    "                # if the token is merged to another token during the process, add the mapping to the dictionary\n",
    "                replacements_per_profession_token_set[met_str_tokens][\n",
    "                    token\n",
    "                ] = same_base_replacements.get(token)\n",
    "\n",
    "    return replacements_per_profession_token_set\n",
    "\n",
    "\n",
    "def print_token_statistics(\n",
    "    prev_token_sets_len: int,\n",
    "    curr_token_sets_len: int,\n",
    "    unique_tokens_count_before: int,\n",
    "    unique_tokens_count_filtered: int,\n",
    "    unique_tokens_count_after: int,\n",
    ") -> NoReturn:\n",
    "    \"\"\"The function prints the statistics about the changes in the number of tokens after the iteration or an round.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prev_token_sets_len : int\n",
    "        The number of token sets i.e. tokens of profession strings before the current iteration.\n",
    "    curr_token_sets_len : int\n",
    "        The number of token sets i.e. tokens of profession strings after the current iteration.\n",
    "    unique_tokens_count_before : int\n",
    "        The number of unique tokens before the current iteration.\n",
    "    unique_tokens_count_filtered : int\n",
    "        The number of unique tokens before the current iteration after filtering based on frequency.\n",
    "    unique_tokens_count_after : int\n",
    "        The number of unique tokens after the current iteration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    NoReturn\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"\\t\\tThe number of unique token sets before merge is {} and after merge is {}\".format(\n",
    "            prev_token_sets_len, curr_token_sets_len\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t\\tThe number of unique tokens before is {}, after filtering {} and after merge is {}\".format(\n",
    "            unique_tokens_count_before,\n",
    "            unique_tokens_count_filtered,\n",
    "            unique_tokens_count_after,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def print_merge_statistics(replacement_scores_list: np.ndarray) -> NoReturn:\n",
    "    \"\"\"The function prints the mean, median, standard deviation, minimum and maximum values of the similarity scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    replacement_scores_list : np.ndarray\n",
    "        The array of similarity scores for the all the merges during the current iteration/round.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    NoReturn\n",
    "    \"\"\"\n",
    "\n",
    "    avg_score, min_score, max_score, median_score, stddev_score = (\n",
    "        np.mean(replacement_scores_list),\n",
    "        np.min(replacement_scores_list),\n",
    "        np.max(replacement_scores_list),\n",
    "        np.median(replacement_scores_list),\n",
    "        np.std(replacement_scores_list),\n",
    "    )\n",
    "\n",
    "    print(\"\\t\\tNumber of Unique Replacements: {}\".format(len(replacement_scores_list)))\n",
    "    print(\n",
    "        \"\\t\\tLeast Similarity Score: {}, Max Similarity Score: {}\".format(\n",
    "            min_score, max_score\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"\\t\\tAverage Score: {}, Standard Deviation: {}, Median: {}\\n\".format(\n",
    "            avg_score, stddev_score, median_score\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_close_spellings(\n",
    "    profession_token_sets_full_dataset: List[List[FrozenSet[str]]],\n",
    "    correctly_spelled_words: Set[str],\n",
    "    minimum_token_frequency: int,\n",
    "    minimum_similarity: float,\n",
    ") -> Tuple[List[List[FrozenSet[str]]], np.ndarray]:\n",
    "    \"\"\"The function accepts the list of tokens sets for all the dataset along with set of correctly spelled words,\n",
    "        the thresholds to check if tokens can be classified as low frequent and if two tokens are similar and\n",
    "        returns the updated list of token sets for all the dataset and an array with the similarity scores of tokens that are merged.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profession_token_sets_full_dataset : List[List[FrozenSet[str]]]\n",
    "        A list of token sets for the unique profession strings stored in a list. The token sets are stored in a list to track the changes.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    minimum_token_frequency : int\n",
    "        The minimum threshold for the frequency of token to be considered frequent (or not low frequent).\n",
    "    minimum_similarity : float\n",
    "        The minimum threshold for the similarity score between the token strings to be mergable.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    updated_token_sets_full_dataset_tracked : List[List[FrozenSet[str]]]\n",
    "        A list of token sets for the unique profession strings stored in a list. The last entry of the list is the updated token set after the current round.\n",
    "    replacement_scores_round : np.ndarray\n",
    "        The array of similarity scores for the all the merges during the current round.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a copy of the profession token sets lists to avoid in place editing.\n",
    "    updated_token_sets_full_dataset_tracked = copy.deepcopy(\n",
    "        profession_token_sets_full_dataset\n",
    "    )\n",
    "\n",
    "    # get the last entry from the list of list of token sets as the last entry is the last updated entry as current token sets.\n",
    "    token_sets_full_dataset = [\n",
    "        met_list[-1] for met_list in updated_token_sets_full_dataset_tracked\n",
    "    ]\n",
    "\n",
    "    # A list to store the the list of token sets before the update, used to stop the (merging) round when the token sets do not merge any further.\n",
    "    prev_token_sets_full_dataset = []\n",
    "    # A dictionary to store the the length wise clustered profession token sets before the update, used to stop the (merging) iteration for the particular length when the token sets do not merge any further.\n",
    "    prev_token_sets_per_length = {}\n",
    "    # a list to store the similarity scores of tokens merged during the whole round.\n",
    "    replacement_scores_round = []\n",
    "    # A counter variable to store the number of iterations in the round\n",
    "    itration_count = 0\n",
    "\n",
    "    while set(token_sets_full_dataset) != set(prev_token_sets_full_dataset):\n",
    "        # Megre the token sets as long as the previous token sets of profession strings is different as the current one. In other words, the round of merging is terminated when the token sets are not changed anymore.\n",
    "\n",
    "        # increase and print the iteration count\n",
    "        itration_count += 1\n",
    "        print(\"Merging {} time\".format(itration_count))\n",
    "\n",
    "        unique_replacement_similarity_scores = {}\n",
    "\n",
    "        # get the frequency counter for all the unique tokens in the current list of token sets for profession strings.\n",
    "        counter_all_tokens = get_tok_count(token_sets_full_dataset)\n",
    "\n",
    "        # assign the current list of list of token sets as previous iteration's list of list of token sets to be used in the next iteration to break the merge loop\n",
    "        prev_token_sets_full_dataset = token_sets_full_dataset\n",
    "\n",
    "        # According to the ``Algorithm for merging tokens`` mentioned above, in the second round the low frequent tokens are removed in the token sets. The filtering of tokens is performed here based on the ``minimum_token_frequency`` variable.\n",
    "        filtered_token_sets = [\n",
    "            frozenset(\n",
    "                [\n",
    "                    tok\n",
    "                    for tok in met_tok_set\n",
    "                    if (\n",
    "                        (counter_all_tokens[tok] > minimum_token_frequency)\n",
    "                        or (tok in correctly_spelled_words)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            for met_tok_set in token_sets_full_dataset\n",
    "        ]\n",
    "\n",
    "        # get the list of unique token sets from the full dataset.\n",
    "        unique_token_sets = get_unique_token_sets(filtered_token_sets)\n",
    "\n",
    "        # get the length wise clustered token sets\n",
    "        token_sets_per_length = get_length_wise_token_sets(unique_token_sets)\n",
    "\n",
    "        # get the similar token paris with same common tokens\n",
    "        contxt_similar_tokens = get_similar_tokens_differed_by_one_token(\n",
    "            token_sets_per_length,\n",
    "            prev_token_sets_per_length,\n",
    "            counter_all_tokens,\n",
    "            correctly_spelled_words,\n",
    "            minimum_similarity,\n",
    "        )\n",
    "\n",
    "        # assign the current clustering of token sets per length as previous iteration's token sets per length to be used in the next iteration to break the merge loop\n",
    "        prev_token_sets_per_length = token_sets_per_length\n",
    "\n",
    "        # get the replacements for the tokens using the similar tokens obtained in the previous step\n",
    "        replacements_per_unique_token_set = group_connected_words_with_common_base(\n",
    "            contxt_similar_tokens, counter_all_tokens\n",
    "        )\n",
    "\n",
    "        # empty list to store the updated list of token sets.\n",
    "        token_sets_full_dataset_updated = []\n",
    "\n",
    "        # loop over the full dataset of token sets and the filtered full dataset of token sets together to add the earlier filtered tokens back\n",
    "        for tok_set, tok_set_filt in zip(token_sets_full_dataset, filtered_token_sets):\n",
    "            # for the token set and its filtered version, get the updated token set. If the token set is not updated then the tokens are left unchanged.\n",
    "\n",
    "            updated_met_tok_set = tok_set\n",
    "            replacements_for_the_token_set = replacements_per_unique_token_set.get(\n",
    "                tok_set_filt, {}\n",
    "            )\n",
    "            if replacements_for_the_token_set:\n",
    "                updated_met_tok_set = []\n",
    "                for tok in tok_set:\n",
    "                    replacement_tok = replacements_for_the_token_set.get(tok, tok)\n",
    "                    updated_met_tok_set.append(replacement_tok)\n",
    "                    if tok != replacement_tok:\n",
    "                        toks_as_frozenset = frozenset([tok, replacement_tok])\n",
    "                        if not check_presence(\n",
    "                            toks_as_frozenset, unique_replacement_similarity_scores\n",
    "                        ):\n",
    "                            unique_replacement_similarity_scores[\n",
    "                                toks_as_frozenset\n",
    "                            ] = fuzz.ratio(tok, replacement_tok, processor=None)\n",
    "\n",
    "            # append the updated token set to the list of updated token sets\n",
    "            token_sets_full_dataset_updated.append(frozenset(updated_met_tok_set))\n",
    "\n",
    "        # assign the updated token sets as the current token sets.\n",
    "        token_sets_full_dataset = token_sets_full_dataset_updated\n",
    "\n",
    "        # append the updated token set to the list of list of token sets for the full dataset.\n",
    "        token_sets_list_tracked = []\n",
    "        for ind_num in range(len(profession_token_sets_full_dataset)):\n",
    "            tok_set_history, curr_met = (\n",
    "                profession_token_sets_full_dataset[ind_num],\n",
    "                token_sets_full_dataset[ind_num],\n",
    "            )\n",
    "\n",
    "            if tok_set_history[-1] != curr_met:\n",
    "                tok_set_history.append(curr_met)\n",
    "            token_sets_list_tracked.append(tok_set_history)\n",
    "\n",
    "        updated_token_sets_full_dataset_tracked = token_sets_list_tracked\n",
    "\n",
    "        # print the tokens related statistics for the iteration\n",
    "        print(\n",
    "            \"\\tDescriptive statistics of Merge Results for {} time\".format(\n",
    "                itration_count\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print_token_statistics(\n",
    "            prev_token_sets_len=len(set(prev_token_sets_full_dataset)),\n",
    "            curr_token_sets_len=len(set(token_sets_full_dataset)),\n",
    "            unique_tokens_count_before=len(counter_all_tokens),\n",
    "            unique_tokens_count_filtered=len(get_tok_count(filtered_token_sets)),\n",
    "            unique_tokens_count_after=len(get_tok_count(token_sets_full_dataset)),\n",
    "        )\n",
    "\n",
    "        # print the similarity score related statistics for the iteration\n",
    "        replacement_scores_iteration = np.fromiter(\n",
    "            unique_replacement_similarity_scores.values(), dtype=float\n",
    "        )\n",
    "\n",
    "        if len(replacement_scores_iteration):\n",
    "            replacement_scores_round.append(replacement_scores_iteration)\n",
    "\n",
    "            flattened_replacement_scores = replacement_scores_iteration.flatten()\n",
    "\n",
    "            print_merge_statistics(replacement_scores_list=flattened_replacement_scores)\n",
    "\n",
    "        else:\n",
    "            print(\"\\t\\tNo Replacements\")\n",
    "\n",
    "    # print the tokens related statistics for the complete round\n",
    "    print(\"\\n\")\n",
    "    print(\"*\" * 100)\n",
    "    print(\"Descriptive statistics of Merge Results for this Round\")\n",
    "\n",
    "    # print the similarity score related statistics for the complete round\n",
    "    if len(replacement_scores_round):\n",
    "\n",
    "        flattened_replacement_scores_round = np.hstack(replacement_scores_round)\n",
    "\n",
    "        print_merge_statistics(\n",
    "            replacement_scores_list=flattened_replacement_scores_round\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"\\t\\tNo Replacements\")\n",
    "    print(\"*\" * 100)\n",
    "\n",
    "    # return the list of list of token sets and the list of similarity scores for tokens merged during the round.\n",
    "    return updated_token_sets_full_dataset_tracked, replacement_scores_round"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a8bf2-5d41-4a35-9f52-d47aec38d868",
   "metadata": {},
   "source": [
    "### Read the data frame with serialized token sets (separated by space) for each row \n",
    "\n",
    "Only the column that contains the serialized token sets (separated by space) is read to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f5b7a-f108-419d-9c9f-12adba0c0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\"metier_after_S2\"]\n",
    "\n",
    "paris_jobs_only_s2 = pd.read_csv(\n",
    "    \"./../data/intermediate_steps/all_paris_jobs_after_step_2.csv\",\n",
    "    dtype={\"metier_after_S2\": \"str\"},\n",
    "    usecols=col_list,\n",
    "    header=0,\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a57099-b17d-42cb-ab40-aa0cdce2e8aa",
   "metadata": {},
   "source": [
    "The serialized token sets are split at space (as they represent tokens) and the token sets for all the rows are stored in a list. The reason for storing them in a list is to keep track of the changes to the token sets throughout the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60b679-b635-435d-bc13-822630eaf445",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_all_profession_token_sets = [\n",
    "    [frozenset(clean_met.split())]\n",
    "    for clean_met in paris_jobs_only_s2[\"metier_after_S2\"]\n",
    "    if len(frozenset(clean_met.split()))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68bfea-e7ad-4e77-9540-4992e284dc17",
   "metadata": {},
   "source": [
    "### Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9739aa-defc-4214-8cae-81e089a847d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current token sets for each profession\n",
    "initial_token_sets = get_current_token_sets(paris_all_profession_token_sets)\n",
    "\n",
    "# get the set of unique tokens from the dataset\n",
    "intial_indv_tokens = set(get_tok_count(initial_token_sets).keys())\n",
    "\n",
    "# create a set of tokens in list of correct words\n",
    "correctly_spelled_tokens = intial_indv_tokens.intersection(words_with_correct_spellings)\n",
    "\n",
    "num_of_correctly_spelled_tokens = len(correctly_spelled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc91f4a-f4ea-46bd-933f-2afcfd693a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of unique tokens that represent the professions before the round 1 are {}, \\nand {} out of them are in the dictionary ({}%).\".format(\n",
    "        len(intial_indv_tokens),\n",
    "        num_of_correctly_spelled_tokens,\n",
    "        round((num_of_correctly_spelled_tokens / len(intial_indv_tokens)) * 100),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d7b76-2d23-4909-815c-2211c402078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1: Merge spellings of words that are close and have rest of the tokens same\n",
    "# Minimum Token frequency is set to zero because in the first round the tokens are not filtered based on their frequency.\n",
    "\n",
    "(\n",
    "    paris_all_profession_token_sets_after_round_1,\n",
    "    replacement_scores_round_after_round_1,\n",
    ") = merge_close_spellings(\n",
    "    profession_token_sets_full_dataset=paris_all_profession_token_sets,\n",
    "    correctly_spelled_words=correctly_spelled_tokens,\n",
    "    minimum_token_frequency=0,\n",
    "    minimum_similarity=MINIMUM_TOKEN_SIMILARITY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b15e74-04b0-4ca8-9652-ab2a6283946a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Saving \n",
    "\n",
    "##### The updated list of list of token sets to disk as pickle file as there are frozensets after round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc32be-b7ce-4c01-ac2c-492c312913f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"paris_all_profession_token_sets_after_round_1.pickle\", \"wb\",\n",
    ") as outfile:\n",
    "    pickle.dump(paris_all_profession_token_sets_after_round_1, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75628a4b-13ae-4892-9d9f-650f1f2255aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### The replacements similarity score to disk as pickle file as it is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d98bc-5448-48b6-b367-635a3d1a4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"replacement_scores_round_after_round_1.pickle\", \"wb\"\n",
    ") as outfile:\n",
    "    pickle.dump(replacement_scores_round_after_round_1, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606de4e1-6f36-42bf-b576-5f2671a9191f",
   "metadata": {},
   "source": [
    "#### Reading\n",
    "\n",
    "##### The updated list of list of token sets from disk after round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9a830-e23d-46ea-8403-c40e31c582a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"paris_all_profession_token_sets_after_round_1.pickle\", \"rb\"\n",
    ") as outfile:\n",
    "    paris_all_profession_token_sets_after_round_1 = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12dfad6-ad1a-4e06-922a-397cdb9b1c8c",
   "metadata": {},
   "source": [
    "##### the replacements similarity score from disk after round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af76984-cadd-45bd-a16f-f344c75484c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"replacement_scores_round_after_round_1.pickle\", \"rb\"\n",
    ") as outfile:\n",
    "    replacement_scores_round_after_round_1 = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8077ce7-9f8e-49f9-89e3-fbccce2e0cb2",
   "metadata": {},
   "source": [
    "### Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52405d69-0290-4469-acb2-4bfc05a857fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current token sets for each profession\n",
    "token_sets_after_round_1 = get_current_token_sets(\n",
    "    paris_all_profession_token_sets_after_round_1\n",
    ")\n",
    "\n",
    "# get the set of unique tokens from the dataset\n",
    "indv_tokens_after_round_1 = set(get_tok_count(token_sets_after_round_1).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd40cf-c49d-48e1-ab1c-e8b24c9d689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of unique tokens that represent the professions before the round 2 are {}, \\nand {} out of them are in the dictionary ({}%).\".format(\n",
    "        len(indv_tokens_after_round_1),\n",
    "        num_of_correctly_spelled_tokens,\n",
    "        round((num_of_correctly_spelled_tokens / len(indv_tokens_after_round_1)) * 100),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf11af0-e2c7-4160-a06e-7d8a61754e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 2: Merge spellings of words that are close and have rest of the tokens same after retaining the tokens that occur more than 50 times\n",
    "\n",
    "(\n",
    "    paris_all_profession_token_sets_after_round_2,\n",
    "    replacement_scores_round_after_round_2,\n",
    ") = merge_close_spellings(\n",
    "    profession_token_sets_full_dataset=paris_all_profession_token_sets_after_round_1,\n",
    "    correctly_spelled_words=correctly_spelled_tokens,\n",
    "    minimum_token_frequency=MINIMUM_TOKEN_FREQUENCY,\n",
    "    minimum_similarity=MINIMUM_TOKEN_SIMILARITY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927de003-fd3e-48ed-9cb0-7e685fd0583a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Saving \n",
    "\n",
    "##### The updated list of list of token sets to disk as pickle file as there are frozensets after round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0531e4e-72a9-43a4-bccb-56308755e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"paris_all_profession_token_sets_after_round_2.pickle\", \"wb\"\n",
    ") as outfile:\n",
    "    pickle.dump(paris_all_profession_token_sets_after_round_2, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49330109-f5e3-4246-9b24-6aad06974121",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### The replacements similarity score to disk as pickle file as it is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083219b-aa41-40b6-95e0-4f436c08be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"replacement_scores_round_after_round_2.pickle\", \"wb\"\n",
    ") as outfile:\n",
    "    pickle.dump(replacement_scores_round_after_round_2, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57684db0-665b-4921-a024-567e96d49fce",
   "metadata": {},
   "source": [
    "#### Reading\n",
    "\n",
    "##### The updated list of list of token sets from disk after round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e342f4e-fd25-4674-8377-8c26d1e780ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"paris_all_profession_token_sets_after_round_2.pickle\", \"rb\"\n",
    ") as outfile:\n",
    "    paris_all_profession_token_sets_after_round_2 = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3fcb6f-5a0d-4959-9061-36ab943781c4",
   "metadata": {},
   "source": [
    "##### the replacements similarity score from disk after round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937b734-7d00-43b7-8c1b-cd232a8a3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"replacement_scores_round_after_round_2.pickle\", \"rb\"\n",
    ") as outfile:\n",
    "    replacement_scores_round_after_round_2 = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98352a12-159b-4e43-977b-122d48d49fae",
   "metadata": {},
   "source": [
    "### Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8c6bd-1e5d-4670-9272-f778fd2e5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current token sets for each profession\n",
    "token_sets_after_round_2 = get_current_token_sets(\n",
    "    paris_all_profession_token_sets_after_round_2\n",
    ")\n",
    "\n",
    "# get the set of unique tokens from the dataset\n",
    "indv_tokens_after_round_2 = set(get_tok_count(token_sets_after_round_2).keys())\n",
    "\n",
    "# Convert each token into a token set\n",
    "full_inv_token_after_round_2 = [\n",
    "    [frozenset([tok])] for tok_set in token_sets_after_round_2 for tok in tok_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706bf1ec-190b-4f0f-935a-a5467f67ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of unique tokens that represent the professions before the round 3 are {}, \\nand {} out of them are in the dictionary ({}%).\".format(\n",
    "        len(indv_tokens_after_round_2),\n",
    "        num_of_correctly_spelled_tokens,\n",
    "        round((num_of_correctly_spelled_tokens / len(indv_tokens_after_round_2)) * 100),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cf731-ea76-4895-8f7d-a22e78d1a018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Round 3\n",
    "\n",
    "(\n",
    "    unique_tokens_after_merging,\n",
    "    replacement_scores_round_after_round_3,\n",
    ") = merge_close_spellings(\n",
    "    profession_token_sets_full_dataset=full_inv_token_after_round_2,\n",
    "    correctly_spelled_words=correctly_spelled_tokens,\n",
    "    minimum_token_frequency=0,\n",
    "    minimum_similarity=MINIMUM_TOKEN_SIMILARITY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a915810-8501-4980-9e0c-1d67e2b3edea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Saving \n",
    "\n",
    "##### All unique tokens (with evolution) to disk as pickle file as there are frozensets after round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdbcee-bf17-4d1a-9113-e5fd331a55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"unique_tokens_with_changes_after_round_3.pickle\", \"wb\"\n",
    ") as outfile:\n",
    "    pickle.dump(unique_tokens_after_merging, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389db8a-30ef-46a0-904a-23ecdad2bbcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### The replacements similarity score to disk as pickle file as it is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ec6c6-ef8c-44f4-8577-773873572f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"replacement_scores_round_after_round_3.pickle\", \"wb\"\n",
    ") as outfile:\n",
    "    pickle.dump(replacement_scores_round_after_round_3, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7fd5ae-c75a-4fbe-828b-62ec046ad62f",
   "metadata": {},
   "source": [
    "#### Reading\n",
    "\n",
    "##### the unique tokens (with evolution) from disk after round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa46628-c4f9-49f8-a766-f84ccfa63abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"unique_tokens_with_changes_after_round_3.pickle\", \"rb\"\n",
    ") as outfile:\n",
    "    unique_tokens_after_merging = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd61c7-9ba3-4cb7-9866-978fab46699a",
   "metadata": {},
   "source": [
    "##### the replacements similarity score from disk after round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc088c7-c6a5-4abc-949c-786027ee56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"replacement_scores_round_after_round_3.pickle\", \"rb\"\n",
    ") as outfile:\n",
    "    replacement_scores_round_after_round_3 = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe3f14-7317-4f92-8500-faf91d63a98e",
   "metadata": {},
   "source": [
    "### Updating Profession token sets after Round 3\n",
    "\n",
    "The result of the third round of merging is a list of changes for each unique token in the dataset. To update the tokens in the dataset with changes, in the next step a dictionary is created with the key as the token before round 3 and the value is the same token after round 3. Only the tokens that have changed are included in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61528ad-cfc8-4958-89ab-ee01305711e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to hold the changes to the tokens\n",
    "unique_token_changes = {}\n",
    "\n",
    "for unique_token_updates in unique_tokens_after_merging:\n",
    "    # for each token changes\n",
    "    original_tok = list(unique_token_updates[0])[0]\n",
    "    # the token before round 3\n",
    "    changed_tok = list(unique_token_updates[-1])[0]\n",
    "    # the token after round 3\n",
    "\n",
    "    if original_tok != changed_tok:\n",
    "        # if the token is changed\n",
    "        if not check_presence(original_tok, unique_token_changes):\n",
    "            # create an entry in the dictionary\n",
    "            unique_token_changes[original_tok] = changed_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3087d8-c5ba-4905-af7c-101ef91a56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_unique_token_sets_after_round_3(\n",
    "    current_unique_profession_token_sets: List[FrozenSet[str]],\n",
    "    individual_token_changes: Dict[str, str],\n",
    ") -> Dict[FrozenSet[str], FrozenSet[str]]:\n",
    "    \"\"\"The function accepts the unique token sets and individual token changes after round 3 and returns a dictionary with the before update token sets as keys and the after update token changes as values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_unique_profession_token_sets : List[FrozenSet[str]]\n",
    "        A list of unique token sets for the professions.\n",
    "    individual_token_changes : Dict[str, str]\n",
    "        A dictionary with the tokens before round 3 as keys and the tokens after round 3 as values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[FrozenSet[str], FrozenSet[str]]\n",
    "        A dictionary with the before update token sets as keys and the after update token changes as values.\n",
    "    \"\"\"\n",
    "    updated_unique_profession_token_sets = {}\n",
    "    for curr_tokens_set in current_unique_profession_token_sets:\n",
    "        updated_unique_profession_token_sets[curr_tokens_set] = frozenset(\n",
    "            [individual_token_changes.get(token, token) for token in curr_tokens_set]\n",
    "        )\n",
    "\n",
    "    return updated_unique_profession_token_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e33c4-851e-44ae-ab4f-204db88ee9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current token sets for each profession\n",
    "token_sets_after_round_2 = get_current_token_sets(\n",
    "    paris_all_profession_token_sets_after_round_2\n",
    ")\n",
    "\n",
    "# get the unique token sets after round 2 (as round 3 deals with tokens and not token sets)\n",
    "unique_token_sets_after_round_2 = get_unique_token_sets(token_sets_after_round_2)\n",
    "\n",
    "# update the unique token sets with changes after round 3\n",
    "updated_token_sets_mapping_after_round_3 = update_unique_token_sets_after_round_3(\n",
    "    unique_token_sets_after_round_2, unique_token_changes\n",
    ")\n",
    "\n",
    "# Update the list of token sets for all the profession strings stored in a list.\n",
    "paris_all_profession_token_sets_after_round_3 = []\n",
    "\n",
    "for token_set_list in tqdm_notebook(paris_all_profession_token_sets_after_round_2):\n",
    "    if check_presence(token_set_list[-1], updated_token_sets_mapping_after_round_3):\n",
    "        if (\n",
    "            updated_token_sets_mapping_after_round_3[token_set_list[-1]]\n",
    "            != token_set_list[-1]\n",
    "        ):\n",
    "            paris_all_profession_token_sets_after_round_3.append(\n",
    "                token_set_list\n",
    "                + [updated_token_sets_mapping_after_round_3[token_set_list[-1]]]\n",
    "            )\n",
    "            continue\n",
    "    paris_all_profession_token_sets_after_round_3.append(token_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2eda2a-4b73-4198-9156-928573d547e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current token sets for each profession after round 3\n",
    "token_sets_after_round_3 = get_current_token_sets(\n",
    "    paris_all_profession_token_sets_after_round_3\n",
    ")\n",
    "\n",
    "# get the set of unique tokens from the dataset after round 3\n",
    "indv_tokens_after_round_3 = get_tok_count(token_sets_after_round_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85972782-c5ec-4f24-9afd-1e5f72a2934e",
   "metadata": {},
   "source": [
    "### Saving the tokens to csv file\n",
    "\n",
    "The token sets is updated from frozen sets to strings (sepearated by space) as frozen sets cannot be saved to disk. Earlier, only the one column was read to save the space. Here, the full file is read and a new column is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b435ab-03ec-4d7f-9674-2334868fd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the full dataset\n",
    "paris_jobs_after_s2 = pd.read_csv(\n",
    "    \"./../data/intermediate_steps/all_paris_jobs_after_step_2.csv\",\n",
    "    dtype={\n",
    "        \"doc_id\": \"str\",\n",
    "        \"page\": \"str\",\n",
    "        \"row\": \"str\",\n",
    "        \"Nom\": \"str\",\n",
    "        \"mÃ©tier_original\": \"str\",\n",
    "        \"rue\": \"str\",\n",
    "        \"numÃ©ro\": \"str\",\n",
    "        \"annee\": \"str\",\n",
    "        #\"gallica_page\": \"str\",\n",
    "        \"mÃ©tier\": \"str\",\n",
    "        \"metier_after_S1\": \"str\",\n",
    "        \"metier_after_S2\": \"str\",\n",
    "    },\n",
    "    header=0,\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# searlize the token sets to strings (seperated by space)\n",
    "metier_strings_after_round_3 = [\n",
    "    \" \".join(tok_frozen_set) for tok_frozen_set in token_sets_after_round_3\n",
    "]\n",
    "\n",
    "# add the new column to the dataset\n",
    "paris_jobs_after_s2[\"metier_after_S3\"] = metier_strings_after_round_3\n",
    "\n",
    "paris_jobs_after_s2.to_csv(\n",
    "    intermediate_steps_folder_prefix + \"all_paris_jobs_after_step_3.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f1e9b-3957-43eb-b3f3-c8e52250099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of unique tokens that represent the professions after the round 3 are {}, \\nand {} out of them are in the dictionary ({}%).\".format(\n",
    "        len(indv_tokens_after_round_3),\n",
    "        num_of_correctly_spelled_tokens,\n",
    "        round((num_of_correctly_spelled_tokens / len(indv_tokens_after_round_3)) * 100),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8d4c7-9b57-43e1-b63c-b1123754302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the unique tokens after the round 3, a dictionary with the token as key and the count is stored as the value (in another dictionary)\n",
    "\n",
    "sorted_unique_tokens_after_round_3 = {\n",
    "    k: {\"count\": v}\n",
    "    for k, v in sorted(\n",
    "        indv_tokens_after_round_3.items(), key=lambda e: e[1], reverse=True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee11653-d3af-47fc-9920-41f5c85f34a7",
   "metadata": {},
   "source": [
    "### Saving \n",
    "\n",
    "##### The updated list of list of token sets to disk as pickle file as there are frozensets after round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebc428-c4b6-46c6-a246-dc155961dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"paris_all_profession_token_sets_after_round_3.pickle\",\n",
    "    \"wb\",\n",
    ") as outfile:\n",
    "    pickle.dump(paris_all_profession_token_sets_after_round_3, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e9763-4b03-48d8-8021-b42cf0ee252b",
   "metadata": {},
   "source": [
    "#### The unique tokens after round 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a08b4a-70a5-46f7-9bfb-8a855db1f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"unique_tokens_after_round_3.json\",\n",
    "    \"w\",\n",
    "    encoding=\"utf8\",\n",
    ") as outfile:\n",
    "    json.dump(sorted_unique_tokens_after_round_3, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bc83a-578f-430c-b007-d869b2f22f1a",
   "metadata": {},
   "source": [
    "### Reading\n",
    "\n",
    "##### The updated list of list of token sets from disk after round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464fa89-168f-4097-8848-67bf57ba2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"paris_all_profession_token_sets_after_round_3.pickle\",\n",
    "    \"rb\",\n",
    ") as outfile:\n",
    "    paris_all_profession_token_sets_after_round_3 = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbb88d-a639-440c-ab8a-d10090670d37",
   "metadata": {},
   "source": [
    "#### The unique tokens after round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4eeac-49ef-4c72-9120-a8726ebc5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"unique_tokens_after_round_3.json\",\n",
    "    encoding=\"utf8\",\n",
    ") as f:\n",
    "    sorted_unique_tokens_after_round_3 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e601bc9-1e5d-4c70-a6c0-145ce4f31e7e",
   "metadata": {},
   "source": [
    "## Completing the Abbreviations with the full word (Step 4)\n",
    "\n",
    "In the previous step, the tokens that do not have a correct spelling were merged with the closest correctly spelled word iteratively. In this last algorithmic step, the tokens that are classified as abbreviations are completed into full words. Any token containing a dot (`.`) is classified as an abbreviation. The idea of completing the abbreviations is drawn from round 1 of the previous step i.e. to use the contextual tokens to decide the abbreviations of the words. After the abbreviations are filled with contextual information, for the second time, the remaining ones are filled based on only frequency and similarity.\n",
    "\n",
    "### Algorithm for completing the abbreviations\n",
    "\n",
    "1. The current token sets for all the entries in the dataset are considered.\n",
    "2. The abbreviations are completed using the contextual tokens (see [Algorithm for completing the abbreviations with support](#algorithm-for-completing-the-abbreviations-with-support))\n",
    "3. Collected the tokens that have a dot as potential abbreviations after _2_.\n",
    "4. The abbreviations are completed without contextual tokens (see [Algorithm for completing the abbreviations without support](#algorithm-for-completing-the-abbreviations-without-support))\n",
    "\n",
    "#### Algorithm for completing the abbreviations with support\n",
    "\n",
    "1. While the abbreviations can be filled, continue the iteration for the current list of token sets per profession (The token sets with only one token are ignored in this sub-step as they do not have contextual information)\n",
    "2. Get the co-occurance frequency using the complete dataset with all possible combination of tokens for a given token set i.e. for a set of 3 tokens T1, T2 and T3, the possible combinations are `{T1, T2}, {T2, T3}, {T1, T3}, {{T1, T2}, T3}, {{T1, T3}, T2} and {{T2, T3}, {T1}}`.\n",
    "3. The unique token sets are clustered based on the length. \n",
    "4. For each length of the token set\n",
    "    1. For each token set of the considered token set length\n",
    "        1. If not all the tokens are in the list of correctly spelled words and there is any token with a dot\n",
    "            1. Compare with all other token sets of the same length that have at least one token in the list of correctly spelled words\n",
    "                1. If the two token sets differ only by one token (i.e. all the tokens are the same except one), only one token has a dot (abbreviation) and the other token is in the list of correctly spelled words (full form), the length the full form is greater than the abbreviation and lastly the 2-gram Jaccard similarity of the abbreviation without a dot and the full form (reduced to the length of the abbreviation without dot) (called modified Jaccard score) is greater than the set threshold\n",
    "                    1. With the same tokens in both the token sets as a base, store all such pairs of abbreviation and full form tokens along with the modified Jaccard score and the frequency of the full form with the common tokens.\n",
    "5. For each base of token sets,\n",
    "    1. For each abbreviation and its possible full forms, select the full form with the highest modified Jaccard score and then the frequency of the full form with the common tokens to produce a base wise update mapping that stores the token and the token to which it should be updated.\n",
    "6. Using the base-wise abbreviation full forms the tokens sets containing those abbreviations are updated. While updating the abbreviations, a counter indicating the number of times an abbreviation is replaced for a particular full form is created. To keep in mind, this counter is created over unique token sets rather than the full dataset.     \n",
    "7. If there aren't anymore full form suggestions from _5_ then stop the iteration, else continue from _2_.\n",
    "\n",
    "#### Algorithm for completing the abbreviations without support\n",
    "\n",
    "1. For each possible abbreviation\n",
    "    1. If the same abbreviation is filled while using the support, then the full form that the abbreviation is replaced with for the most number of times using the support is used to complete the unfilled abbreviation.\n",
    "    2. If the abbreviation is not filled while using the support, then the closest abbreviation that was filled using the support is obtained by using the `fuzz.ratio` similarity. If the multiple filled abbreviations have the same similarity as the unfilled one, the filled abbreviation with high frequency is considered and the unfilled abbreviation is filled with the full form that the filled abbreviation is replaced with for the most number of times using the support.\n",
    "\n",
    "\n",
    "### Utility functions to fill the abbreviations \n",
    "\n",
    "The functions are\n",
    "\n",
    "1. `get_cooccurring_frequency`: Returns the co-occurrence frequency using the current list of token sets with all possible combinations of tokens for a given token set.\n",
    "2. `get_possible_full_form`: The function accepts the token sets grouped length-wise along with a set of correctly spelled words, the tokens cooccurring frequency, and a threshold score for similarity of full form and abbreviation and return a dictionary with the common tokens as keys and the abbreviation and potential full forms as values.\n",
    "3. `get_full_form_suggestions_per_unique_tokens_set`: The function accepts a list of possible full for a given abbreviation with a set of common tokens between the abbreviation and the full form and selects the full form that is similar and most frequent.\n",
    "4. `fill_abbreviations_with_support`: The function accepts the current token sets for the full dataset and iteratively fills the abbreviations with support until no more abbreviations can be filled.\n",
    "5. `fill_abbreviations_without_support`: The function accepts the unique tokens left after filling abbreviations using contextual tokens and other related arguments and returns a mapping of unfilled abbreviations to a full form and the counter of abbreviation and full form \n",
    "6. `fill_abbreviations_with_full_forms`: The function accepts the list of token sets after round 3 of step 3 along with the list of correctly spelled words, the similarity thresholds and return the list of lists of tokens sets after completing full forms for an abbreviation with and without the support and the counter for abbreviation and full form pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b5040-6bd4-4169-9183-4c69b7833d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurring_frequency(\n",
    "    current_profession_token_sets: List[FrozenSet[str]],\n",
    ") -> Dict[FrozenSet[str], int]:\n",
    "    \"\"\"Returns the co-occurance frequency using the current list of token sets with all possible combination of tokens for a given token set i.e. for a set of 3 tokens T1, T2 and T3, the possible combinations are `{T1, T2}, {T2, T3}, {T1, T3}, {{T1, T2}, T3}, {{T1, T3}, T2} and {{T2, T3}, {T1}}`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_profession_token_sets : List[FrozenSet[str]]\n",
    "        The list of profession token sets for full the dataset \n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pair_wise_count : Dict[FrozenSet[str], int]\n",
    "        A dictionary with a frozenset of tokens as key as the number of times they appeared together as the value.\n",
    "    \"\"\"\n",
    "\n",
    "    # a dictionary to store cooccurring frequency of tokens with the words as key and count as value\n",
    "    pair_wise_count = {}\n",
    "\n",
    "    for prfes_toks in current_profession_token_sets:\n",
    "        # for each token set for profession\n",
    "        if len(prfes_toks) > 1:\n",
    "            # if there are more than one tokens, otherwise there is no cooccurring frequency\n",
    "            for comb_len in range(2, len(prfes_toks) + 1):\n",
    "                # for all length combinations of tokens\n",
    "                for tok_pair in itertools.combinations(prfes_toks, comb_len):\n",
    "                    # for each combination\n",
    "                    frozen_tok_pair = frozenset(tok_pair)\n",
    "                    if frozen_tok_pair not in pair_wise_count:\n",
    "                        pair_wise_count[frozen_tok_pair] = 0\n",
    "                    # increase the count\n",
    "                    pair_wise_count[frozen_tok_pair] += 1\n",
    "    return pair_wise_count\n",
    "\n",
    "\n",
    "def get_possible_full_form(\n",
    "    length_wise_token_sets: Dict[int, List[FrozenSet[str]]],\n",
    "    prev_len_wise_token_sets: Dict[int, List[FrozenSet[str]]],\n",
    "    correctly_spelled_words: Set[str],\n",
    "    tokens_cooccur_freq: Dict[FrozenSet[str], int],\n",
    "    abbvr_similarity_theshold: float,\n",
    ") -> Dict[FrozenSet[str], Dict[str, Dict[str, Tuple[float, int]]]]:\n",
    "    \"\"\"The function accepts the token sets grouped length-wise along with a set of correctly spelled words, the tokens cooccurring frequency, and a threshold score for similarity of full form and abbreviation and return a dictionary with the common tokens as keys and the abbreviation and potential full forms as values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    length_wise_token_sets : Dict[int, List[FrozenSet[str]]]\n",
    "         A dictionary with the length of the token sets as the key and the token sets as the values in a list.\n",
    "    prev_len_wise_token_sets : Dict[int, List[FrozenSet[str]]]\n",
    "        A dictionary with length of the token sets as the key and the token sets as the values in a list at previous iteration.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    tokens_cooccur_freq : Dict[FrozenSet[str], int]\n",
    "        A dictionary with a frozenset of tokens as key and the number of times they appeared together as the value.\n",
    "    abbvr_similarity_theshold : float\n",
    "        The minimum threshold for the modified Jaccard similarity score between the full form and the abbreviation.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    possible_full_forms : Dict[FrozenSet[str], Dict[str, Dict[str, Tuple[float, int]]]]\n",
    "        A three-level dictionary with the common tokens between two token sets as primary key, the abbreviation as the secondary key and the full form as the tertiary key, and the tuple of modified Jaccard similarity score and the frequency of full form with the common tokens as value.\n",
    "    \"\"\"\n",
    "\n",
    "    possible_full_forms = {}\n",
    "    for str_len, str_lists in length_wise_token_sets.items():\n",
    "        # for token sets of same length\n",
    "        if str_len != 1:\n",
    "            # ignore the sets with only one token\n",
    "\n",
    "            if set(str_lists) != set(prev_len_wise_token_sets.get(str_len, [])):\n",
    "                # If the token sets of a given length at this iteration are not same as the token sets of a same length in previous iteration, continue with the process. If the sets are same with previous iteration, it means that the those tokens are not filled and the checking for full forms is redundant.\n",
    "\n",
    "                print(\"Checking for token sets of length {}\".format(str_len))\n",
    "\n",
    "                len_str_lists = len(str_lists)\n",
    "                for ind_lst_1 in tqdm_notebook(range(len_str_lists)):\n",
    "                    # for each token set\n",
    "                    tok_set1 = str_lists[ind_lst_1]\n",
    "                    if (\n",
    "                        not all(tok in correctly_spelled_words for tok in tok_set1)\n",
    "                    ) and any(\".\" in tok for tok in tok_set1):\n",
    "                        # if not all the tokens are in the dictionary and there is atleast one token with a dot\n",
    "                        for ind_lst_2 in range(ind_lst_1 + 1, len_str_lists):\n",
    "                            # for each pair of token sets of same length\n",
    "                            tok_set2 = str_lists[ind_lst_2]\n",
    "                            if any(tok in correctly_spelled_words for tok in tok_set2):\n",
    "                                # if there is atleast one word in the list of correctly spelled words and\n",
    "                                # get the tokens that are different in the two token sets (acts as secondary key)\n",
    "                                set_sym_diff = tok_set1 ^ tok_set2\n",
    "                                if len(set_sym_diff) == 2:\n",
    "                                    # if the two token sets only differ by one element\n",
    "                                    abbvr, full_form = set_sym_diff\n",
    "                                    if \".\" in full_form:\n",
    "                                        abbvr, full_form = full_form, abbvr\n",
    "\n",
    "                                    if (\n",
    "                                        ((\".\" in abbvr) and (\".\" not in full_form))\n",
    "                                        and check_presence(\n",
    "                                            full_form, correctly_spelled_words\n",
    "                                        )\n",
    "                                        and (len(full_form) > len(abbvr))\n",
    "                                    ):\n",
    "                                        # and if only one of them has a dot and one is the list of correctly spelled words\n",
    "\n",
    "                                        froz_set_same = frozenset(\n",
    "                                            tok_set1.intersection(tok_set2)\n",
    "                                        )\n",
    "\n",
    "                                        if froz_set_same not in possible_full_forms:\n",
    "                                            possible_full_forms[froz_set_same] = {}\n",
    "\n",
    "                                        if (\n",
    "                                            abbvr\n",
    "                                            not in possible_full_forms[froz_set_same]\n",
    "                                        ):\n",
    "                                            possible_full_forms[froz_set_same][\n",
    "                                                abbvr\n",
    "                                            ] = {}\n",
    "                                        if (\n",
    "                                            full_form\n",
    "                                            not in possible_full_forms[froz_set_same][\n",
    "                                                abbvr\n",
    "                                            ]\n",
    "                                        ):\n",
    "                                            # get the modified jaccard score\n",
    "                                            modified_jaccard_score = (\n",
    "                                                textdistance.Jaccard(\n",
    "                                                    qval=2\n",
    "                                                ).normalized_similarity(\n",
    "                                                    abbvr[:-1],\n",
    "                                                    full_form[: len(abbvr) - 1],\n",
    "                                                )\n",
    "                                                * 100\n",
    "                                            )\n",
    "                                            if (\n",
    "                                                modified_jaccard_score\n",
    "                                                > abbvr_similarity_theshold\n",
    "                                            ):\n",
    "                                                # if the modified jaccard score is greater than the threshold, create the mapping\n",
    "                                                possible_full_forms[froz_set_same][\n",
    "                                                    abbvr\n",
    "                                                ][full_form] = (\n",
    "                                                    modified_jaccard_score,\n",
    "                                                    tokens_cooccur_freq.get(\n",
    "                                                        froz_set_same.union(\n",
    "                                                            frozenset([full_form])\n",
    "                                                        ),\n",
    "                                                        0,\n",
    "                                                    ),\n",
    "                                                )\n",
    "            else:\n",
    "                print(\"Skipping checking for token sets of length {}\".format(str_len))\n",
    "    return possible_full_forms\n",
    "\n",
    "\n",
    "def get_full_form_suggestions_per_unique_tokens_set(\n",
    "    possible_full_forms: Dict[FrozenSet[str], Dict[str, Dict[str, Tuple[float, int]]]]\n",
    ") -> Dict[FrozenSet[str], Dict[str, str]]:\n",
    "    \"\"\"The function accepts a list of possible full for a given abbreviation with a set of common tokens between the abbreviation and the full form and selects the full form that is similar and most frequent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    possible_full_forms : Dict[FrozenSet[str], Dict[str, Dict[str, Tuple[float, int]]]]\n",
    "        A three-level dictionary with the common tokens between two token sets as primary key, the abbreviation as the secondary key and the full form as the tertiary key, and the tuple of modified Jaccard similarity score and the frequency of full form with the common tokens as value.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    replacements_per_token_set: Dict[FrozenSet[str], Dict[str, str]]\n",
    "        A two-level dictionary with the common tokens between two token sets as the primary key, the abbreviation as the secondary key, and the full form as the value.\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary to store the full form for abbreviation per common base\n",
    "    replacements_per_token_set = {}\n",
    "\n",
    "    for context_set, abvr_full_form_map in possible_full_forms.items():\n",
    "        # for each common set of tokens\n",
    "        for abvr, possible_full_forms_tuples in abvr_full_form_map.items():\n",
    "            # for each abbreviation per common base\n",
    "            if possible_full_forms_tuples:\n",
    "                # if there are any possible full forms\n",
    "                selected_full_forms = sorted(\n",
    "                    possible_full_forms_tuples.items(),\n",
    "                    key=lambda item: (item[1][0], item[1][1]),\n",
    "                    reverse=True,\n",
    "                )[0]\n",
    "                # sort the possible full forms based on similarity score and the frequency of the full form with the common set of tokens\n",
    "                # select the first element of the soreted list and add the mapping to the dictionary.\n",
    "                tok_set = context_set.union(frozenset([abvr]))\n",
    "                if tok_set not in replacements_per_token_set:\n",
    "                    replacements_per_token_set[tok_set] = {}\n",
    "                if abvr not in replacements_per_token_set[tok_set]:\n",
    "                    replacements_per_token_set[tok_set][abvr] = selected_full_forms[0]\n",
    "\n",
    "    return replacements_per_token_set\n",
    "\n",
    "\n",
    "def fill_abbreviations_with_support(\n",
    "    current_profession_token_sets: List[FrozenSet[str]],\n",
    "    correctly_spelled_words: Set[str],\n",
    "    abbvr_similarity_theshold: float,\n",
    ") -> Tuple[List[FrozenSet[str]], Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"The function accepts the current token sets for the full dataset and iteratively fills the abbreviations based on the common tokens in other sets until no more abbreviations can be filled.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_profession_token_sets : List[FrozenSet[str]]\n",
    "        The list of profession token sets for full the dataset \n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    abbvr_similarity_theshold : float\n",
    "        The minimum threshold for the modified Jaccard similarity score between the full form and the abbreviation.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    current_profession_token_sets : List[FrozenSet[str]]\n",
    "        The list of profession token sets for full the dataset after filling full forms with support\n",
    "    filled_abbreviations : Dict[str, Dict[str, int]]\n",
    "        A two-level dictionary with the abbreviation as the first key and the full form as the secondary key and the number of times the abbreviation is filled with that full form per unique set of tokens as the count.\n",
    "    \"\"\"\n",
    "\n",
    "    fill_abbreviations = True\n",
    "    iteration_counter = 0\n",
    "    filled_abbreviations = {}\n",
    "    # A dictionary to store the the length wise clustered profession token sets before the update, used to stop the (filling) iteration for the particular length when the token sets do not update any further.\n",
    "    prev_token_sets_per_length = {}\n",
    "\n",
    "    while fill_abbreviations:\n",
    "        # try to fill the abbreviations as long as possible\n",
    "        iteration_counter += 1\n",
    "        print(\"\\nAdding abbreviations  {} time\".format(iteration_counter))\n",
    "\n",
    "        # get the unique tokens sets\n",
    "        unique_profes_toks = get_unique_token_sets(current_profession_token_sets)\n",
    "        # get the length wise clustered token sets\n",
    "        token_sets_per_length = get_length_wise_token_sets(unique_profes_toks)\n",
    "        # get the tokens cooccurring frequency\n",
    "        cooccr_freq_map = get_cooccurring_frequency(current_profession_token_sets)\n",
    "\n",
    "        # get the possible full forms for each abbreviation\n",
    "        full_form_suggestions = get_possible_full_form(\n",
    "            token_sets_per_length,\n",
    "            prev_token_sets_per_length,\n",
    "            correctly_spelled_words,\n",
    "            cooccr_freq_map,\n",
    "            abbvr_similarity_theshold,\n",
    "        )\n",
    "\n",
    "        # assign the current clustering of token sets per length as previous iteration's token sets per length to be used in the next iteration to break the merge loop\n",
    "        prev_token_sets_per_length = token_sets_per_length\n",
    "\n",
    "        # select the full form for abbreviation\n",
    "        full_form_sugges_per_token_set = get_full_form_suggestions_per_unique_tokens_set(\n",
    "            full_form_suggestions\n",
    "        )\n",
    "\n",
    "        if full_form_sugges_per_token_set:\n",
    "            # if there are some full forms for abbreviations\n",
    "            updated_unique_token_sets = {}\n",
    "\n",
    "            # change the abbreviation to the full form and increase or create a count for the abbreviation full form pair\n",
    "            for org_tok_set, replacements in full_form_sugges_per_token_set.items():\n",
    "\n",
    "                new_tokens = []\n",
    "                for org_tok in org_tok_set:\n",
    "                    new_token = replacements.get(org_tok, org_tok)\n",
    "                    new_tokens.append(new_token)\n",
    "                    if new_token != org_tok:\n",
    "                        if org_tok not in filled_abbreviations:\n",
    "                            filled_abbreviations[org_tok] = {}\n",
    "                        if new_token not in filled_abbreviations[org_tok]:\n",
    "                            filled_abbreviations[org_tok][new_token] = 0\n",
    "                        filled_abbreviations[org_tok][new_token] += 1\n",
    "\n",
    "                updated_unique_token_sets[org_tok_set] = frozenset(new_tokens)\n",
    "\n",
    "            # update the token sets with the new full forms and continue filling the abbreviations\n",
    "            current_profession_token_sets = [\n",
    "                updated_unique_token_sets.get(token_set, token_set)\n",
    "                for token_set in current_profession_token_sets\n",
    "            ]\n",
    "        else:\n",
    "            # if there aren't any suggestions for abbreviations then stop the iteration\n",
    "            fill_abbreviations = False\n",
    "\n",
    "    return current_profession_token_sets, filled_abbreviations\n",
    "\n",
    "\n",
    "def fill_abbreviations_without_support(\n",
    "    possible_abbreviations: List[str],\n",
    "    filled_abvr_full_forms: Dict[str, Dict[str, int]],\n",
    "    tokens_counter: Counter_type[str],\n",
    "    min_abvr_abvr_similarity: float,\n",
    ") -> Tuple[Dict[str, str], Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"The function accepts the unique tokens left after filling abbreviations using contextual tokens and other related arguments and returns a mapping of unfilled abbreviations to a full form and the counter of abbreviation and full form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    possible_abbreviations : List[str]\n",
    "        A list of unique tokens from the dataset that contain a dot after filling the abbreviations with support (unfilled abbreviations).\n",
    "    filled_abvr_full_forms : Dict[str, Dict[str, int]]\n",
    "        A two-level dictionary with the abbreviation as the first key and the full form as the secondary key and the number of times the abbreviation is filled with that full form per unique set of tokens as the count.\n",
    "    tokens_counter : Counter_type[str]\n",
    "        The frequency counter of the individual tokens in the dataset.\n",
    "    min_abvr_abvr_similarity : float\n",
    "        The minimum threshold between the filled and unfilled abbreviations to be considered similar.\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_abvr_fullforms_mapping : Dict[str, str]\n",
    "        A dictionary with unfilled abbreviation as the key and the selected full form as the value.\n",
    "    filled_abvr_full_forms_updated : Dict[str, Dict[str, int]]\n",
    "        A two-level dictionary with the abbreviation as the first key and the full form as the secondary key and the number of times the abbreviation is filled with that full form per unique set of tokens as the count.\n",
    "    \"\"\"\n",
    "\n",
    "    filled_abvr_full_forms_updated = copy.deepcopy(filled_abvr_full_forms)\n",
    "    filled_abbreviations = set(filled_abvr_full_forms.keys())\n",
    "    new_abvr_fullforms_mapping = {}\n",
    "\n",
    "    print(\"Filling abbreviations without support\")\n",
    "\n",
    "    for pos_abvr in tqdm_notebook(possible_abbreviations):\n",
    "        # for each unfilled abbreviation\n",
    "        filled_dict = None\n",
    "        if pos_abvr in filled_abvr_full_forms:\n",
    "            # if the same abbreviation was filled in the previous step then get the most frequently substitued full form\n",
    "            filled_dict = filled_abvr_full_forms[pos_abvr]\n",
    "        else:\n",
    "            # if the abbreviation is not previously filled, get the closest abbreviation. If there are multiple abbreviations close to the abbreviation to be filled.\n",
    "            # Select the previously filled abbreviation with highest frequency in the entire dataset.\n",
    "\n",
    "            close_abvrs = process.extract(\n",
    "                pos_abvr,\n",
    "                filled_abbreviations,\n",
    "                processor=simple_processor,\n",
    "                scorer=fuzz.ratio,\n",
    "                score_cutoff=min_abvr_abvr_similarity,\n",
    "            )\n",
    "\n",
    "            selected_filled_abvr = None\n",
    "            if close_abvrs:\n",
    "                if len(close_abvrs) == 1:\n",
    "                    # if there is only one abbreviation, select it.\n",
    "                    selected_filled_abvr = close_abvrs[0][0]\n",
    "                else:\n",
    "                    # if there are more one one possible abbreviation, then they are sorted based on the similarity score\n",
    "                    sim_scores = [sim_score for _, sim_score, _ in close_abvrs]\n",
    "                    abvr_contenders = np.array(close_abvrs)[\n",
    "                        np.where(sim_scores == np.max(sim_scores))\n",
    "                    ]\n",
    "                    if len(abvr_contenders) == 1:\n",
    "                        # select the abbreviation with highest similarity score to merge with.\n",
    "                        selected_filled_abvr = abvr_contenders[0][0]\n",
    "                    else:\n",
    "                        # if there are more than one possible abbreviations with same similarity score, then they are sorted based on their frequency in the dataset.\n",
    "                        abvr_freq = [\n",
    "                            tokens_counter[mergee] for mergee, _, _ in abvr_contenders\n",
    "                        ]\n",
    "                        abvr_contenders_freq = np.array(abvr_contenders)[\n",
    "                            np.where(abvr_freq == np.max(abvr_freq))\n",
    "                        ]\n",
    "                        if len(abvr_contenders_freq) == 1:\n",
    "                            # select the token with highest frequency to merge with.\n",
    "                            selected_filled_abvr = abvr_contenders_freq[0][0]\n",
    "\n",
    "            if selected_filled_abvr:\n",
    "                # if any filled abbreviation is selected, get the full forms of the filled abbreviation.\n",
    "                filled_dict = filled_abvr_full_forms[selected_filled_abvr]\n",
    "\n",
    "        if filled_dict:\n",
    "            # select the full form that is replaced highest number of times and update the count\n",
    "            suggested_fullform = max(filled_dict, key=filled_dict.get)\n",
    "\n",
    "            new_abvr_fullforms_mapping[pos_abvr] = suggested_fullform\n",
    "\n",
    "            if pos_abvr not in filled_abvr_full_forms_updated:\n",
    "                filled_abvr_full_forms_updated[pos_abvr] = {}\n",
    "            if suggested_fullform not in filled_abvr_full_forms_updated[pos_abvr]:\n",
    "                filled_abvr_full_forms_updated[pos_abvr][suggested_fullform] = 0\n",
    "\n",
    "            filled_abvr_full_forms_updated[pos_abvr][suggested_fullform] += 1\n",
    "    return new_abvr_fullforms_mapping, filled_abvr_full_forms_updated\n",
    "\n",
    "\n",
    "def fill_abbreviations_with_full_forms(\n",
    "    profession_token_sets_full_dataset: List[List[FrozenSet[str]]],\n",
    "    correctly_spelled_words: Set[str],\n",
    "    abbvr_full_form_similarity_theshold: float,\n",
    "    min_abvr_abvr_similarity: float,\n",
    ") -> Tuple[List[List[FrozenSet[str]]], Dict[str, Dict[str, int]]]:\n",
    "    \"\"\"The function accepts the list of token sets after round 3 of step 3 along with the list of correctly spelled words, the similarity thresholds and return the list of lists of tokens sets after completing full forms for an abbreviation with and without the support and the counter for abbreviation and full form pair.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profession_token_sets_full_dataset : List[List[FrozenSet[str]]]\n",
    "        A list of token sets for the unique profession strings is stored in a list. The token sets are stored in a list to track the changes.\n",
    "    correctly_spelled_words : Set[str]\n",
    "        The set of tokens from the unique tokens for the dataset belonging to the list of correctly spelled words.\n",
    "    abbvr_full_form_similarity_theshold : float\n",
    "        The minimum threshold for the modified Jaccard similarity score between the full form and the abbreviation.\n",
    "    min_abvr_abvr_similarity : float\n",
    "        The minimum threshold between the filled and unfilled abbreviations to be considered similar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    token_sets_full_dataset_with_full_forms : List[List[FrozenSet[str]]]\n",
    "        A list of token sets for the unique profession strings is stored in a list after changing abbreviations to full forms. The token sets are stored in a list to track the changes.\n",
    "    full_forms_counter : Dict[str, Dict[str, int]]\n",
    "        A two-level dictionary with the abbreviation as the first key and the full form as the secondary key and the number of times the abbreviation is filled with that full form per unique set of tokens as the count.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a copy of the profession token sets lists to avoid in place editing.\n",
    "    updated_token_sets_full_dataset_tracked = copy.deepcopy(\n",
    "        profession_token_sets_full_dataset\n",
    "    )\n",
    "\n",
    "    # get the last entry from the list of list of token sets as the last entry is the last updated entry as current token sets.\n",
    "    token_sets_full_dataset = [\n",
    "        tok_list[-1] for tok_list in updated_token_sets_full_dataset_tracked\n",
    "    ]\n",
    "    # get the counter of tokens before completing full forms\n",
    "    tokens_count_before_fullforms = get_tok_count(token_sets_full_dataset)\n",
    "\n",
    "    # fill the abbreviations using contextual tokens\n",
    "    (\n",
    "        token_sets_full_dataset_with_support_full_forms,\n",
    "        support_full_forms,\n",
    "    ) = fill_abbreviations_with_support(\n",
    "        token_sets_full_dataset,\n",
    "        correctly_spelled_words,\n",
    "        abbvr_full_form_similarity_theshold,\n",
    "    )\n",
    "\n",
    "    # get the individual tokens after filling the full form using support tokens\n",
    "    indv_tokens_after_support_fullforms = get_tok_count(\n",
    "        token_sets_full_dataset_with_support_full_forms\n",
    "    )\n",
    "    # get the unique token sets after filling the full form using support tokens\n",
    "    unique_token_sets_after_support_fullforms = get_unique_token_sets(\n",
    "        token_sets_full_dataset_with_support_full_forms\n",
    "    )\n",
    "\n",
    "    # get the list of tokens with dot\n",
    "    left_over_abbrvs = [\n",
    "        tok for tok in indv_tokens_after_support_fullforms if \".\" in tok\n",
    "    ]\n",
    "\n",
    "    # fill the abbreviations without using contextual tokens\n",
    "    abvrs_fullforms, full_forms_counter = fill_abbreviations_without_support(\n",
    "        left_over_abbrvs,\n",
    "        support_full_forms,\n",
    "        tokens_count_before_fullforms,\n",
    "        min_abvr_abvr_similarity,\n",
    "    )\n",
    "\n",
    "    # update the token sets for full dataset by adding the possible full forms\n",
    "    unique_professions_before_after_fullforms = {\n",
    "        tok_froz_set: frozenset(\n",
    "            abvrs_fullforms.get(token, token) for token in tok_froz_set\n",
    "        )\n",
    "        for tok_froz_set in unique_token_sets_after_support_fullforms\n",
    "    }\n",
    "\n",
    "    token_sets_full_dataset_with_full_forms = [\n",
    "        [unique_professions_before_after_fullforms.get(token_set)]\n",
    "        for token_set in token_sets_full_dataset_with_support_full_forms\n",
    "    ]\n",
    "\n",
    "    return token_sets_full_dataset_with_full_forms, full_forms_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eecb2c-14a3-4d25-a460-9d8063b64751",
   "metadata": {},
   "source": [
    "Using the list of list of token sets of all the dataset, the abbrevations are filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe450b05-2a64-4ccb-b110-6a96210d7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    paris_profession_token_sets_after_round_4,\n",
    "    fullforms_counter,\n",
    ") = fill_abbreviations_with_full_forms(\n",
    "    paris_all_profession_token_sets_after_round_3,\n",
    "    words_with_correct_spellings,\n",
    "    abbvr_full_form_similarity_theshold=MINIMUM_ABVR_FULLFORM_SIMILARITY,\n",
    "    min_abvr_abvr_similarity=MINIMUM_INTER_ABVR_SIMILARITY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4afb6-8346-4875-9bc0-38d6bacf881c",
   "metadata": {},
   "source": [
    "### Saving \n",
    "\n",
    "#### the dataset with abbreviations added tokens\n",
    "\n",
    "The token sets is updated from frozen sets to strings (sepearated by space) as frozen sets cannot be saved to disk. Earlier, only the one column was read to save the space. Here, the full file is read and a new column is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810de082-a912-4a9d-8272-e21ce84fa129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the full dataset\n",
    "paris_jobs_after_s3 = pd.read_csv(\n",
    "    intermediate_steps_folder_prefix + \"/all_paris_jobs_after_step_3.csv\",\n",
    "    dtype={\n",
    "        \"doc_id\": \"str\",\n",
    "        \"page\": \"str\",\n",
    "        \"row\": \"str\",\n",
    "        \"Nom\": \"str\",\n",
    "        \"mÃ©tier_original\": \"str\",\n",
    "        \"rue\": \"str\",\n",
    "        \"numÃ©ro\": \"str\",\n",
    "        \"annee\": \"str\",\n",
    "        #\"gallica_page\": \"str\",\n",
    "        \"mÃ©tier\": \"str\",\n",
    "        \"metier_after_S1\": \"str\",\n",
    "        \"metier_after_S2\": \"str\",\n",
    "        \"metier_after_S3\": \"str\",\n",
    "    },\n",
    "    header=0,\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# get the current token sets for each profession after step 4\n",
    "token_sets_after_round_4 = get_current_token_sets(\n",
    "    paris_profession_token_sets_after_round_4\n",
    ")\n",
    "\n",
    "\n",
    "# searlize the token sets to strings (seperated by space)\n",
    "metier_strings_after_round4 = [\n",
    "    \" \".join(met_frozen_set) for met_frozen_set in token_sets_after_round_4\n",
    "]\n",
    "\n",
    "# add the new column to the dataset\n",
    "paris_jobs_after_s3[\"metier_after_S4\"] = metier_strings_after_round4\n",
    "\n",
    "paris_jobs_after_s3.to_csv(\n",
    "    intermediate_steps_folder_prefix + \"all_paris_jobs_after_step_4.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019c9d9-136a-4ba3-a83c-9b57ef07821c",
   "metadata": {},
   "source": [
    "#### the individual tokens for the dataset after merging and filling full forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e429af-e69b-4302-b92a-d815d34e5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the unique tokens after the step 4, a dictionary with the token as key and the count is stored as the value (in another dictionary)\n",
    "\n",
    "sorted_unique_tokens_after_step_4 = {\n",
    "    k: {\"count\": v}\n",
    "    for k, v in sorted(\n",
    "        get_tok_count(token_sets_after_round_4).items(), key=lambda e: e[1], reverse=True\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"cleaned_unique_tokens.json\",\n",
    "    \"w\",\n",
    "    encoding=\"utf8\",\n",
    ") as outfile:\n",
    "    json.dump(sorted_unique_tokens_after_step_4, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ba83f-8720-4da7-af20-d1624a89d381",
   "metadata": {},
   "source": [
    "#### the abbreviation and full form mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa84d1-3854-4eca-941c-1a20bd3e33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    intermediate_steps_folder_prefix + \"abbreviation_full_forms.json\", \"w\", encoding=\"utf8\"\n",
    ") as outfile:\n",
    "    json.dump(fullforms_counter, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93f426-e74c-40b0-88ac-8b6caa3973a0",
   "metadata": {},
   "source": [
    "## Saving the tags (Step 5)\n",
    "\n",
    "Until this step, the profession string of the dataset has been cleaned at multiple levels. In the last step, the data is stored on a disk. \n",
    "\n",
    "Data storage has two main components. First, is the complete dataset (nearly 4.5 million lines) that contains the information of all the addresses of Paris. Second, in the previous phase of the project, a subset of lines from the 5.5 million lines correspond to the Richelieu district where the street name (rue) and the number (numÃ©ro) are cleaned.\n",
    "\n",
    "For the first component, the data shall be stored as a CSV. The reason for choosing a CSV file format is that the street name (rue) and the number (numÃ©ro) are not cleaned and normalized. Thus it is difficult to group the addresses spatially. The CSV file will contain the following columns\n",
    "\n",
    "1. `annee`: The year of the entry\n",
    "2. `gallica_ark`: The arc identifier on the Gallica\n",
    "3. `gallica_page`: The page in the document (identified through `gallica_ark`) where the entry is present.\n",
    "4. `row`: The row in the page `gallica_page` where the entry is present\n",
    "5. `name`: The name of the person (sometimes, it is a business/ entity)\n",
    "6. `mÃ©tier_from_ocr`: The profession obtained as a result of OCRisation during phase 1 of the project\n",
    "7. `rue`: The street name\n",
    "8. `numÃ©ro`: The number of the house in the given street\n",
    "9. `tags`: A list of strings called tags that represent the profession of the person/entity.\n",
    "\n",
    "\n",
    "For the second component, the data will be stored as JSON pivoted based on the address. The format of the file will be \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"rue name\": {\n",
    "Â  Â  Â  Â  \"details\": [\n",
    "Â  Â  Â  Â  Â  Â  {\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  \"rue number\": {\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"location\": {\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"geo coordinates\": {\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"latitude\": \"\",\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"longitude\": \"\"\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"people\": {\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"year\": [\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Person Name\": \"\",\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Profession\": \"\",\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Tags\": [],\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Related Data\": {\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Multimedia URLs\": [],\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Other URLs\": []\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"gallica link\": \"\"\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }\n",
    "Â  Â  Â  Â  Â  Â  Â  Â  }\n",
    "Â  Â  Â  Â  Â  Â  }\n",
    "Â  Â  Â  Â  ]\n",
    "Â  Â  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Data for complete Paris (first component)\n",
    "\n",
    "During the process of creating and cleaning the tokens, some rows in the dataset were dropped as they contained small profession stings or empty profession. In the next step, the original rows that were dropped earlier will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16963f9e-3fc1-43b3-9eb6-72cb03434bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset before any post processing\n",
    "paris_jobs_before_normalising = pd.read_csv(\n",
    "    \"./../data/intermediate_steps/all_paris_jobs_with_gallica_pageno.csv\",\n",
    "    names=[\n",
    "        \"doc_id\",\n",
    "        \"page\",\n",
    "        \"row\",\n",
    "        \"Nom\",\n",
    "        \"mÃ©tier_original\",\n",
    "        \"rue\",\n",
    "        \"numÃ©ro\",\n",
    "        \"annee\",\n",
    "        #\"gallica_page\",\n",
    "    ],\n",
    "    dtype={\n",
    "        \"doc_id\": \"str\",\n",
    "        \"page\": \"str\",\n",
    "        \"row\": \"str\",\n",
    "        \"Nom\": \"str\",\n",
    "        \"mÃ©tier\": \"str\",\n",
    "        \"rue\": \"str\",\n",
    "        \"numÃ©ro\": \"str\",\n",
    "        \"annee\": \"str\",\n",
    "        #\"gallica_page\": \"str\",\n",
    "    },\n",
    "    header=0,\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e558cc-9371-43d3-bfe4-a8cf1ae19916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset before adding tags is merged with dataset after four steps of cleaning on the components that remain unchanged.\n",
    "paris_jobs_full_dataset = pd.merge(\n",
    "    paris_jobs_before_normalising,\n",
    "    paris_jobs_after_s3,\n",
    "    how=\"outer\",\n",
    "    on=[\n",
    "        \"doc_id\",\n",
    "        \"page\",\n",
    "        \"row\",\n",
    "        \"Nom\",\n",
    "        \"rue\",\n",
    "        \"numÃ©ro\",\n",
    "        \"mÃ©tier_original\",\n",
    "        \"annee\",\n",
    "        #\"gallica_page\",\n",
    "    ],\n",
    ")\n",
    "# The columns containing the tokens in the intermediate steps are removed.\n",
    "paris_jobs_full_dataset.drop(\n",
    "    columns=[\"page\", \"mÃ©tier\", \"metier_after_S1\", \"metier_after_S2\", \"metier_after_S3\"],\n",
    "    inplace=True,\n",
    ")\n",
    "# The columns are renamed to understand easily.\n",
    "paris_jobs_full_dataset.rename(\n",
    "    columns={\n",
    "        \"doc_id\": \"gallica_ark\",\n",
    "        \"Nom\": \"name\",\n",
    "        \"mÃ©tier_original\": \"mÃ©tier_from_ocr\",\n",
    "        \"metier_after_S4\": \"tags\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "# the tokens after step 4 were stored as string seperated by space. Now, the token string is split to produce list of tags and it stored as a new column.\n",
    "paris_jobs_full_dataset[\"tags\"] = paris_jobs_full_dataset[\"tags\"].str.split()\n",
    "\n",
    "# This dataset is saved to disk\n",
    "paris_jobs_full_dataset.to_csv(\n",
    "    \"./../data/outcome_of_current_project/paris_jobs_with_tags_richelieu_project.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094f094-8257-4f6d-a67a-1ddc3c96bf3d",
   "metadata": {},
   "source": [
    "### Data for Richelieu district (second component)\n",
    "\n",
    "The data extracted for Richelieu district is stored at `df_addressing_with_numbers.csv`. Similarily to previous component, the two datasets are merged and then stored as a csv. However, the the profession under `mÃ©tier` is wrong and simplified and the `rue` is cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5fc05-d196-4816-aa29-b1e10ce84153",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "richelieu_jobs_before_normalising = pd.read_csv(\n",
    "    \"./../data/from_previous_project/people_of_richelieu_1839_1922.csv\",\n",
    "    names=[\n",
    "        \"old_index\",\n",
    "        \"doc_id\",\n",
    "        \"annee\",\n",
    "        \"page\",\n",
    "        \"row\",\n",
    "        \"Nom\",\n",
    "        \"mÃ©tier_simplified\",\n",
    "        \"rue\",\n",
    "        \"numÃ©ro\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ],\n",
    "    dtype={\n",
    "        \"old_index\": \"str\",\n",
    "        \"doc_id\": \"str\",\n",
    "        \"annee\": \"str\",\n",
    "        \"page\": \"str\",\n",
    "        \"row\": \"str\",\n",
    "        \"nom\": \"str\",\n",
    "        \"mÃ©tier\": \"str\",\n",
    "        \"rue\": \"str\",\n",
    "        \"numÃ©ro\": \"str\",\n",
    "        \"latitude\": \"str\",\n",
    "        \"longitude\": \"str\",\n",
    "    },\n",
    "    header=0,\n",
    "    encoding=\"utf-8\",\n",
    "    usecols=[\n",
    "        \"doc_id\",\n",
    "        \"annee\",\n",
    "        \"page\",\n",
    "        \"row\",\n",
    "        \"Nom\",\n",
    "        \"mÃ©tier_simplified\",\n",
    "        \"rue\",\n",
    "        \"numÃ©ro\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ],\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f408a-83db-4153-b243-920b4576c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# The dataset of Richelieu district (without tags) is merged with dataset after four steps of cleaning on the components that remain unchanged.\n",
    "richelieu_jobs_with_tags = pd.merge(\n",
    "    richelieu_jobs_before_normalising,\n",
    "    paris_jobs_after_s3,\n",
    "    how=\"left\",\n",
    "    on=[\"doc_id\", \"page\", \"row\", \"Nom\", \"annee\"],\n",
    ")\n",
    "\n",
    "# The columns containing the tokens in the intermediate steps are removed.\n",
    "richelieu_jobs_with_tags.drop(\n",
    "    columns=[\n",
    "        \"page\",\n",
    "        \"mÃ©tier_simplified\",\n",
    "        \"rue_y\",\n",
    "        \"numÃ©ro_y\",\n",
    "        \"mÃ©tier\",\n",
    "        \"metier_after_S1\",\n",
    "        \"metier_after_S2\",\n",
    "        \"metier_after_S3\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# The columns are renamed to understand easily.\n",
    "richelieu_jobs_with_tags.rename(\n",
    "    columns={\n",
    "        \"doc_id\": \"gallica_ark\",\n",
    "        \"Nom\": \"name\",\n",
    "        \"mÃ©tier_original\": \"mÃ©tier_from_ocr\",\n",
    "        \"metier_after_S4\": \"tags\",\n",
    "        \"rue_x\": \"rue\",\n",
    "        \"numÃ©ro_x\": \"numÃ©ro\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# the tokens after step 4 were stored as string seperated by space. Now, the token string is split to produce list of tags and it stored as a new column.\n",
    "richelieu_jobs_with_tags[\"tags\"] = richelieu_jobs_with_tags[\"tags\"].str.split()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41387a85-319e-4ed5-a048-fd5db3ba1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "richelieu_jobs_with_tags\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ddf22-563b-4719-8eae-cbafde71f3fa",
   "metadata": {},
   "source": [
    "To store the data in a JSON, it is first grouped by the street name, number, the latitude, longitude and the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bf117-1190-4209-a684-26872274622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "richelieu_jobs_with_tags_spatial_grouped = richelieu_jobs_with_tags.groupby(\n",
    "    by=[\"rue\", \"numÃ©ro\", \"latitude\", \"longitude\", \"annee\"]\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9bffc-31bd-4534-abd7-a88a3c0cc2b2",
   "metadata": {},
   "source": [
    "### Utility function to create a dictionary object for each entry in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190e1de-4025-494c-9f1e-b8eb9e5aa638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_person_object(\n",
    "    data_row: pd.Series,\n",
    ") -> Dict[str, Union[str, Dict[str, List[str]]]]:\n",
    "    \"\"\"The function accepts a row from the dataset and returns the information as a dictionary to enable storage in a JSON file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_row : pd.Series\n",
    "        A row of the dataset with name, profession, tags, gallica ark and gallica page number\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Union[str, Dict[str, List[str]]]]\n",
    "        The data in the row is restructured as a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        \"Person Name\": data_row[\"name\"],\n",
    "        \"Profession\": data_row[\"mÃ©tier_from_ocr\"],\n",
    "        \"Tags\": data_row[\"tags\"],\n",
    "        \"Related Data\": {\"Multimedia URLs\": [], \"Other URLs\": []},\n",
    "        \"gallica link\": \"https://gallica.bnf.fr/ark:/12148/{}/f{}.zoom\".format(\n",
    "            data_row[\"gallica_ark\"], data_row[\"gallica_page\"]\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d28c5-77a2-4361-9f4e-57963ef9ed1f",
   "metadata": {},
   "source": [
    "Loop over each of the address and year grouped dataframe and store the data in a nested dictionary with the street name as the primary key, the number as the secondary key. The geographical coordinates and year wise person information as values under the secondary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174a1b0-2589-460b-8120-3920c0fd7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "street_wise_data = {}\n",
    "\n",
    "for grp_name, group in tqdm_notebook(richelieu_jobs_with_tags_spatial_grouped):\n",
    "    street_name, number, lat, long, year = grp_name\n",
    "    if street_name not in street_wise_data:\n",
    "        street_wise_data[street_name] = {}\n",
    "    if number not in street_wise_data[street_name]:\n",
    "        street_wise_data[street_name][number] = {\n",
    "            \"geographic_coordinate\": {\"latitude\": lat, \"longitude\": long},\n",
    "            \"people\": {},\n",
    "        }\n",
    "    if year not in street_wise_data[street_name][number][\"people\"]:\n",
    "        street_wise_data[street_name][number][\"people\"][year] = []\n",
    "\n",
    "    for _, person_data in group.iterrows():\n",
    "        street_wise_data[street_name][number][\"people\"][year].append(\n",
    "            create_person_object(person_data)\n",
    "        )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886dbe9-d122-41b0-b54a-2449574517e2",
   "metadata": {},
   "source": [
    "### Saving\n",
    "\n",
    "Save the street wise dictionary created above to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8992011-7724-492b-ba73-9167ed5bc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\n",
    "    \"./../data/outcome_of_current_project/street_wise_richelieu_people.json\",\n",
    "    \"w\",\n",
    "    encoding=\"utf8\",\n",
    ") as outfile:\n",
    "    json.dump(street_wise_data, outfile, indent=4, ensure_ascii=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d01469-d548-462f-af52-efa3dd18a9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct  7 2022, 20:14:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "7c2682987f00c8c803475925d827e887daeba32793bd1ae3ff6e12f2969d73b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
